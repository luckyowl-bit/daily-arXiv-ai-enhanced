<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 128]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Learnable-Differentiable Finite Volume Solver for Accelerated Simulation of Flows](https://arxiv.org/abs/2507.01975)
*Mengtao Yan,Qi Wang,Haining Wang,Ruizhi Chengze,Yi Zhang,Hongsheng Liu,Zidong Wang,Fan Yu,Qi Qi,Hao Sun*

Main category: cs.LG

TL;DR: 提出了一种名为LDSolver的可学习且可微分的有限体积求解器，用于在粗网格上高效准确地模拟流体流动，解决了传统数值求解器和机器学习方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统数值求解器计算成本高，机器学习方法存在可解释性、泛化性和数据依赖性问题，因此需要一种更高效且通用的解决方案。

Method: LDSolver由两部分组成：(1)可微分有限体积求解器，(2)可学习模块，用于在粗网格上提供等效通量近似和时间误差校正。

Result: 实验表明，LDSolver在多种流动系统中表现优异，即使训练数据有限也能保持高精度和泛化能力，超越基线模型。

Conclusion: LDSolver为流体流动模拟提供了一种高效、准确且通用的方法，具有显著的实际应用潜力。

Abstract: Simulation of fluid flows is crucial for modeling physical phenomena like
meteorology, aerodynamics, and biomedicine. Classical numerical solvers often
require fine spatiotemporal grids to satisfy stability, consistency, and
convergence conditions, leading to substantial computational costs. Although
machine learning has demonstrated better efficiency, they typically suffer from
issues of interpretability, generalizability, and data dependency. Hence, we
propose a learnable and differentiable finite volume solver, called LDSolver,
designed for efficient and accurate simulation of fluid flows on spatiotemporal
coarse grids. LDSolver comprises two key components: (1) a differentiable
finite volume solver, and (2) an learnable module providing equivalent
approximation for fluxes (derivatives and interpolations), and temporal error
correction on coarse grids. Even with limited training data (e.g., only a few
trajectories), our model could accelerate the simulation while maintaining a
high accuracy with superior generalizability. Experiments on different flow
systems (e.g., Burgers, decaying, forced and shear flows) show that LDSolver
achieves state-of-the-art performance, surpassing baseline models with notable
margins.

</details>


### [2] [DKGCM: A Spatio-Temporal Prediction Model for Traffic Flow by Fusing Spatial Node Clustering Method and Fourier Bidirectional Mamba Mechanism](https://arxiv.org/abs/2507.01982)
*Siqing Long,Xiangzhi Huang,Jiemin Xie,Ming Cai*

Main category: cs.LG

TL;DR: 提出了一种名为DKGCM的新图卷积网络结构，用于提高交通需求预测的准确性，通过动态时间规整和K均值聚类优化空间依赖捕捉，并结合FFT和双向Mamba框架处理时间依赖性。


<details>
  <summary>Details</summary>
Motivation: 交通需求预测的准确性对资源分配至关重要，但复杂的时空关系限制了现有模型的性能。

Method: 提出DK-GCN方法，结合DTW和K-means聚类优化空间依赖捕捉；在时间尺度上，集成FFT和双向Mamba框架；采用GRPO强化学习策略优化训练。

Result: 在三个公开数据集上表现优于多种先进方法。

Conclusion: DKGCM模型显著提升了交通需求预测的准确性，为交通管理提供了有效工具。

Abstract: Accurate traffic demand forecasting enables transportation management
departments to allocate resources more effectively, thereby improving their
utilization efficiency. However, complex spatiotemporal relationships in
traffic systems continue to limit the performance of demand forecasting models.
To improve the accuracy of spatiotemporal traffic demand prediction, we propose
a new graph convolutional network structure called DKGCM. Specifically, we
first consider the spatial flow distribution of different traffic nodes and
propose a novel temporal similarity-based clustering graph convolution method,
DK-GCN. This method utilizes Dynamic Time Warping (DTW) and K-means clustering
to group traffic nodes and more effectively capture spatial dependencies. On
the temporal scale, we integrate the Fast Fourier Transform (FFT) within the
bidirectional Mamba deep learning framework to capture temporal dependencies in
traffic demand. To further optimize model training, we incorporate the GRPO
reinforcement learning strategy to enhance the loss function feedback
mechanism. Extensive experiments demonstrate that our model outperforms several
advanced methods and achieves strong results on three public datasets.

</details>


### [3] [Multimodal Misinformation Detection Using Early Fusion of Linguistic, Visual, and Social Features](https://arxiv.org/abs/2507.01984)
*Gautam Kishore Shahi*

Main category: cs.LG

TL;DR: 研究探讨了多模态特征（文本、图像和社交特征）在虚假信息检测中的有效性，发现结合无监督和监督机器学习模型能显著提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 社交媒体在选举和危机期间充斥着虚假信息，现有研究多集中于单模态（文本或图像），而多模态特征组合的研究较少。

Method: 采用早期融合方法，结合文本、图像和社交特征构建分类模型，分析1,529条推文，并通过数据增强提取视觉和社交特征。

Result: 多模态模型比单模态模型性能提升15%，比双模态模型提升5%，并分析了虚假信息的传播模式。

Conclusion: 多模态特征组合能有效提升虚假信息检测性能，同时揭示了虚假信息的传播特征。

Abstract: Amid a tidal wave of misinformation flooding social media during elections
and crises, extensive research has been conducted on misinformation detection,
primarily focusing on text-based or image-based approaches. However, only a few
studies have explored multimodal feature combinations, such as integrating text
and images for building a classification model to detect misinformation. This
study investigates the effectiveness of different multimodal feature
combinations, incorporating text, images, and social features using an early
fusion approach for the classification model. This study analyzed 1,529 tweets
containing both text and images during the COVID-19 pandemic and election
periods collected from Twitter (now X). A data enrichment process was applied
to extract additional social features, as well as visual features, through
techniques such as object detection and optical character recognition (OCR).
The results show that combining unsupervised and supervised machine learning
models improves classification performance by 15% compared to unimodal models
and by 5% compared to bimodal models. Additionally, the study analyzes the
propagation patterns of misinformation based on the characteristics of
misinformation tweets and the users who disseminate them.

</details>


### [4] [Positive region preserved random sampling: an efficient feature selection method for massive data](https://arxiv.org/abs/2507.01998)
*Hexiang Bai,Deyu Li,Jiye Liang,Yanhui Zhai*

Main category: cs.LG

TL;DR: 提出了一种基于采样技术和粗糙集理论的特征选择方法，用于处理大规模数据，能够在个人计算机上高效找到具有高区分能力的特征子集。


<details>
  <summary>Details</summary>
Motivation: 智能机器在处理大规模数据时计算资源有限，需要高效的特征选择方法。

Method: 利用可区分对象对比例衡量特征集的区分能力，构建正区域保留样本以选择特征子集。

Result: 在11个数据集上验证了方法的有效性，能在短时间内找到近似约简，且区分能力高于预估下限。

Conclusion: 该方法在个人计算机上能高效处理大规模数据，找到高区分能力的特征子集。

Abstract: Selecting relevant features is an important and necessary step for
intelligent machines to maximize their chances of success. However, intelligent
machines generally have no enough computing resources when faced with huge
volume of data. This paper develops a new method based on sampling techniques
and rough set theory to address the challenge of feature selection for massive
data. To this end, this paper proposes using the ratio of discernible object
pairs to all object pairs that should be distinguished to measure the
discriminatory ability of a feature set. Based on this measure, a new feature
selection method is proposed. This method constructs positive region preserved
samples from massive data to find a feature subset with high discriminatory
ability. Compared with other methods, the proposed method has two advantages.
First, it is able to select a feature subset that can preserve the
discriminatory ability of all the features of the target massive data set
within an acceptable time on a personal computer. Second, the lower boundary of
the probability of the object pairs that can be discerned using the feature
subset selected in all object pairs that should be distinguished can be
estimated before finding reducts. Furthermore, 11 data sets of different sizes
were used to validate the proposed method. The results show that approximate
reducts can be found in a very short period of time, and the discriminatory
ability of the final reduct is larger than the estimated lower boundary.
Experiments on four large-scale data sets also showed that an approximate
reduct with high discriminatory ability can be obtained in reasonable time on a
personal computer.

</details>


### [5] [Continuous Wavelet Transform and Siamese Network-Based Anomaly Detection in Multi-variate Semiconductor Process Time Series](https://arxiv.org/abs/2507.01999)
*Bappaditya Dey,Daniel Sorensen,Minjin Hwang,Sandip Halder*

Main category: cs.LG

TL;DR: 论文提出了一种基于机器学习的多变量时间序列异常检测方法，通过连续小波变换将数据转换为图像，并利用微调的VGG-16架构和Siamese网络进行异常检测。


<details>
  <summary>Details</summary>
Motivation: 半导体制造过程中存在高维数据、类别不平衡、噪声和缺失值等挑战，传统方法难以有效检测异常。

Method: 1. 使用连续小波变换将时间序列数据转换为图像；2. 微调预训练的VGG-16架构用于图像分类；3. 构建Siamese网络比较输入图像的嵌入向量。

Result: 在真实半导体制造数据集上表现出高精度异常检测能力，适用于监督和半监督场景。

Conclusion: 该方法为半导体制造中的异常检测提供了一种灵活且高效的解决方案。

Abstract: Semiconductor manufacturing is an extremely complex process, characterized by
thousands of interdependent parameters collected across diverse tools and
process steps. Multi-variate time-series (MTS) analysis has emerged as a
critical methodology for enabling real-time monitoring, fault detection, and
predictive maintenance in such environments. However, anomaly prediction in
semiconductor fabrication presents several critical challenges, including high
data dimensionality, severe class imbalance due to the rarity of true faults,
noisy and missing measurements, and non-stationary behavior of production
systems. Furthermore, the complex interdependencies between variables and the
delayed emergence of faults across downstream stages complicate both anomaly
detection and root-cause-analysis. This paper presents a novel and generic
approach for anomaly detection in MTS data using machine learning. The proposed
methodology consists of three main steps: a) converting MTS data into
image-based representations using the Continuous Wavelet Transform, b)
developing a multi-class image classifier by fine-tuning a pretrained VGG-16
architecture on custom CWT image datasets, and c) constructing a Siamese
network composed of two identical sub-networks, each utilizing the fine-tuned
VGG-16 as a backbone. The network takes pairs of CWT images as input -one
serving as a reference or anchor (representing a known-good signal), and the
other as a query (representing an unknown signal). The model then compares the
embeddings of both inputs to determine whether they belong to the same class at
a given time step. Our approach demonstrates high accuracy in identifying
anomalies on a real FAB process time-series dataset, offering a promising
solution for offline anomaly detection in process and tool trace data.
Moreover, the approach is flexible and can be applied in both supervised and
semi-supervised settings.

</details>


### [6] [Temporal Chain of Thought: Long-Video Understanding by Thinking in Frames](https://arxiv.org/abs/2507.02001)
*Anurag Arnab,Ahmet Iscen,Mathilde Caron,Alireza Fathi,Cordelia Schmid*

Main category: cs.LG

TL;DR: 提出了一种名为Temporal Chain of Thought的推理策略，通过迭代选择视频中最相关的帧来提升长视频问答的准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管现有视觉语言模型（VLMs）能处理约1000帧输入，但在长视频理解中仍难以有效利用上下文并易受无关内容干扰。

Method: 使用VLM自身迭代识别并提取视频中最相关的帧，作为问答的输入上下文。

Result: 在4个视频问答数据集上取得最优结果，尤其在长视频（超过1小时）上表现突出，优于传统推理方法。

Conclusion: 通过推理时计算优化上下文选择，显著提升了长视频问答的准确性。

Abstract: Despite recent advances in Vision-Language Models (VLMs), long-video
understanding remains a challenging problem. Although state-of-the-art
long-context VLMs can process around 1000 input frames, they still struggle to
effectively leverage this sequence length, and succumb to irrelevant
distractors within the context window. We present Temporal Chain of Thought, an
inference strategy for video question-answering that curates the model's input
context. We use the VLM itself to iteratively identify and extract the most
relevant frames from the video, which are then used for answering. We
demonstrate how leveraging more computation at inference-time to select the
most relevant context leads to improvements in accuracy, in agreement with
recent work on inference-time scaling of LLMs. Moreover, we achieve
state-of-the-art results on 4 diverse video question-answering datasets,
showing consistent improvements with 3 different VLMs. In particular, our
method shines on longer videos which would not otherwise fit within the model's
context window: On longer videos of more than 1 hour on LVBench, our approach
using a context window of 32K outperforms the same VLM using standard inference
with a 700K context window by 2.8 points.

</details>


### [7] [AIRES: Accelerating Out-of-Core GCNs via Algorithm-System Co-Design](https://arxiv.org/abs/2507.02006)
*Shakya Jayakody,Youpeng Zhao,Jun Wang*

Main category: cs.LG

TL;DR: AIRES是一种算法-系统协同设计解决方案，用于加速GCN中的外存SpGEMM计算，通过数据块对齐和三阶段动态调度显著降低延迟。


<details>
  <summary>Details</summary>
Motivation: 现有系统在高I/O延迟和GPU利用率不足的问题上表现不佳，尤其是在稀疏格式数据对齐和内存分配方面存在性能瓶颈。

Method: AIRES提出稀疏格式矩阵的块级数据对齐和分块算法，并采用三阶段动态调度和双路数据传输策略，结合GPU内存、GDS和主机内存。

Result: AIRES在真实图处理基准测试中表现出色，延迟降低了1.8倍。

Conclusion: AIRES通过算法和系统协同优化，显著提升了外存SpGEMM的计算效率。

Abstract: Graph convolutional networks (GCNs) are fundamental in various scientific
applications, ranging from biomedical protein-protein interactions (PPI) to
large-scale recommendation systems. An essential component for modeling graph
structures in GCNs is sparse general matrix-matrix multiplication (SpGEMM). As
the size of graph data continues to scale up, SpGEMMs are often conducted in an
out-of-core fashion due to limited GPU memory space in resource-constrained
systems. Albeit recent efforts that aim to alleviate the memory constraints of
out-of-core SpGEMM through either GPU feature caching, hybrid CPU-GPU memory
layout, or performing the computation in sparse format, current systems suffer
from both high I/O latency and GPU under-utilization issues.
  In this paper, we first identify the problems of existing systems, where
sparse format data alignment and memory allocation are the main performance
bottlenecks, and propose AIRES, a novel algorithm-system co-design solution to
accelerate out-of-core SpGEMM computation for GCNs. Specifically, from the
algorithm angle, AIRES proposes to alleviate the data alignment issues on the
block level for matrices in sparse formats and develops a tiling algorithm to
facilitate row block-wise alignment. On the system level, AIRES employs a
three-phase dynamic scheduling that features a dual-way data transfer strategy
utilizing a tiered memory system: integrating GPU memory, GPU Direct Storage
(GDS), and host memory to reduce I/O latency and improve throughput.
Evaluations show that AIRES significantly outperforms the state-of-the-art
methods, achieving up to 1.8x lower latency in real-world graph processing
benchmarks.

</details>


### [8] [GeoAda: Efficiently Finetune Geometric Diffusion Models with Equivariant Adapters](https://arxiv.org/abs/2507.02085)
*Wanjia Zhao,Jiaqi Han,Siyi Gu,Mingjian Jiang,James Zou,Stefano Ermon*

Main category: cs.LG

TL;DR: GeoAda是一种SE(3)-等变适配器框架，用于高效微调几何扩散模型，保持几何一致性并避免过拟合和灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 几何扩散模型在下游任务中的高效微调尚未充分探索，尤其是在不同几何控制条件下。

Method: 提出SE(3)-等变适配器框架（GeoAda），通过轻量级适配器模块实现参数高效微调，保持模型几何一致性。

Result: GeoAda在多种几何控制任务中表现优异，保持原始任务准确性，而其他基线方法因过拟合和灾难性遗忘性能下降。

Conclusion: GeoAda为几何扩散模型提供了一种灵活且高效的微调方法，适用于广泛的应用领域。

Abstract: Geometric diffusion models have shown remarkable success in molecular
dynamics and structure generation. However, efficiently fine-tuning them for
downstream tasks with varying geometric controls remains underexplored. In this
work, we propose an SE(3)-equivariant adapter framework ( GeoAda) that enables
flexible and parameter-efficient fine-tuning for controlled generative tasks
without modifying the original model architecture. GeoAda introduces a
structured adapter design: control signals are first encoded through coupling
operators, then processed by a trainable copy of selected pretrained model
layers, and finally projected back via decoupling operators followed by an
equivariant zero-initialized convolution. By fine-tuning only these lightweight
adapter modules, GeoAda preserves the model's geometric consistency while
mitigating overfitting and catastrophic forgetting. We theoretically prove that
the proposed adapters maintain SE(3)-equivariance, ensuring that the geometric
inductive biases of the pretrained diffusion model remain intact during
adaptation. We demonstrate the wide applicability of GeoAda across diverse
geometric control types, including frame control, global control, subgraph
control, and a broad range of application domains such as particle dynamics,
molecular dynamics, human motion prediction, and molecule generation. Empirical
results show that GeoAda achieves state-of-the-art fine-tuning performance
while preserving original task accuracy, whereas other baselines experience
significant performance degradation due to overfitting and catastrophic
forgetting.

</details>


### [9] [Evaluating the Promise and Pitfalls of LLMs in Hiring Decisions](https://arxiv.org/abs/2507.02087)
*Eitan Anzenberg,Arunava Samajpati,Sivasankaran Chandrasekar,Varun Kacholia*

Main category: cs.LG

TL;DR: 研究比较了通用大语言模型（LLM）与专有招聘模型（Match Score）在招聘中的表现，发现Match Score在准确性和公平性上均优于LLM，强调领域专用模型和偏见审核的重要性。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM在招聘中的应用问题，尤其是准确性和算法偏见，旨在验证专有模型是否能更好地解决这些问题。

Method: 通过ROC AUC、Precision-Recall AUC、F1-score等指标评估多个LLM和Match Score的预测准确性，并分析其在不同人口统计群体中的公平性。

Result: Match Score在准确性（ROC AUC 0.85 vs 0.77）和公平性（最低种族影响比0.957 vs 0.809）上均优于LLM。

Conclusion: 领域专用模型和偏见审核对高风险领域（如招聘）至关重要，且准确性与公平性可以兼顾。

Abstract: The use of large language models (LLMs) in hiring promises to streamline
candidate screening, but it also raises serious concerns regarding accuracy and
algorithmic bias where sufficient safeguards are not in place. In this work, we
benchmark several state-of-the-art foundational LLMs - including models from
OpenAI, Anthropic, Google, Meta, and Deepseek, and compare them with our
proprietary domain-specific hiring model (Match Score) for job candidate
matching. We evaluate each model's predictive accuracy (ROC AUC,
Precision-Recall AUC, F1-score) and fairness (impact ratio of cut-off analysis
across declared gender, race, and intersectional subgroups). Our experiments on
a dataset of roughly 10,000 real-world recent candidate-job pairs show that
Match Score outperforms the general-purpose LLMs on accuracy (ROC AUC 0.85 vs
0.77) and achieves significantly more equitable outcomes across demographic
groups. Notably, Match Score attains a minimum race-wise impact ratio of 0.957
(near-parity), versus 0.809 or lower for the best LLMs, (0.906 vs 0.773 for the
intersectionals, respectively). We discuss why pretraining biases may cause
LLMs with insufficient safeguards to propagate societal biases in hiring
scenarios, whereas a bespoke supervised model can more effectively mitigate
these biases. Our findings highlight the importance of domain-specific modeling
and bias auditing when deploying AI in high-stakes domains such as hiring, and
caution against relying on off-the-shelf LLMs for such tasks without extensive
fairness safeguards. Furthermore, we show with empirical evidence that there
shouldn't be a dichotomy between choosing accuracy and fairness in hiring: a
well-designed algorithm can achieve both accuracy in hiring and fairness in
outcomes.

</details>


### [10] [Sample Complexity Bounds for Linear Constrained MDPs with a Generative Model](https://arxiv.org/abs/2507.02089)
*Xingtu Liu,Lin F. Yang,Sharan Vaswani*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We consider infinite-horizon $\gamma$-discounted (linear) constrained Markov
decision processes (CMDPs) where the objective is to find a policy that
maximizes the expected cumulative reward subject to expected cumulative
constraints. Given access to a generative model, we propose to solve CMDPs with
a primal-dual framework that can leverage any black-box unconstrained MDP
solver. For linear CMDPs with feature dimension $d$, we instantiate the
framework by using mirror descent value iteration
(\texttt{MDVI})~\citep{kitamura2023regularization} an example MDP solver. We
provide sample complexity bounds for the resulting CMDP algorithm in two cases:
(i) relaxed feasibility, where small constraint violations are allowed, and
(ii) strict feasibility, where the output policy is required to exactly satisfy
the constraint. For (i), we prove that the algorithm can return an
$\epsilon$-optimal policy with high probability by using
$\tilde{O}\left(\frac{d^2}{(1-\gamma)^4\epsilon^2}\right)$ samples. We note
that these results exhibit a near-optimal dependence on both $d$ and
$\epsilon$. For (ii), we show that the algorithm requires
$\tilde{O}\left(\frac{d^2}{(1-\gamma)^6\epsilon^2\zeta^2}\right)$ samples,
where $\zeta$ is the problem-dependent Slater constant that characterizes the
size of the feasible region. Finally, we instantiate our framework for tabular
CMDPs and show that it can be used to recover near-optimal sample complexities
in this setting.

</details>


### [11] [Energy-Based Transformers are Scalable Learners and Thinkers](https://arxiv.org/abs/2507.02092)
*Alexi Gladstone,Ganesh Nanduru,Md Mofijul Islam,Peixuan Han,Hyeonjeong Ha,Aman Chadha,Yilun Du,Heng Ji,Jundong Li,Tariq Iqbal*

Main category: cs.LG

TL;DR: 论文提出了一种基于无监督学习的Energy-Based Transformers（EBTs），通过验证输入与候选预测的兼容性，将预测问题转化为优化问题，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有推理时计算方法存在局限性（如模态特定、问题特定或需要额外监督），本文旨在探索是否可以通过无监督学习实现通用的System 2 Thinking。

Method: 训练EBTs（一种基于能量的模型），为输入和候选预测对分配能量值，通过梯度下降优化能量最小化。

Result: EBTs在训练和推理中表现优于现有方法，语言任务性能提升29%，图像去噪优于Diffusion Transformers，且泛化能力更强。

Conclusion: EBTs是一种有前景的新范式，可同时提升模型的学习和推理能力。

Abstract: Inference-time computation techniques, analogous to human System 2 Thinking,
have recently become popular for improving model performances. However, most
existing approaches suffer from several limitations: they are modality-specific
(e.g., working only in text), problem-specific (e.g., verifiable domains like
math and coding), or require additional supervision/training on top of
unsupervised pretraining (e.g., verifiers or verifiable rewards). In this
paper, we ask the question "Is it possible to generalize these System 2
Thinking approaches, and develop models that learn to think solely from
unsupervised learning?" Interestingly, we find the answer is yes, by learning
to explicitly verify the compatibility between inputs and
candidate-predictions, and then re-framing prediction problems as optimization
with respect to this verifier. Specifically, we train Energy-Based Transformers
(EBTs) -- a new class of Energy-Based Models (EBMs) -- to assign an energy
value to every input and candidate-prediction pair, enabling predictions
through gradient descent-based energy minimization until convergence. Across
both discrete (text) and continuous (visual) modalities, we find EBTs scale
faster than the dominant Transformer++ approach during training, achieving an
up to 35% higher scaling rate with respect to data, batch size, parameters,
FLOPs, and depth. During inference, EBTs improve performance with System 2
Thinking by 29% more than the Transformer++ on language tasks, and EBTs
outperform Diffusion Transformers on image denoising while using fewer forward
passes. Further, we find that EBTs achieve better results than existing models
on most downstream tasks given the same or worse pretraining performance,
suggesting that EBTs generalize better than existing approaches. Consequently,
EBTs are a promising new paradigm for scaling both the learning and thinking
capabilities of models.

</details>


### [12] [Parametric Neural Amp Modeling with Active Learning](https://arxiv.org/abs/2507.02109)
*Florian Grötschla,Luca A. Lanzendörfer,Longxiang Jiao,Roger Wattenhofer*

Main category: cs.LG

TL;DR: PANAMA是一种基于主动学习的框架，用于训练端到端的参数化吉他放大器模型，采用WaveNet架构，通过最小化数据点（如旋钮设置）实现高效建模。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够通过少量数据点高效训练吉他放大器模型的方法，以解决传统建模方法需要大量数据的问题。

Method: 使用WaveNet-like架构和基于梯度的优化算法，通过主动学习策略选择最优数据点进行采样。

Result: 在有限样本条件下，该方法能够有效优化数据点选择，提高建模效率。

Conclusion: PANAMA框架通过主动学习和梯度优化，实现了高效且数据节约的吉他放大器建模。

Abstract: We introduce PANAMA, an active learning framework for the training of
end-to-end parametric guitar amp models using a WaveNet-like architecture. With
\model, one can create a virtual amp by recording samples that are determined
by an active learning strategy to use a minimum amount of datapoints (i.e., amp
knob settings). We show that gradient-based optimization algorithms can be used
to determine the optimal datapoints to sample, and that the approach helps
under a constrained number of samples.

</details>


### [13] [Scaling Collapse Reveals Universal Dynamics in Compute-Optimally Trained Neural Networks](https://arxiv.org/abs/2507.02119)
*Shikai Qiu,Lechao Xiao,Andrew Gordon Wilson,Jeffrey Pennington,Atish Agarwala*

Main category: cs.LG

TL;DR: 论文研究了神经网络训练动态的缩放极限，发现计算最优训练的模型表现出精确的普适性，损失曲线在归一化后能完美重合，称为“超级坍缩”。


<details>
  <summary>Details</summary>
Motivation: 探索模型规模和训练时间同步增长时，神经网络训练动态的普适规律。

Method: 通过分析不同模型大小、学习率计划和数据集的训练损失曲线，提出归一化方法，并研究SGD噪声动态的简单模型。

Result: 发现损失曲线在归一化后能完美重合（超级坍缩），且这种现象在超参数优化时尤为明显。

Conclusion: 超级坍缩现象揭示了神经网络训练的普适规律，为优化超参数提供了实用指标。

Abstract: What scaling limits govern neural network training dynamics when model size
and training time grow in tandem? We show that despite the complex interactions
between architecture, training algorithms, and data, compute-optimally trained
models exhibit a remarkably precise universality. Specifically, loss curves
from models of varying sizes collapse onto a single universal curve when
training compute and loss are normalized to unity at the end of training. With
learning rate decay, the collapse becomes so tight that differences in the
normalized curves across models fall below the noise floor of individual loss
curves across random seeds, a phenomenon we term supercollapse. We observe
supercollapse across learning rate schedules, datasets, and architectures,
including transformers trained on next-token prediction, and find it breaks
down when hyperparameters are scaled suboptimally, providing a precise and
practical indicator of good scaling. We explain these phenomena by connecting
collapse to the power-law structure in typical neural scaling laws, and
analyzing a simple yet surprisingly effective model of SGD noise dynamics that
accurately predicts loss curves across various learning rate schedules and
quantitatively explains the origin of supercollapse.

</details>


### [14] [CROP: Circuit Retrieval and Optimization with Parameter Guidance using LLMs](https://arxiv.org/abs/2507.02128)
*Jingyu Pan,Isaac Jacobson,Zheng Zhao,Tung-Chieh Chen,Guanglei Zhou,Chen-Chia Chang,Vineet Rashingkar,Yiran Chen*

Main category: cs.LG

TL;DR: CROP是一个基于大型语言模型（LLM）的自动VLSI设计流程优化框架，通过向量化RTL代码、相似电路匹配和检索增强生成技术，显著提升设计效率和质量。


<details>
  <summary>Details</summary>
Motivation: 现代VLSI设计复杂度高，参数空间庞大，传统人工参数选择效率低且依赖专家经验，亟需自动化解决方案。

Method: CROP包括RTL代码向量化、相似电路匹配系统和检索增强的LLM参数搜索系统。

Result: 实验显示CROP在工业设计中能以更少迭代次数实现更优结果，如功耗降低9.9%。

Conclusion: CROP为VLSI设计自动化提供了高效解决方案，显著提升了设计质量和效率。

Abstract: Modern very large-scale integration (VLSI) design requires the implementation
of integrated circuits using electronic design automation (EDA) tools. Due to
the complexity of EDA algorithms, the vast parameter space poses a huge
challenge to chip design optimization, as the combination of even moderate
numbers of parameters creates an enormous solution space to explore. Manual
parameter selection remains industrial practice despite being excessively
laborious and limited by expert experience. To address this issue, we present
CROP, the first large language model (LLM)-powered automatic VLSI design flow
tuning framework. Our approach includes: (1) a scalable methodology for
transforming RTL source code into dense vector representations, (2) an
embedding-based retrieval system for matching designs with semantically similar
circuits, and (3) a retrieval-augmented generation (RAG)-enhanced LLM-guided
parameter search system that constrains the search process with prior knowledge
from similar designs. Experiment results demonstrate CROP's ability to achieve
superior quality-of-results (QoR) with fewer iterations than existing
approaches on industrial designs, including a 9.9% reduction in power
consumption.

</details>


### [15] [Generative Latent Diffusion for Efficient Spatiotemporal Data Reduction](https://arxiv.org/abs/2507.02129)
*Xiao Li,Liangji Zhu,Anand Rangarajan,Sanjay Ranka*

Main category: cs.LG

TL;DR: 提出了一种高效的潜在扩散框架，结合变分自编码器和条件扩散模型，显著提升数据压缩性能。


<details>
  <summary>Details</summary>
Motivation: 生成模型在条件设置下表现优异，但可控性和重建精度限制了其实际应用。

Method: 通过变分自编码器和条件扩散模型结合，仅压缩少量关键帧并用于生成其余帧。

Result: 实验显示压缩比提升10倍，性能优于现有方法63%。

Conclusion: 该方法在降低存储成本的同时实现了高精度时空重建。

Abstract: Generative models have demonstrated strong performance in conditional
settings and can be viewed as a form of data compression, where the condition
serves as a compact representation. However, their limited controllability and
reconstruction accuracy restrict their practical application to data
compression. In this work, we propose an efficient latent diffusion framework
that bridges this gap by combining a variational autoencoder with a conditional
diffusion model. Our method compresses only a small number of keyframes into
latent space and uses them as conditioning inputs to reconstruct the remaining
frames via generative interpolation, eliminating the need to store latent
representations for every frame. This approach enables accurate spatiotemporal
reconstruction while significantly reducing storage costs. Experimental results
across multiple datasets show that our method achieves up to 10 times higher
compression ratios than rule-based state-of-the-art compressors such as SZ3,
and up to 63 percent better performance than leading learning-based methods
under the same reconstruction error.

</details>


### [16] [Non-exchangeable Conformal Prediction for Temporal Graph Neural Networks](https://arxiv.org/abs/2507.02151)
*Tuo Wang,Jian Kang,Yujun Yan,Adithya Kulkarni,Dawei Zhou*

Main category: cs.LG

TL;DR: NCPNET是一种针对时序图的端到端共形预测框架，解决了现有方法在动态图数据中因时间依赖性导致的统计覆盖问题。


<details>
  <summary>Details</summary>
Motivation: 现有共形预测方法主要针对静态图，忽略了真实世界图的动态性，导致时间依赖性违反共形预测的基本假设。

Method: 提出了一种基于扩散的非共形性评分，捕捉拓扑和时间不确定性，并开发了效率感知的优化算法。

Result: 在多个真实时序图数据集上验证，NCPNET显著减少了预测集大小（如WIKI数据集减少31%），并提高了效率。

Conclusion: NCPNET为时序图提供了一种高效的共形预测框架，显著提升了覆盖率和计算效率。

Abstract: Conformal prediction for graph neural networks (GNNs) offers a promising
framework for quantifying uncertainty, enhancing GNN reliability in high-stakes
applications. However, existing methods predominantly focus on static graphs,
neglecting the evolving nature of real-world graphs. Temporal dependencies in
graph structure, node attributes, and ground truth labels violate the
fundamental exchangeability assumption of standard conformal prediction
methods, limiting their applicability. To address these challenges, in this
paper, we introduce NCPNET, a novel end-to-end conformal prediction framework
tailored for temporal graphs. Our approach extends conformal prediction to
dynamic settings, mitigating statistical coverage violations induced by
temporal dependencies. To achieve this, we propose a diffusion-based
non-conformity score that captures both topological and temporal uncertainties
within evolving networks. Additionally, we develop an efficiency-aware
optimization algorithm that improves the conformal prediction process,
enhancing computational efficiency and reducing coverage violations. Extensive
experiments on diverse real-world temporal graphs, including WIKI, REDDIT,
DBLP, and IBM Anti-Money Laundering dataset, demonstrate NCPNET's capability to
ensure guaranteed coverage in temporal graphs, achieving up to a 31% reduction
in prediction set size on the WIKI dataset, significantly improving efficiency
compared to state-of-the-art methods. Our data and code are available at
https://github.com/ODYSSEYWT/NCPNET.

</details>


### [17] [Statistical Inference for Responsiveness Verification](https://arxiv.org/abs/2507.02169)
*Seung Hyun Cheon,Meredith Stewart,Bogdan Kulynych,Tsui-Wei Weng,Berk Ustun*

Main category: cs.LG

TL;DR: 论文提出了一种验证机器学习模型预测对特征干预的响应性的方法，支持黑盒估计，应用于风险评估和内容审核等领域。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在贷款、招聘等内容审核中因未考虑个体输入变化而导致安全失败，需验证预测的响应性。

Method: 通过敏感性分析框架，控制干预约束和下游效应分布，估计黑盒模型的响应性，支持伪造和失败概率估计。

Result: 开发了生成可达点均匀样本的算法，应用于累犯预测、器官移植优先级和内容审核等实际场景。

Conclusion: 该方法能有效提升机器学习模型在现实应用中的安全性。

Abstract: Many safety failures in machine learning arise when models are used to assign
predictions to people (often in settings like lending, hiring, or content
moderation) without accounting for how individuals can change their inputs. In
this work, we introduce a formal validation procedure for the responsiveness of
predictions with respect to interventions on their features. Our procedure
frames responsiveness as a type of sensitivity analysis in which practitioners
control a set of changes by specifying constraints over interventions and
distributions over downstream effects. We describe how to estimate
responsiveness for the predictions of any model and any dataset using only
black-box access, and how to use these estimates to support tasks such as
falsification and failure probability estimation. We develop algorithms that
construct these estimates by generating a uniform sample of reachable points,
and demonstrate how they can promote safety in real-world applications such as
recidivism prediction, organ transplant prioritization, and content moderation.

</details>


### [18] [Metric Design != Metric Behavior: Improving Metric Selection for the Unbiased Evaluation of Dimensionality Reduction](https://arxiv.org/abs/2507.02225)
*Jiyeon Bae,Hyeon Jeon,Jinwook Seo*

Main category: cs.LG

TL;DR: 提出了一种通过聚类评估指标以减少维度降维（DR）投影评估偏见的流程。


<details>
  <summary>Details</summary>
Motivation: 维度降维投影评估中，若选择高度相关的指标会导致评估偏见，偏向于强调某些结构特征的DR技术。

Method: 通过计算指标间的成对相关性聚类指标，减少重叠，并从每个聚类中选择代表性指标。

Result: 定量实验表明该方法提高了DR评估的稳定性，减少了评估偏见。

Conclusion: 提出的流程有效减少了DR评估中的偏见，提升了评估的可靠性。

Abstract: Evaluating the accuracy of dimensionality reduction (DR) projections in
preserving the structure of high-dimensional data is crucial for reliable
visual analytics. Diverse evaluation metrics targeting different structural
characteristics have thus been developed. However, evaluations of DR
projections can become biased if highly correlated metrics--those measuring
similar structural characteristics--are inadvertently selected, favoring DR
techniques that emphasize those characteristics. To address this issue, we
propose a novel workflow that reduces bias in the selection of evaluation
metrics by clustering metrics based on their empirical correlations rather than
on their intended design characteristics alone. Our workflow works by computing
metric similarity using pairwise correlations, clustering metrics to minimize
overlap, and selecting a representative metric from each cluster. Quantitative
experiments demonstrate that our approach improves the stability of DR
evaluation, which indicates that our workflow contributes to mitigating
evaluation bias.

</details>


### [19] [PhysicsCorrect: A Training-Free Approach for Stable Neural PDE Simulations](https://arxiv.org/abs/2507.02227)
*Xinquan Huang,Paris Perdikaris*

Main category: cs.LG

TL;DR: PhysicsCorrect是一种无需训练的校正框架，通过基于PDE残差的线性化逆问题解决神经网络在长期预测中的误差累积问题，显著提升物理模拟的准确性。


<details>
  <summary>Details</summary>
Motivation: 神经网络作为PDE求解器存在长期预测误差累积问题，导致物理模拟失效，亟需一种高效校正方法。

Method: 提出PhysicsCorrect框架，利用离线预计算Jacobian及其伪逆，以线性化逆问题形式校正PDE残差，显著降低计算开销。

Result: 在三种典型PDE系统中，PhysicsCorrect将预测误差降低100倍，推理时间增加不足5%，兼容多种神经网络架构。

Conclusion: PhysicsCorrect有效解决了神经网络模拟的稳定性问题，兼具计算效率与物理保真度，适用于科学应用。

Abstract: Neural networks have emerged as powerful surrogates for solving partial
differential equations (PDEs), offering significant computational speedups over
traditional methods. However, these models suffer from a critical limitation:
error accumulation during long-term rollouts, where small inaccuracies compound
exponentially, eventually causing complete divergence from physically valid
solutions. We present PhysicsCorrect, a training-free correction framework that
enforces PDE consistency at each prediction step by formulating correction as a
linearized inverse problem based on PDE residuals. Our key innovation is an
efficient caching strategy that precomputes the Jacobian and its pseudoinverse
during an offline warm-up phase, reducing computational overhead by two orders
of magnitude compared to standard correction approaches. Across three
representative PDE systems -- Navier-Stokes fluid dynamics, wave equations, and
the chaotic Kuramoto-Sivashinsky equation -- PhysicsCorrect reduces prediction
errors by up to 100x while adding negligible inference time (under 5\%). The
framework integrates seamlessly with diverse architectures including Fourier
Neural Operators, UNets, and Vision Transformers, effectively transforming
unstable neural surrogates into reliable simulation tools that bridge the gap
between deep learning's computational efficiency and the physical fidelity
demanded by practical scientific applications.

</details>


### [20] [VERBA: Verbalizing Model Differences Using Large Language Models](https://arxiv.org/abs/2507.02241)
*Shravan Doda,Shashidhar Reddy Javaji,Zining Zhu*

Main category: cs.LG

TL;DR: 论文提出了VERBA方法，利用大语言模型（LLM）生成模型差异的文本描述，以解决模型选择中的文档化难题。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习中存在大量性能相似但行为不同的模型，手动比较和文档化成本过高。

Method: 通过采样两个模型的数据，利用LLM生成差异描述，并通过模拟评估其信息量。

Result: VERBA在决策树模型上表现出80%的准确率，结合结构信息后提升至90%。

Conclusion: VERBA为提升模型透明度和可比性提供了新思路。

Abstract: In the current machine learning landscape, we face a "model lake" phenomenon:
Given a task, there is a proliferation of trained models with similar
performances despite different behavior. For model users attempting to navigate
and select from the models, documentation comparing model pairs is helpful.
However, for every $N$ models there could be $O(N^2)$ pairwise comparisons, a
number prohibitive for the model developers to manually perform pairwise
comparisons and prepare documentations. To facilitate fine-grained pairwise
comparisons among models, we introduced $\textbf{VERBA}$. Our approach
leverages a large language model (LLM) to generate verbalizations of model
differences by sampling from the two models. We established a protocol that
evaluates the informativeness of the verbalizations via simulation. We also
assembled a suite with a diverse set of commonly used machine learning models
as a benchmark. For a pair of decision tree models with up to 5% performance
difference but 20-25% behavioral differences, $\textbf{VERBA}$ effectively
verbalizes their variations with up to 80% overall accuracy. When we included
the models' structural information, the verbalization's accuracy further
improved to 90%. $\textbf{VERBA}$ opens up new research avenues for improving
the transparency and comparability of machine learning models in a post-hoc
manner.

</details>


### [21] [Order Acquisition Under Competitive Pressure: A Rapidly Adaptive Reinforcement Learning Approach for Ride-Hailing Subsidy Strategies](https://arxiv.org/abs/2507.02244)
*Fangzhou Shi,Xiaopeng Ke,Xinye Xiong,Kexin Meng,Chang Men,Zhengdan Zhu*

Main category: cs.LG

TL;DR: 论文提出了一种基于强化学习的优惠券策略框架FCA-RL，用于动态适应市场波动并优化订单获取，同时满足预算约束。


<details>
  <summary>Details</summary>
Motivation: 网约车聚合平台的竞争排名机制促使服务提供商通过降价策略获取更多订单，但现有研究缺乏有效的动态优惠券策略设计。

Method: 结合快速竞争适应（FCA）和强化拉格朗日调整（RLA）技术，提出FCA-RL框架，并开发了专用模拟环境RideGym。

Result: 实验表明，FCA-RL在多样市场条件下优于基线方法，有效优化了补贴策略。

Conclusion: FCA-RL为网约车服务提供商提供了一种动态适应竞争并优化订单的有效解决方案。

Abstract: The proliferation of ride-hailing aggregator platforms presents significant
growth opportunities for ride-service providers by increasing order volume and
gross merchandise value (GMV). On most ride-hailing aggregator platforms,
service providers that offer lower fares are ranked higher in listings and,
consequently, are more likely to be selected by passengers. This competitive
ranking mechanism creates a strong incentive for service providers to adopt
coupon strategies that lower prices to secure a greater number of orders, as
order volume directly influences their long-term viability and sustainability.
Thus, designing an effective coupon strategy that can dynamically adapt to
market fluctuations while optimizing order acquisition under budget constraints
is a critical research challenge. However, existing studies in this area remain
scarce.
  To bridge this gap, we propose FCA-RL, a novel reinforcement learning-based
subsidy strategy framework designed to rapidly adapt to competitors' pricing
adjustments. Our approach integrates two key techniques: Fast Competition
Adaptation (FCA), which enables swift responses to dynamic price changes, and
Reinforced Lagrangian Adjustment (RLA), which ensures adherence to budget
constraints while optimizing coupon decisions on new price landscape.
Furthermore, we introduce RideGym, the first dedicated simulation environment
tailored for ride-hailing aggregators, facilitating comprehensive evaluation
and benchmarking of different pricing strategies without compromising
real-world operational efficiency. Experimental results demonstrate that our
proposed method consistently outperforms baseline approaches across diverse
market conditions, highlighting its effectiveness in subsidy optimization for
ride-hailing service providers.

</details>


### [22] [Uncertainty-aware Reward Design Process](https://arxiv.org/abs/2507.02256)
*Yang Yang,Xiaolu Zhou,Bosong Ding,Miao Xin*

Main category: cs.LG

TL;DR: URDP框架结合大型语言模型和贝叶斯优化，提升强化学习中奖励函数设计的效率和质量。


<details>
  <summary>Details</summary>
Motivation: 传统奖励函数设计方法效率低且不一致，现有基于LLM的方法在数值优化上表现不佳，进化搜索范式则资源利用率低。

Method: 提出URDP框架，通过自一致性分析量化奖励函数不确定性，引入UABO优化超参数配置，并采用双层优化架构。

Result: 在35个任务中验证，URDP生成的奖励函数质量更高，设计效率显著提升。

Conclusion: URDP通过结合LLM的逻辑推理和贝叶斯优化的数值优势，有效解决了奖励函数设计的挑战。

Abstract: Designing effective reward functions is a cornerstone of reinforcement
learning (RL), yet it remains a challenging process due to the inefficiencies
and inconsistencies inherent in conventional reward engineering methodologies.
Recent advances have explored leveraging large language models (LLMs) to
automate reward function design. However, their suboptimal performance in
numerical optimization often yields unsatisfactory reward quality, while the
evolutionary search paradigm demonstrates inefficient utilization of simulation
resources, resulting in prohibitively lengthy design cycles with
disproportionate computational overhead. To address these challenges, we
propose the Uncertainty-aware Reward Design Process (URDP), a novel framework
that integrates large language models to streamline reward function design and
evaluation in RL environments. URDP quantifies candidate reward function
uncertainty based on self-consistency analysis, enabling simulation-free
identification of ineffective reward components while discovering novel reward
components. Furthermore, we introduce uncertainty-aware Bayesian optimization
(UABO), which incorporates uncertainty estimation to significantly enhance
hyperparameter configuration efficiency. Finally, we construct a bi-level
optimization architecture by decoupling the reward component optimization and
the hyperparameter tuning. URDP orchestrates synergistic collaboration between
the reward logic reasoning of the LLMs and the numerical optimization strengths
of the Bayesian Optimization. We conduct a comprehensive evaluation of URDP
across 35 diverse tasks spanning three benchmark environments. Our experimental
results demonstrate that URDP not only generates higher-quality reward
functions but also achieves significant improvements in the efficiency of
automated reward design compared to existing approaches.

</details>


### [23] [Knowledge Graph-Based Explainable and Generalized Zero-Shot Semantic Communications](https://arxiv.org/abs/2507.02291)
*Zhaoyu Zhang,Lingyi Wang,Wei Wu,Fuhui Zhou,Qihui Wu*

Main category: cs.LG

TL;DR: 提出了一种基于知识图谱的零样本语义通信网络（KGZS-SC），通过知识图谱增强语义表示和推理能力，解决数据驱动语义通信的泛化性和可解释性问题。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的语义通信缺乏泛化性和可解释性，尤其在面对未见数据时表现不佳。

Method: 利用知识图谱语义知识库（KG-SKB）对齐语义特征，增强发射器的泛化能力，接收端采用零样本学习（ZSL）直接分类未见数据。

Result: 在APY数据集上的实验表明，KGZS-SC网络在未见类别分类中表现优异，显著优于现有框架。

Conclusion: KGZS-SC网络通过知识图谱增强语义表示和零样本学习，提升了语义通信的泛化性和效率。

Abstract: Data-driven semantic communication is based on superficial statistical
patterns, thereby lacking interpretability and generalization, especially for
applications with the presence of unseen data. To address these challenges, we
propose a novel knowledge graph-enhanced zero-shot semantic communication
(KGZS-SC) network. Guided by the structured semantic information from a
knowledge graph-based semantic knowledge base (KG-SKB), our scheme provides
generalized semantic representations and enables reasoning for unseen cases.
Specifically, the KG-SKB aligns the semantic features in a shared category
semantics embedding space and enhances the generalization ability of the
transmitter through aligned semantic features, thus reducing communication
overhead by selectively transmitting compact visual semantics. At the receiver,
zero-shot learning (ZSL) is leveraged to enable direct classification for
unseen cases without the demand for retraining or additional computational
overhead, thereby enhancing the adaptability and efficiency of the
classification process in dynamic or resource-constrained environments. The
simulation results conducted on the APY datasets show that the proposed KGZS-SC
network exhibits robust generalization and significantly outperforms existing
SC frameworks in classifying unseen categories across a range of SNR levels.

</details>


### [24] [Holistic Continual Learning under Concept Drift with Adaptive Memory Realignment](https://arxiv.org/abs/2507.02310)
*Alif Ashrafee,Jedrzej Kozal,Michal Wozniak,Bartosz Krawczyk*

Main category: cs.LG

TL;DR: 论文提出了一种名为AMR的轻量级方法，用于动态数据流中的持续学习，通过选择性更新记忆缓冲区来应对概念漂移，性能接近完全重新学习但成本更低。


<details>
  <summary>Details</summary>
Motivation: 传统持续学习方法假设数据分布静态，忽视了现实数据流的动态性（如概念漂移），需要兼顾稳定性和快速适应性。

Method: 提出AMR方法，通过选择性移除过时样本并补充新样本，实时调整记忆缓冲区以匹配新分布。

Result: AMR在多个数据集上表现优异，接近完全重新学习的性能，但显著减少了标注和计算开销。

Conclusion: AMR是一种可扩展的解决方案，在非静态持续学习环境中平衡了稳定性和可塑性。

Abstract: Traditional continual learning methods prioritize knowledge retention and
focus primarily on mitigating catastrophic forgetting, implicitly assuming that
the data distribution of previously learned tasks remains static. This
overlooks the dynamic nature of real-world data streams, where concept drift
permanently alters previously seen data and demands both stability and rapid
adaptation.
  We introduce a holistic framework for continual learning under concept drift
that simulates realistic scenarios by evolving task distributions. As a
baseline, we consider Full Relearning (FR), in which the model is retrained
from scratch on newly labeled samples from the drifted distribution. While
effective, this approach incurs substantial annotation and computational
overhead. To address these limitations, we propose Adaptive Memory Realignment
(AMR), a lightweight alternative that equips rehearsal-based learners with a
drift-aware adaptation mechanism. AMR selectively removes outdated samples of
drifted classes from the replay buffer and repopulates it with a small number
of up-to-date instances, effectively realigning memory with the new
distribution. This targeted resampling matches the performance of FR while
reducing the need for labeled data and computation by orders of magnitude.
  To enable reproducible evaluation, we introduce four concept-drift variants
of standard vision benchmarks: Fashion-MNIST-CD, CIFAR10-CD, CIFAR100-CD, and
Tiny-ImageNet-CD, where previously seen classes reappear with shifted
representations. Comprehensive experiments on these datasets using several
rehearsal-based baselines show that AMR consistently counters concept drift,
maintaining high accuracy with minimal overhead. These results position AMR as
a scalable solution that reconciles stability and plasticity in non-stationary
continual learning environments.

</details>


### [25] [Improving Constrained Generation in Language Models via Self-Distilled Twisted Sequential Monte Carlo](https://arxiv.org/abs/2507.02315)
*Sooyeon Kim,Giung Nam,Juho Lee*

Main category: cs.LG

TL;DR: 通过自蒸馏迭代优化基础模型，解决约束文本生成中奖励信号稀疏的问题，显著提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 在约束文本生成中，目标分布集中在基础模型不太可能生成的输出上，导致学习困难，奖励信号稀疏且无信息。

Method: 采用自蒸馏方法迭代优化基础模型，逐步使其与目标对齐。

Result: 通过自蒸馏迭代优化，显著提升了生成质量。

Conclusion: 自蒸馏是一种有效的方法，可以解决约束文本生成中的奖励稀疏问题，并显著改善生成结果。

Abstract: Recent work has framed constrained text generation with autoregressive
language models as a probabilistic inference problem. Among these, Zhao et al.
(2024) introduced a promising approach based on twisted Sequential Monte Carlo,
which incorporates learned twist functions and twist-induced proposals to guide
the generation process. However, in constrained generation settings where the
target distribution concentrates on outputs that are unlikely under the base
model, learning becomes challenging due to sparse and uninformative reward
signals. We show that iteratively refining the base model through
self-distillation alleviates this issue by making the model progressively more
aligned with the target, leading to substantial gains in generation quality.

</details>


### [26] [Transformer-based EEG Decoding: A Survey](https://arxiv.org/abs/2507.02320)
*Haodong Zhang,Hongqi Li*

Main category: cs.LG

TL;DR: 该论文综述了Transformer模型在EEG解码中的最新应用，包括其架构演变、混合架构设计以及未来挑战和发展前景。


<details>
  <summary>Details</summary>
Motivation: 探索深度学习（尤其是Transformer）如何革新EEG信号解码领域，以提升脑机接口的性能和效率。

Method: 通过综述Transformer在EEG解码中的应用，分析其架构演变、与其他深度学习技术的结合以及定制化改进。

Result: 总结了Transformer在EEG解码中的优势、混合架构的设计以及当前研究的局限性。

Conclusion: Transformer在EEG解码中展现出巨大潜力，但仍需解决数据稀缺和模型泛化等挑战，未来研究应关注多模态融合和实时性优化。

Abstract: Electroencephalography (EEG) is one of the most common signals used to
capture the electrical activity of the brain, and the decoding of EEG, to
acquire the user intents, has been at the forefront of brain-computer/machine
interfaces (BCIs/BMIs) research. Compared to traditional EEG analysis methods
with machine learning, the advent of deep learning approaches have gradually
revolutionized the field by providing an end-to-end long-cascaded architecture,
which can learn more discriminative features automatically. Among these,
Transformer is renowned for its strong handling capability of sequential data
by the attention mechanism, and the application of Transformers in various EEG
processing tasks is increasingly prevalent. This article delves into a relevant
survey, summarizing the latest application of Transformer models in EEG
decoding since it appeared. The evolution of the model architecture is followed
to sort and organize the related advances, in which we first elucidate the
fundamentals of the Transformer that benefits EEG decoding and its direct
application. Then, the common hybrid architectures by integrating basic
Transformer with other deep learning techniques
(convolutional/recurrent/graph/spiking neural netwo-rks, generative adversarial
networks, diffusion models, etc.) is overviewed in detail. The research
advances of applying the modified intrinsic structures of customized
Transformer have also been introduced. Finally, the current challenges and
future development prospects in this rapidly evolving field are discussed. This
paper aims to help readers gain a clear understanding of the current state of
Transformer applications in EEG decoding and to provide valuable insights for
future research endeavors.

</details>


### [27] [DeltaSHAP: Explaining Prediction Evolutions in Online Patient Monitoring with Shapley Values](https://arxiv.org/abs/2507.02342)
*Changhun Kim,Yechan Mun,Sangchul Hahn,Eunho Yang*

Main category: cs.LG

TL;DR: DeltaSHAP是一种新型可解释人工智能算法，专为在线患者监测系统设计，解决了临床时间序列解释任务的独特需求。


<details>
  <summary>Details</summary>
Motivation: 在临床环境中，了解患者风险变化的原因对及时干预至关重要，但现有XAI方法无法满足临床时间序列解释的特殊需求。

Method: DeltaSHAP通过调整Shapley值以适应时间设置，准确捕捉特征联合效应，并仅使用实际观察到的特征组合来归因预测变化。

Result: 实验表明，DeltaSHAP在解释质量和计算效率上均优于现有XAI方法，在MIMIC-III基准测试中分别提升62%和减少33%时间。

Conclusion: DeltaSHAP是一种高效、实用的临床时间序列解释工具，适用于实时患者监测。

Abstract: This study proposes DeltaSHAP, a novel explainable artificial intelligence
(XAI) algorithm specifically designed for online patient monitoring systems. In
clinical environments, discovering the causes driving patient risk evolution is
critical for timely intervention, yet existing XAI methods fail to address the
unique requirements of clinical time series explanation tasks. To this end,
DeltaSHAP addresses three key clinical needs: explaining the changes in the
consecutive predictions rather than isolated prediction scores, providing both
magnitude and direction of feature attributions, and delivering these insights
in real time. By adapting Shapley values to temporal settings, our approach
accurately captures feature coalition effects. It further attributes prediction
changes using only the actually observed feature combinations, making it
efficient and practical for time-sensitive clinical applications. We also
introduce new evaluation metrics to evaluate the faithfulness of the
attributions for online time series, and demonstrate through experiments on
online patient monitoring tasks that DeltaSHAP outperforms state-of-the-art XAI
methods in both explanation quality as 62% and computational efficiency as 33%
time reduction on the MIMIC-III decompensation benchmark. We release our code
at https://github.com/AITRICS/DeltaSHAP.

</details>


### [28] [Offline Reinforcement Learning with Penalized Action Noise Injection](https://arxiv.org/abs/2507.02356)
*JunHyeok Oh,Byung-Jun Lee*

Main category: cs.LG

TL;DR: 论文提出了一种名为PANI的方法，通过注入噪声动作来提升离线强化学习的性能，同时减少计算需求。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习在环境交互成本高时具有实用性，但现有方法（如扩散模型）计算需求高，因此需要更高效的方法。

Method: 提出PANI方法，通过噪声注入动作覆盖整个动作空间，并根据噪声量进行惩罚。

Result: PANI在多种基准测试中表现出显著性能提升，且兼容现有算法。

Conclusion: PANI是一种简单高效的方法，能够显著提升离线强化学习的性能，同时避免高计算成本。

Abstract: Offline reinforcement learning (RL) optimizes a policy using only a fixed
dataset, making it a practical approach in scenarios where interaction with the
environment is costly. Due to this limitation, generalization ability is key to
improving the performance of offline RL algorithms, as demonstrated by recent
successes of offline RL with diffusion models. However, it remains questionable
whether such diffusion models are necessary for highly performing offline RL
algorithms, given their significant computational requirements during
inference. In this paper, we propose Penalized Action Noise Injection (PANI), a
method that simply enhances offline learning by utilizing noise-injected
actions to cover the entire action space, while penalizing according to the
amount of noise injected. This approach is inspired by how diffusion models
have worked in offline RL algorithms. We provide a theoretical foundation for
this method, showing that offline RL algorithms with such noise-injected
actions solve a modified Markov Decision Process (MDP), which we call the noisy
action MDP. PANI is compatible with a wide range of existing off-policy and
offline RL algorithms, and despite its simplicity, it demonstrates significant
performance improvements across various benchmarks.

</details>


### [29] [Deep Reinforcement Learning-Based DRAM Equalizer Parameter Optimization Using Latent Representations](https://arxiv.org/abs/2507.02365)
*Muhammad Usama,Dong Eui Chang*

Main category: cs.LG

TL;DR: 提出了一种基于数据驱动和强化学习的框架，用于高效优化高速DRAM系统中的信号完整性。


<details>
  <summary>Details</summary>
Motivation: 传统方法在优化高速DRAM系统中的信号完整性时计算量大或依赖模型。

Method: 使用学习的潜在信号表示进行信号完整性评估，并结合无模型的Advantage Actor-Critic强化学习代理进行参数优化。

Result: 在行业标准DRAM波形上，方法显著提升了眼图窗口面积：级联CTLE和DFE结构提升42.7%，仅DFE结构提升36.8%。

Conclusion: 该方法在性能、计算效率和泛化能力上优于现有技术，核心贡献包括高效的潜在信号完整性度量、无模型强化学习策略以及对复杂均衡器架构的验证。

Abstract: Equalizer parameter optimization for signal integrity in high-speed Dynamic
Random Access Memory systems is crucial but often computationally demanding or
model-reliant. This paper introduces a data-driven framework employing learned
latent signal representations for efficient signal integrity evaluation,
coupled with a model-free Advantage Actor-Critic reinforcement learning agent
for parameter optimization. The latent representation captures vital signal
integrity features, offering a fast alternative to direct eye diagram analysis
during optimization, while the reinforcement learning agent derives optimal
equalizer settings without explicit system models. Applied to industry-standard
Dynamic Random Access Memory waveforms, the method achieved significant
eye-opening window area improvements: 42.7\% for cascaded Continuous-Time
Linear Equalizer and Decision Feedback Equalizer structures, and 36.8\% for
Decision Feedback Equalizer-only configurations. These results demonstrate
superior performance, computational efficiency, and robust generalization
across diverse Dynamic Random Access Memory units compared to existing
techniques. Core contributions include an efficient latent signal integrity
metric for optimization, a robust model-free reinforcement learning strategy,
and validated superior performance for complex equalizer architectures.

</details>


### [30] [Improving Consistency in Vehicle Trajectory Prediction Through Preference Optimization](https://arxiv.org/abs/2507.02406)
*Caio Azevedo,Lina Achaji,Stefano Sabatini,Nicola Poerio,Grzegorz Bartyzel,Sascha Hornauer,Fabien Moutarde*

Main category: cs.LG

TL;DR: 通过偏好优化微调轨迹预测模型，显著提升场景一致性，同时几乎不影响预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习轨迹预测模型在复杂交互场景中难以捕捉代理间依赖关系，导致预测不一致。

Method: 利用偏好优化微调多智能体设置下的轨迹预测模型，输入自动计算的预测未来偏好排名。

Result: 在三个数据集上显著提升场景一致性，预测精度损失极小，且推理时不增加计算负担。

Conclusion: 偏好优化是提升轨迹预测模型在交互场景中表现的有效方法。

Abstract: Trajectory prediction is an essential step in the pipeline of an autonomous
vehicle. Inaccurate or inconsistent predictions regarding the movement of
agents in its surroundings lead to poorly planned maneuvers and potentially
dangerous situations for the end-user. Current state-of-the-art
deep-learning-based trajectory prediction models can achieve excellent accuracy
on public datasets. However, when used in more complex, interactive scenarios,
they often fail to capture important interdependencies between agents, leading
to inconsistent predictions among agents in the traffic scene. Inspired by the
efficacy of incorporating human preference into large language models, this
work fine-tunes trajectory prediction models in multi-agent settings using
preference optimization. By taking as input automatically calculated preference
rankings among predicted futures in the fine-tuning process, our
experiments--using state-of-the-art models on three separate datasets--show
that we are able to significantly improve scene consistency while minimally
sacrificing trajectory prediction accuracy and without adding any excess
computational requirements at inference time.

</details>


### [31] [S2FGL: Spatial Spectral Federated Graph Learning](https://arxiv.org/abs/2507.02409)
*Zihan Tan,Suyuan Huang,Guancheng Wan,Wenke Huang,He Li,Mang Ye*

Main category: cs.LG

TL;DR: S2FGL框架通过结合空间和频谱策略，解决了联邦图学习中子图间的信号传播问题，提升了全局模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究仅从结构角度处理子图联邦学习，忽略了图信号在空间和频谱域的传播问题，导致标签信号中断和频谱客户端漂移。

Method: 提出全局知识库缓解标签信号中断，并通过频率对齐解决频谱客户端漂移，形成S2FGL框架。

Result: 在多个数据集上的实验验证了S2FGL的优越性。

Conclusion: S2FGL有效解决了联邦图学习中的空间和频谱问题，提升了模型性能。

Abstract: Federated Graph Learning (FGL) combines the privacy-preserving capabilities
of federated learning (FL) with the strong graph modeling capability of Graph
Neural Networks (GNNs). Current research addresses subgraph-FL only from the
structural perspective, neglecting the propagation of graph signals on spatial
and spectral domains of the structure. From a spatial perspective, subgraph-FL
introduces edge disconnections between clients, leading to disruptions in label
signals and a degradation in the class knowledge of the global GNN. From a
spectral perspective, spectral heterogeneity causes inconsistencies in signal
frequencies across subgraphs, which makes local GNNs overfit the local signal
propagation schemes. As a result, spectral client drifts occur, undermining
global generalizability. To tackle the challenges, we propose a global
knowledge repository to mitigate label signal disruption and a frequency
alignment to address spectral client drifts. The combination of spatial and
spectral strategies forms our framework S2FGL. Extensive experiments on
multiple datasets demonstrate the superiority of S2FGL. The code is available
at https://github.com/Wonder7racer/S2FGL.git.

</details>


### [32] [Variational Kolmogorov-Arnold Network](https://arxiv.org/abs/2507.02466)
*Francesco Alesiani,Henrik Christiansen,Federico Errica*

Main category: cs.LG

TL;DR: InfinityKAN通过变分推理优化自适应学习无限基函数，扩展了Kolmogorov Arnold Networks（KANs）的适用性。


<details>
  <summary>Details</summary>
Motivation: KANs基于Kolmogorov-Arnold定理，但作为多层感知机（MLP）的替代方案时，其基函数数量的选择是临时的。本文旨在解决这一问题。

Method: 提出InfinityKAN，通过变分推理优化自适应学习无限基函数，并将超参数纳入学习过程。

Result: InfinityKAN扩展了KANs的潜在适用性。

Conclusion: 通过自适应学习基函数，InfinityKAN解决了KANs的关键限制，提升了其作为表示学习工具的实用性。

Abstract: Kolmogorov Arnold Networks (KANs) are an emerging architecture for building
machine learning models. KANs are based on the theoretical foundation of the
Kolmogorov-Arnold Theorem and its expansions, which provide an exact
representation of a multi-variate continuous bounded function as the
composition of a limited number of univariate continuous functions. While such
theoretical results are powerful, their use as a representation learning
alternative to a multi-layer perceptron (MLP) hinges on the ad-hoc choice of
the number of bases modeling each of the univariate functions. In this work, we
show how to address this problem by adaptively learning a potentially infinite
number of bases for each univariate function during training. We therefore
model the problem as a variational inference optimization problem. Our
proposal, called InfinityKAN, which uses backpropagation, extends the potential
applicability of KANs by treating an important hyperparameter as part of the
learning process.

</details>


### [33] [Online Conformal Prediction with Efficiency Guarantees](https://arxiv.org/abs/2507.02496)
*Vaidehi Srinivas*

Main category: cs.LG

TL;DR: 该论文研究了在线框架下的共形预测问题，目标是优化效率并实现目标覆盖率和最小化区间长度。在可交换序列中，算法能接近最优覆盖率和长度；而在任意序列中，算法必须在覆盖率和长度之间权衡。


<details>
  <summary>Details</summary>
Motivation: 解决在线共形预测问题，优化效率（区间长度）同时满足目标覆盖率。

Method: 提出算法处理任意和可交换序列，分析其覆盖率和效率的权衡。

Result: 在可交换序列中实现接近最优覆盖率和长度；在任意序列中，算法需在覆盖率和长度间权衡。

Conclusion: 可交换和任意序列下的算法表现存在差异，需设计不同策略以实现最优性能。

Abstract: We study the problem of conformal prediction in a novel online framework that
directly optimizes efficiency. In our problem, we are given a target
miscoverage rate $\alpha > 0$, and a time horizon $T$. On each day $t \le T$ an
algorithm must output an interval $I_t \subseteq [0, 1]$, then a point $y_t \in
[0, 1]$ is revealed. The goal of the algorithm is to achieve coverage, that is,
$y_t \in I_t$ on (close to) a $(1 - \alpha)$-fraction of days, while
maintaining efficiency, that is, minimizing the average volume (length) of the
intervals played. This problem is an online analogue to the problem of
constructing efficient confidence intervals.
  We study this problem over arbitrary and exchangeable (random order) input
sequences. For exchangeable sequences, we show that it is possible to construct
intervals that achieve coverage $(1 - \alpha) - o(1)$, while having length
upper bounded by the best fixed interval that achieves coverage in hindsight.
For arbitrary sequences however, we show that any algorithm that achieves a
$\mu$-approximation in average length compared to the best fixed interval
achieving coverage in hindsight, must make a multiplicative factor more
mistakes than $\alpha T$, where the multiplicative factor depends on $\mu$ and
the aspect ratio of the problem. Our main algorithmic result is a matching
algorithm that can recover all Pareto-optimal settings of $\mu$ and number of
mistakes. Furthermore, our algorithm is deterministic and therefore robust to
an adaptive adversary.
  This gap between the exchangeable and arbitrary settings is in contrast to
the classical online learning problem. In fact, we show that no single
algorithm can simultaneously be Pareto-optimal for arbitrary sequences and
optimal for exchangeable sequences. On the algorithmic side, we give an
algorithm that achieves the near-optimal tradeoff between the two cases.

</details>


### [34] [Continual Gradient Low-Rank Projection Fine-Tuning for LLMs](https://arxiv.org/abs/2507.02503)
*Chenxu Wang,Yilin Lyu,Zicheng Sun,Liping Jing*

Main category: cs.LG

TL;DR: GORP是一种新的训练策略，通过结合全参数和低秩参数，在统一低秩梯度子空间中联合更新，解决了LLMs持续微调中效率与表达能力之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 解决低秩适应（LoRA）在持续学习中因低秩特性和显式参数约束导致的学习新任务和知识迁移能力受限的问题。

Method: 提出GORP策略，结合全参数和低秩参数，在统一低秩梯度子空间中联合更新，扩展优化空间并保持效率。

Result: 在持续学习基准测试中，GORP表现优于现有最先进方法。

Conclusion: GORP通过结合全参数和低秩参数，有效解决了持续学习中的效率与表达能力问题，并减轻了灾难性遗忘。

Abstract: Continual fine-tuning of Large Language Models (LLMs) is hampered by the
trade-off between efficiency and expressiveness. Low-Rank Adaptation (LoRA)
offers efficiency but constrains the model's ability to learn new tasks and
transfer knowledge due to its low-rank nature and reliance on explicit
parameter constraints. We propose GORP (Gradient LOw Rank Projection) for
Continual Learning, a novel training strategy that overcomes these limitations
by synergistically combining full and low-rank parameters and jointly updating
within a unified low-rank gradient subspace. GORP expands the optimization
space while preserving efficiency and mitigating catastrophic forgetting.
Extensive experiments on continual learning benchmarks demonstrate GORP's
superior performance compared to existing state-of-the-art approaches. Code is
available at https://github.com/Wcxwcxw/GORP.

</details>


### [35] [TFOC-Net: A Short-time Fourier Transform-based Deep Learning Approach for Enhancing Cross-Subject Motor Imagery Classification](https://arxiv.org/abs/2507.02510)
*Ahmed G. Habashi,Ahmed M. Azab,Seif Eldawlatly,Gamal M. Aly*

Main category: cs.LG

TL;DR: 论文提出了一种优化预处理和深度学习的跨被试运动想象分类方法，显著提升了分类性能。


<details>
  <summary>Details</summary>
Motivation: 由于不同个体间EEG模式的显著差异，跨被试运动想象分类在脑机接口中具有挑战性，限制了无校准BCI的实际应用。

Method: 采用STFT变换的EEG数据直接分类，优化STFT参数，并在CNN训练中使用平衡批处理策略。

Result: 在多个数据集上表现优异，分类准确率分别为67.60%（IV-1）、65.96%（IV-2A）和80.22%（IV-2B），优于现有技术。

Conclusion: 该方法为无校准运动想象分类设定了新基准，并贡献了一个开放数据集以推动研究。

Abstract: Cross-subject motor imagery (CS-MI) classification in brain-computer
interfaces (BCIs) is a challenging task due to the significant variability in
Electroencephalography (EEG) patterns across different individuals. This
variability often results in lower classification accuracy compared to
subject-specific models, presenting a major barrier to developing
calibration-free BCIs suitable for real-world applications. In this paper, we
introduce a novel approach that significantly enhances cross-subject MI
classification performance through optimized preprocessing and deep learning
techniques. Our approach involves direct classification of Short-Time Fourier
Transform (STFT)-transformed EEG data, optimized STFT parameters, and a
balanced batching strategy during training of a Convolutional Neural Network
(CNN). This approach is uniquely validated across four different datasets,
including three widely-used benchmark datasets leading to substantial
improvements in cross-subject classification, achieving 67.60% on the BCI
Competition IV Dataset 1 (IV-1), 65.96% on Dataset 2A (IV-2A), and 80.22% on
Dataset 2B (IV-2B), outperforming state-of-the-art techniques. Additionally, we
systematically investigate the classification performance using MI windows
ranging from the full 4-second window to 1-second windows. These results
establish a new benchmark for generalizable, calibration-free MI classification
in addition to contributing a robust open-access dataset to advance research in
this domain.

</details>


### [36] [OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device Speculative Decoding](https://arxiv.org/abs/2507.02659)
*Ramchalam Kinattinkara Ramakrishnan,Zhaocong Yuan,Shaojie Zhuo,Chen Feng,Yicheng Lin,Chenzheng Su,Xiaopeng Zhang*

Main category: cs.LG

TL;DR: OmniDraft是一个统一的框架，使单个草稿模型能与任何目标模型配合使用，并通过在线学习和自适应技术提升解码速度。


<details>
  <summary>Details</summary>
Motivation: 解决在线部署中草稿模型与目标模型不兼容的问题，以及随时间推移对延迟改进的需求。

Method: 引入在线n-gram缓存和混合蒸馏微调，结合自适应草拟技术。

Result: OmniDraft使单个Llama-68M模型能与多种目标模型配合，解码速度提升1.5-2倍。

Conclusion: OmniDraft适用于设备端LLM应用，支持“一个草稿模型适配所有”的范式。

Abstract: Speculative decoding generally dictates having a small, efficient draft model
that is either pretrained or distilled offline to a particular target model
series, for instance, Llama or Qwen models. However, within online deployment
settings, there are two major challenges: 1) usage of a target model that is
incompatible with the draft model; 2) expectation of latency improvements over
usage and time. In this work, we propose OmniDraft, a unified framework that
enables a single draft model to operate with any target model and adapt
dynamically to user data. We introduce an online n-gram cache with hybrid
distillation fine-tuning to address the cross-vocabulary mismatch across draft
and target models; and further improve decoding speed by leveraging adaptive
drafting techniques. OmniDraft is particularly suitable for on-device LLM
applications where model cost, efficiency and user customization are the major
points of contention. This further highlights the need to tackle the above
challenges and motivates the \textit{``one drafter for all''} paradigm. We
showcase the proficiency of the OmniDraft framework by performing online
learning on math reasoning, coding and text generation tasks. Notably,
OmniDraft enables a single Llama-68M model to pair with various target models
including Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;
and additionally provides up to 1.5-2x speedup.

</details>


### [37] [RetrySQL: text-to-SQL training with retry data for self-correcting query generation](https://arxiv.org/abs/2507.02529)
*Alicja Rączkowska,Riccardo Belluzzo,Piotr Zieliński,Joanna Baran,Paweł Olszewski*

Main category: cs.LG

TL;DR: RetrySQL提出了一种通过自校正生成策略改进文本到SQL任务的方法，利用错误和修正步骤的数据进行预训练，提升了执行准确性。


<details>
  <summary>Details</summary>
Motivation: 现有文本到SQL任务的研究多依赖黑盒语言模型，缺乏针对SQL的生成模型研究，且自校正生成策略的应用尚未探索。

Method: 通过准备参考SQL查询的推理步骤并人为引入错误，生成包含错误和修正步骤的数据，用于持续预训练开源编码模型。

Result: RetrySQL在整体和挑战性执行准确性指标上提升了4个百分点，且全参数预训练是学习自校正行为的必要条件。

Conclusion: RetrySQL证明了自校正行为在文本到SQL任务中的可学习性，为SQL导向的语言模型提供了一种新颖的改进生成准确性的方法。

Abstract: The text-to-SQL task is an active challenge in Natural Language Processing.
Many existing solutions focus on using black-box language models extended with
specialized components within customized end-to-end text-to-SQL pipelines.
While these solutions use both closed-source proprietary language models and
coding-oriented open-source models, there is a lack of research regarding
SQL-specific generative models. At the same time, recent advancements in
self-correcting generation strategies show promise for improving the
capabilities of existing architectures. The application of these concepts to
the text-to-SQL task remains unexplored. In this paper, we introduce RetrySQL,
a new approach to training text-to-SQL generation models. We prepare reasoning
steps for reference SQL queries and then corrupt them to create retry data that
contains both incorrect and corrected steps, divided with a special token. We
continuously pre-train an open-source coding model with this data and
demonstrate that retry steps yield an improvement of up to 4 percentage points
in both overall and challenging execution accuracy metrics, compared to
pre-training without retry data. Additionally, we confirm that supervised
fine-tuning with LoRA is ineffective for learning from retry data and that
full-parameter pre-training is a necessary requirement for that task. We
showcase that the self-correcting behavior is learned by the model and the
increase in downstream accuracy metrics is a result of this additional skill.
Finally, we incorporate RetrySQL-trained models into the full text-to-SQL
pipeline and showcase that they are competitive in terms of execution accuracy
with proprietary models that contain orders of magnitude more parameters.
RetrySQL demonstrates that self-correction can be learned in the text-to-SQL
task and provides a novel way of improving generation accuracy for SQL-oriented
language models.

</details>


### [38] [Position: A Theory of Deep Learning Must Include Compositional Sparsity](https://arxiv.org/abs/2507.02550)
*David A. Danhofer,Davide D'Ascenzo,Rafael Dubach,Tomaso Poggio*

Main category: cs.LG

TL;DR: 深度神经网络（DNNs）通过利用目标函数的组合稀疏结构取得成功，这种结构普遍存在于高效可计算函数中。


<details>
  <summary>Details</summary>
Motivation: 探讨DNNs成功背后的基本原理，尤其是其如何利用组合稀疏性。

Method: 提出DNNs能够利用目标函数的组合稀疏结构，即函数由少量低维子函数组成。

Result: 组合稀疏性是DNNs成功的关键，且普遍存在于高效可计算函数中。

Conclusion: 深入理解组合稀疏性对完善深度学习理论及人工智能发展至关重要。

Abstract: Overparametrized Deep Neural Networks (DNNs) have demonstrated remarkable
success in a wide variety of domains too high-dimensional for classical shallow
networks subject to the curse of dimensionality. However, open questions about
fundamental principles, that govern the learning dynamics of DNNs, remain. In
this position paper we argue that it is the ability of DNNs to exploit the
compositionally sparse structure of the target function driving their success.
As such, DNNs can leverage the property that most practically relevant
functions can be composed from a small set of constituent functions, each of
which relies only on a low-dimensional subset of all inputs. We show that this
property is shared by all efficiently Turing-computable functions and is
therefore highly likely present in all current learning problems. While some
promising theoretical insights on questions concerned with approximation and
generalization exist in the setting of compositionally sparse functions,
several important questions on the learnability and optimization of DNNs
remain. Completing the picture of the role of compositional sparsity in deep
learning is essential to a comprehensive theory of artificial, and even
general, intelligence.

</details>


### [39] [ExPO: Unlocking Hard Reasoning with Self-Explanation-Guided Reinforcement Learning](https://arxiv.org/abs/2507.02834)
*Ruiyang Zhou,Shuozhe Li,Amy Zhang,Liu Leqi*

Main category: cs.LG

TL;DR: 论文提出了一种名为ExPO的新方法，通过结合真实答案生成高质量样本，解决了传统强化学习后训练在早期阶段和复杂推理任务中的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习后训练方法依赖模型初始能力生成正样本，难以解决模型初始失败的问题，尤其是在早期训练和复杂推理任务中。

Method: 提出Self-Explanation Policy Optimization (ExPO)，通过结合真实答案生成符合当前策略且能提高正确预测概率的样本。

Result: ExPO在推理基准测试中提升了学习效率和最终性能，尤其在MATH level-5等挑战性任务中优于基于专家示范的方法。

Conclusion: ExPO通过高效探索和高质量样本生成，显著提升了模型在复杂推理任务中的表现。

Abstract: Recent advances in large language models have been driven by reinforcement
learning (RL)-style post-training, which improves reasoning by optimizing model
outputs based on reward or preference signals. GRPO-style approaches implement
this by using self-generated samples labeled by an outcome-based verifier.
However, these methods depend heavily on the model's initial ability to produce
positive samples. They primarily refine what the model already knows
(distribution sharpening) rather than enabling the model to solve problems
where it initially fails. This limitation is especially problematic in
early-stage RL training and on challenging reasoning tasks, where positive
samples are unlikely to be generated. To unlock reasoning ability in such
settings, the model must explore new reasoning trajectories beyond its current
output distribution. Such exploration requires access to sufficiently good
positive samples to guide the learning. While expert demonstrations seem like a
natural solution, we find that they are often ineffective in RL post-training.
Instead, we identify two key properties of effective positive samples: they
should (1) be likely under the current policy, and (2) increase the model's
likelihood of predicting the correct answer. Based on these insights, we
propose $\textbf{Self-Explanation Policy Optimization (ExPO)}$-a simple and
modular framework that generates such samples by conditioning on the
ground-truth answer. ExPO enables efficient exploration and guides the model to
produce reasoning trajectories more aligned with its policy than expert-written
CoTs, while ensuring higher quality than its own (incorrect) samples.
Experiments show that ExPO improves both learning efficiency and final
performance on reasoning benchmarks, surpassing expert-demonstration-based
methods in challenging settings such as MATH level-5, where the model initially
struggles the most.

</details>


### [40] [Transformers Don't Need LayerNorm at Inference Time: Scaling LayerNorm Removal to GPT-2 XL and the Implications for Mechanistic Interpretability](https://arxiv.org/abs/2507.02559)
*Luca Baroni,Galvin Khara,Joachim Schaeffer,Marat Subkhankulov,Stefan Heimersheim*

Main category: cs.LG

TL;DR: 研究发现层归一化（LN）在GPT-2模型中可以完全移除，仅带来轻微性能损失，且不影响语言建模能力。


<details>
  <summary>Details</summary>
Motivation: 探讨LN在推理阶段的作用及其对模型可解释性的影响。

Method: 移除GPT-2模型中的所有LN层，并通过验证损失和可解释性技术评估效果。

Result: 移除LN后，验证损失仅小幅增加（如GPT-2 XL的交叉熵损失+0.03），且可解释性技术如直接logit归因更精确。

Conclusion: LN在语言建模中作用有限，移除LN的模型有助于更精确的可解释性研究。

Abstract: Layer-wise normalization (LN) is an essential component of virtually all
transformer-based large language models. While its effects on training
stability are well documented, its role at inference time is poorly understood.
Additionally, LN layers hinder mechanistic interpretability by introducing
additional nonlinearities and increasing the interconnectedness of individual
model components. Here, we show that all LN layers can be removed from every
GPT-2 model with only a small increase in validation loss (e.g. +0.03
cross-entropy loss for GPT-2 XL). Thus, LN cannot play a substantial role in
language modeling. We find that the amount of fine-tuning data needed for LN
removal grows sublinearly with model parameters, suggesting scaling to larger
models is feasible. We release a suite of LN-free GPT-2 models on Hugging Face.
Furthermore, we test interpretability techniques on LN-free models. Direct
logit attribution now gives the exact direct effect of individual components,
while the accuracy of attribution patching does not significantly improve. We
also confirm that GPT-2's "confidence neurons" are inactive in the LN-free
models. Our work clarifies the role of LN layers in language modeling, showing
that GPT-2-class models can function without LN layers. We hope that our
LN-free analogs of the GPT-2 family of models will enable more precise
interpretability research and improve our understanding of language models.

</details>


### [41] [Scalable Interconnect Learning in Boolean Networks](https://arxiv.org/abs/2507.02585)
*Fabian Kresse,Emily Yu,Christoph H. Lampert*

Main category: cs.LG

TL;DR: 论文扩展了可微布尔逻辑网络（DBNs），引入可训练、可微的互联结构，支持更宽的输入层，并提出两阶段剪枝方法以减小模型规模。


<details>
  <summary>Details</summary>
Motivation: 提升DBNs在资源受限硬件上的扩展性和效率，同时保持其准确性。

Method: 1. 引入可训练、可微的互联结构；2. 提出两阶段剪枝：基于SAT的逻辑等价剪枝和基于相似性的数据驱动剪枝。

Result: 扩展后的DBNs支持更宽的输入层，剪枝方法在压缩与准确性之间取得更好平衡。

Conclusion: 该方法显著提升了DBNs的扩展性和效率，同时保持了高准确性。

Abstract: Learned Differentiable Boolean Logic Networks (DBNs) already deliver
efficient inference on resource-constrained hardware. We extend them with a
trainable, differentiable interconnect whose parameter count remains constant
as input width grows, allowing DBNs to scale to far wider layers than earlier
learnable-interconnect designs while preserving their advantageous accuracy. To
further reduce model size, we propose two complementary pruning stages: an
SAT-based logic equivalence pass that removes redundant gates without affecting
performance, and a similarity-based, data-driven pass that outperforms a
magnitude-style greedy baseline and offers a superior compression-accuracy
trade-off.

</details>


### [42] [Padé Approximant Neural Networks for Enhanced Electric Motor Fault Diagnosis Using Vibration and Acoustic Data](https://arxiv.org/abs/2507.02599)
*Sertac Kilickaya,Levent Eren*

Main category: cs.LG

TL;DR: 研究旨在通过Padé Approximant Neuron (PAON)模型提升感应电机故障诊断性能，比较PadéNets与传统CNN和Self-ONNs的效果。


<details>
  <summary>Details</summary>
Motivation: 传统方法（如加速度计和麦克风）在电机状态监测中存在局限，非线性神经元架构的深度学习模型有望提升诊断性能。

Method: 比较一维CNN、Self-ONNs和PadéNets在振动和声学数据上的诊断能力，使用公开数据集测试。

Result: PadéNets表现最优，诊断准确率高达99.96%（加速度计1）和98.33%（声学传感器）。

Conclusion: PadéNets的非线性增强和兼容无界激活函数显著提升了故障诊断性能。

Abstract: Purpose: The primary aim of this study is to enhance fault diagnosis in
induction machines by leveraging the Pad\'e Approximant Neuron (PAON) model.
While accelerometers and microphones are standard in motor condition
monitoring, deep learning models with nonlinear neuron architectures offer
promising improvements in diagnostic performance. This research addresses the
question: Can Pad\'e Approximant Neural Networks (Pad\'eNets) outperform
conventional Convolutional Neural Networks (CNNs) and Self-Organized
Operational Neural Networks (Self-ONNs) in diagnosing electrical and mechanical
faults using vibration and acoustic data?
  Methods: We evaluate and compare the diagnostic capabilities of three deep
learning architectures: one-dimensional CNNs, Self-ONNs, and Pad\'eNets. These
models are tested on the University of Ottawa's publicly available
constant-speed induction motor datasets, which include both vibration and
acoustic sensor data. The Pad\'eNet model is designed to introduce enhanced
nonlinearity and is compatible with unbounded activation functions such as
Leaky ReLU.
  Results and Conclusion: Pad\'eNets consistently outperformed the baseline
models, achieving diagnostic accuracies of 99.96%, 98.26%, 97.61%, and 98.33%
for accelerometers 1, 2, 3, and the acoustic sensor, respectively. The enhanced
nonlinearity of Pad\'eNets, together with their compatibility with unbounded
activation functions, significantly improves fault diagnosis performance in
induction motor condition monitoring.

</details>


### [43] [Lost in Latent Space: An Empirical Study of Latent Diffusion Models for Physics Emulation](https://arxiv.org/abs/2507.02608)
*François Rozet,Ruben Ohana,Michael McCabe,Gilles Louppe,François Lanusse,Shirley Ho*

Main category: cs.LG

TL;DR: 研究表明，通过潜在空间生成扩散模型可以有效降低计算成本，且精度对压缩率（高达1000倍）表现出惊人的鲁棒性。扩散模型在精度和多样性上优于非生成模型。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在推理时的高计算成本限制了其作为快速物理模拟器的应用，因此研究是否可以通过潜在空间生成来降低成本。

Method: 在潜在空间而非像素空间生成扩散模型，并测试其在动力学系统模拟中的有效性。

Result: 潜在空间模拟的精度对高压缩率（高达1000倍）表现出鲁棒性，且扩散模型在精度和多样性上优于非生成模型。

Conclusion: 潜在空间生成扩散模型是一种高效且准确的物理模拟方法，设计选择（如架构和优化器）对其训练至关重要。

Abstract: The steep computational cost of diffusion models at inference hinders their
use as fast physics emulators. In the context of image and video generation,
this computational drawback has been addressed by generating in the latent
space of an autoencoder instead of the pixel space. In this work, we
investigate whether a similar strategy can be effectively applied to the
emulation of dynamical systems and at what cost. We find that the accuracy of
latent-space emulation is surprisingly robust to a wide range of compression
rates (up to 1000x). We also show that diffusion-based emulators are
consistently more accurate than non-generative counterparts and compensate for
uncertainty in their predictions with greater diversity. Finally, we cover
practical design choices, spanning from architectures to optimizers, that we
found critical to train latent-space emulators.

</details>


### [44] [L-VAE: Variational Auto-Encoder with Learnable Beta for Disentangled Representation](https://arxiv.org/abs/2507.02619)
*Hazal Mogultay Ozcan,Sinan Kalkan,Fatos T. Yarman-Vural*

Main category: cs.LG

TL;DR: L-VAE是一种新型变分自编码器，通过学习损失函数的超参数和模型参数，动态平衡解耦和重构损失，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决β-VAE中超参数η需经验调整的问题，实现动态权衡解耦和重构损失。

Method: 提出L-VAE模型，学习损失项权重和模型参数，并添加正则化项避免偏置。

Result: 在多个数据集上表现最佳或次佳，成功解耦CelebA中的面部属性。

Conclusion: L-VAE有效平衡重构和解耦，性能优于现有模型。

Abstract: In this paper, we propose a novel model called Learnable VAE (L-VAE), which
learns a disentangled representation together with the hyperparameters of the
cost function. L-VAE can be considered as an extension of \b{eta}-VAE, wherein
the hyperparameter, \b{eta}, is empirically adjusted. L-VAE mitigates the
limitations of \b{eta}-VAE by learning the relative weights of the terms in the
loss function to control the dynamic trade-off between disentanglement and
reconstruction losses. In the proposed model, the weight of the loss terms and
the parameters of the model architecture are learned concurrently. An
additional regularization term is added to the loss function to prevent bias
towards either reconstruction or disentanglement losses. Experimental analyses
show that the proposed L-VAE finds an effective balance between reconstruction
fidelity and disentangling the latent dimensions. Comparisons of the proposed
L-VAE against \b{eta}-VAE, VAE, ControlVAE, DynamicVAE, and {\sigma}-VAE on
datasets, such as dSprites, MPI3D-complex, Falcor3D, and Isaac3D reveals that
L-VAE consistently provides the best or the second best performances measured
by a set of disentanglement metrics. Moreover, qualitative experiments on
CelebA dataset, confirm the success of the L-VAE model for disentangling the
facial attributes.

</details>


### [45] [A Matrix Variational Auto-Encoder for Variant Effect Prediction in Pharmacogenes](https://arxiv.org/abs/2507.02624)
*Antoine Honoré,Borja Rodríguez Gálvez,Yoomi Park,Yitian Zhou,Volker M. Lauschke,Ming Xiao*

Main category: cs.LG

TL;DR: 提出了一种基于Transformer的矩阵变分自编码器（matVAE），用于评估蛋白质变体的功能影响，并在DMS数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖多序列比对（MSA），但某些药物基因进化压力低，DMS数据集提供了更直接的适应性评分。

Method: 使用matVAE结合结构化先验，并在33个DMS数据集上评估性能，同时比较了基于MSA和DMS训练的模型。

Result: matVAE-MSA在零样本预测中优于DeepSequence，且计算效率更高；结合AlphaFold结构后性能进一步提升。

Conclusion: DMS数据集有望替代MSA，未来需进一步开发DMS数据集以提升变体效应预测能力。

Abstract: Variant effect predictors (VEPs) aim to assess the functional impact of
protein variants, traditionally relying on multiple sequence alignments (MSAs).
This approach assumes that naturally occurring variants are fit, an assumption
challenged by pharmacogenomics, where some pharmacogenes experience low
evolutionary pressure. Deep mutational scanning (DMS) datasets provide an
alternative by offering quantitative fitness scores for variants. In this work,
we propose a transformer-based matrix variational auto-encoder (matVAE) with a
structured prior and evaluate its performance on 33 DMS datasets corresponding
to 26 drug target and ADME proteins from the ProteinGym benchmark. Our model
trained on MSAs (matVAE-MSA) outperforms the state-of-the-art DeepSequence
model in zero-shot prediction on DMS datasets, despite using an order of
magnitude fewer parameters and requiring less computation at inference time. We
also compare matVAE-MSA to matENC-DMS, a model of similar capacity trained on
DMS data, and find that the latter performs better on supervised prediction
tasks. Additionally, incorporating AlphaFold-generated structures into our
transformer model further improves performance, achieving results comparable to
DeepSequence trained on MSAs and finetuned on DMS. These findings highlight the
potential of DMS datasets to replace MSAs without significant loss in
predictive performance, motivating further development of DMS datasets and
exploration of their relationships to enhance variant effect prediction.

</details>


### [46] [Medical Data Pecking: A Context-Aware Approach for Automated Quality Evaluation of Structured Medical Data](https://arxiv.org/abs/2507.02628)
*Irena Girshovitz,Atai Ambus,Moni Shahar,Ran Gilad-Bachrach*

Main category: cs.LG

TL;DR: 论文提出了一种名为Medical Data Pecking的方法，通过结合软件工程的单元测试和覆盖率概念，评估电子健康记录（EHR）数据的质量，并开发了工具MDPT来自动生成测试并识别数据问题。


<details>
  <summary>Details</summary>
Motivation: EHR数据在流行病学研究和AI训练中应用广泛，但其质量问题和现有评估方法的不足影响了研究结果的可靠性。

Method: 采用软件工程的单元测试和覆盖率概念，开发了MDPT工具，包括自动测试生成器和数据测试框架，用于识别数据质量问题。

Result: 在三个数据集上测试，MDPT生成了55-73个测试，成功识别了20-43个数据问题，并分析了LLM生成测试的准确性和参考基础。

Conclusion: 该方法通过结合外部医学知识，提升了数据质量测试的上下文敏感性，为未来改进数据模态和基础方法奠定了基础。

Abstract: Background: The use of Electronic Health Records (EHRs) for epidemiological
studies and artificial intelligence (AI) training is increasing rapidly. The
reliability of the results depends on the accuracy and completeness of EHR
data. However, EHR data often contain significant quality issues, including
misrepresentations of subpopulations, biases, and systematic errors, as they
are primarily collected for clinical and billing purposes. Existing quality
assessment methods remain insufficient, lacking systematic procedures to assess
data fitness for research.
  Methods: We present the Medical Data Pecking approach, which adapts unit
testing and coverage concepts from software engineering to identify data
quality concerns. We demonstrate our approach using the Medical Data Pecking
Tool (MDPT), which consists of two main components: (1) an automated test
generator that uses large language models and grounding techniques to create a
test suite from data and study descriptions, and (2) a data testing framework
that executes these tests, reporting potential errors and coverage.
  Results: We evaluated MDPT on three datasets: All of Us (AoU), MIMIC-III, and
SyntheticMass, generating 55-73 tests per cohort across four conditions. These
tests correctly identified 20-43 non-aligned or non-conforming data issues. We
present a detailed analysis of the LLM-generated test suites in terms of
reference grounding and value accuracy.
  Conclusion: Our approach incorporates external medical knowledge to enable
context-sensitive data quality testing as part of the data analysis workflow to
improve the validity of its outcomes. Our approach tackles these challenges
from a quality assurance perspective, laying the foundation for further
development such as additional data modalities and improved grounding methods.

</details>


### [47] [High-Order Deep Meta-Learning with Category-Theoretic Interpretation](https://arxiv.org/abs/2507.02634)
*David H. Mguni*

Main category: cs.LG

TL;DR: 提出了一种分层深度学习框架，通过递归高阶元学习使神经网络能够构建、解决并泛化任务层次结构。


<details>
  <summary>Details</summary>
Motivation: 解决传统机器学习依赖人类生成数据的局限性，通过虚拟任务生成和软约束学习实现自主任务生成与泛化。

Method: 采用生成机制创建虚拟任务，通过元学习器迭代优化约束区域，结合范畴论视角构建分层学习结构。

Result: 框架能自主生成任务并优化学习过程，支持抽象化和知识迁移，提升泛化能力。

Conclusion: 该架构有望推动机器学习向通用人工智能发展，实现自主任务生成与解决。

Abstract: We introduce a new hierarchical deep learning framework for recursive
higher-order meta-learning that enables neural networks (NNs) to construct,
solve, and generalise across hierarchies of tasks. Central to this approach is
a generative mechanism that creates \emph{virtual tasks} -- synthetic problem
instances designed to enable the meta-learner to learn \emph{soft constraints}
and unknown generalisable rules across related tasks. Crucially, this enables
the framework to generate its own informative, task-grounded datasets thereby
freeing machine learning (ML) training from the limitations of relying entirely
on human-generated data. By actively exploring the virtual point landscape and
seeking out tasks lower-level learners find difficult, the meta-learner
iteratively refines constraint regions. This enhances inductive biases,
regularises the adaptation process, and produces novel, unanticipated tasks and
constraints required for generalisation. Each meta-level of the hierarchy
corresponds to a progressively abstracted generalisation of problems solved at
lower levels, enabling a structured and interpretable learning progression. By
interpreting meta-learners as category-theoretic \emph{functors} that generate
and condition a hierarchy of subordinate learners, we establish a compositional
structure that supports abstraction and knowledge transfer across progressively
generalised tasks. The category-theoretic perspective unifies existing
meta-learning models and reveals how learning processes can be transformed and
compared through functorial relationships, while offering practical design
principles for structuring meta-learning. We speculate this architecture may
underpin the next generation of NNs capable of autonomously generating novel,
instructive tasks and their solutions, thereby advancing ML towards general
artificial intelligence.

</details>


### [48] [On Efficient Bayesian Exploration in Model-Based Reinforcement Learning](https://arxiv.org/abs/2507.02639)
*Alberto Caron,Chris Hicks,Vasilios Mavroudis*

Main category: cs.LG

TL;DR: 本文提出了一种基于信息论的内在动机方法，针对强化学习中的数据高效探索问题，通过目标化认知不确定性而非环境固有噪声，证明了这些奖励信号的自然收敛性，并提出了实际可用的近似方法。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习中数据高效探索的挑战，特别是针对稀疏奖励或纯探索任务。

Method: 提出了一种基于信息论的探索奖励方法，结合稀疏变分高斯过程、深度核和深度集成模型，并设计了一个整合模型规划与信息论奖励的框架PTS-BE。

Result: PTS-BE在稀疏奖励或纯探索任务中显著优于其他基线方法。

Conclusion: 通过理论分析和实验验证，证明了基于信息论的探索奖励方法的有效性，并展示了PTS-BE框架的优越性。

Abstract: In this work, we address the challenge of data-efficient exploration in
reinforcement learning by examining existing principled, information-theoretic
approaches to intrinsic motivation. Specifically, we focus on a class of
exploration bonuses that targets epistemic uncertainty rather than the
aleatoric noise inherent in the environment. We prove that these bonuses
naturally signal epistemic information gains and converge to zero once the
agent becomes sufficiently certain about the environment's dynamics and
rewards, thereby aligning exploration with genuine knowledge gaps. Our analysis
provides formal guarantees for IG-based approaches, which previously lacked
theoretical grounding. To enable practical use, we also discuss tractable
approximations via sparse variational Gaussian Processes, Deep Kernels and Deep
Ensemble models. We then outline a general framework - Predictive Trajectory
Sampling with Bayesian Exploration (PTS-BE) - which integrates model-based
planning with information-theoretic bonuses to achieve sample-efficient deep
exploration. We empirically demonstrate that PTS-BE substantially outperforms
other baselines across a variety of environments characterized by sparse
rewards and/or purely exploratory tasks.

</details>


### [49] [Fair Deepfake Detectors Can Generalize](https://arxiv.org/abs/2507.02645)
*Harry Cheng,Ming-Hui Liu,Yangyang Guo,Tianyi Wang,Liqiang Nie,Mohan Kankanhalli*

Main category: cs.LG

TL;DR: 论文提出了一种名为DAID的框架，通过因果分析揭示公平性与泛化能力的关系，并通过数据重平衡和特征聚合提升检测模型的性能。


<details>
  <summary>Details</summary>
Motivation: 解决深度伪造检测模型在泛化能力和人口群体公平性之间的冲突问题。

Method: 提出DAID框架，包括人口感知数据重平衡和人口无关特征聚合。

Result: 在三个跨域基准测试中，DAID在公平性和泛化能力上均优于现有检测器。

Conclusion: DAID通过因果分析和干预措施，有效解决了公平性与泛化能力的冲突。

Abstract: Deepfake detection models face two critical challenges: generalization to
unseen manipulations and demographic fairness among population groups. However,
existing approaches often demonstrate that these two objectives are inherently
conflicting, revealing a trade-off between them. In this paper, we, for the
first time, uncover and formally define a causal relationship between fairness
and generalization. Building on the back-door adjustment, we show that
controlling for confounders (data distribution and model capacity) enables
improved generalization via fairness interventions. Motivated by this insight,
we propose Demographic Attribute-insensitive Intervention Detection (DAID), a
plug-and-play framework composed of: i) Demographic-aware data rebalancing,
which employs inverse-propensity weighting and subgroup-wise feature
normalization to neutralize distributional biases; and ii) Demographic-agnostic
feature aggregation, which uses a novel alignment loss to suppress
sensitive-attribute signals. Across three cross-domain benchmarks, DAID
consistently achieves superior performance in both fairness and generalization
compared to several state-of-the-art detectors, validating both its theoretical
foundation and practical effectiveness.

</details>


### [50] [Guided Generation for Developable Antibodies](https://arxiv.org/abs/2507.02670)
*Siqi Zhao,Joshua Moller,Porfi Quintero-Cadena,Lood van Niekerk*

Main category: cs.LG

TL;DR: 提出了一种基于离散扩散模型的抗体序列优化方法，结合SVDD模块提升可开发性。


<details>
  <summary>Details</summary>
Motivation: 抗体治疗需要高亲和力、可制造性、稳定性和安全性，这些统称为可开发性。现有方法难以同时满足这些需求。

Method: 使用自然抗体序列和临床抗体数据训练离散扩散模型，引入SVDD模块引导生成。

Result: 模型能生成符合自然抗体特征的序列，SVDD显著提升可开发性评分。

Conclusion: 该框架为抗体设计提供了结合结合性与生物物理特性的迭代优化方法。

Abstract: Therapeutic antibodies require not only high-affinity target engagement, but
also favorable manufacturability, stability, and safety profiles for clinical
effectiveness. These properties are collectively called `developability'. To
enable a computational framework for optimizing antibody sequences for
favorable developability, we introduce a guided discrete diffusion model
trained on natural paired heavy- and light-chain sequences from the Observed
Antibody Space (OAS) and quantitative developability measurements for 246
clinical-stage antibodies. To steer generation toward biophysically viable
candidates, we integrate a Soft Value-based Decoding in Diffusion (SVDD) Module
that biases sampling without compromising naturalness. In unconstrained
sampling, our model reproduces global features of both the natural repertoire
and approved therapeutics, and under SVDD guidance we achieve significant
enrichment in predicted developability scores over unguided baselines. When
combined with high-throughput developability assays, this framework enables an
iterative, ML-driven pipeline for designing antibodies that satisfy binding and
biophysical criteria in tandem.

</details>


### [51] [Embedding-Based Federated Data Sharing via Differentially Private Conditional VAEs](https://arxiv.org/abs/2507.02671)
*Francesco Di Salvo,Hanh Huyen My Nguyen,Christian Ledig*

Main category: cs.LG

TL;DR: 提出一种基于差分隐私生成模型的数据共享方法，通过DP-CVAE实现隐私保护、高效和可扩展的医学影像分析。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习在医学影像中因数据稀缺和隐私法规导致的局限性，以及联邦学习的高通信成本和任务单一性问题。

Method: 采用差分隐私条件变分自编码器（DP-CVAE）提取紧凑且信息丰富的嵌入，支持多样化的下游任务。

Result: 方法在隐私保护、可扩展性和效率上优于传统联邦学习，且DP-CVAE比DP-CGAN生成更高保真度的嵌入，参数需求减少5倍。

Conclusion: DP-CVAE是一种高效、隐私保护的解决方案，适用于医学影像分析的多样化任务。

Abstract: Deep Learning (DL) has revolutionized medical imaging, yet its adoption is
constrained by data scarcity and privacy regulations, limiting access to
diverse datasets. Federated Learning (FL) enables decentralized training but
suffers from high communication costs and is often restricted to a single
downstream task, reducing flexibility. We propose a data-sharing method via
Differentially Private (DP) generative models. By adopting foundation models,
we extract compact, informative embeddings, reducing redundancy and lowering
computational overhead. Clients collaboratively train a Differentially Private
Conditional Variational Autoencoder (DP-CVAE) to model a global, privacy-aware
data distribution, supporting diverse downstream tasks. Our approach, validated
across multiple feature extractors, enhances privacy, scalability, and
efficiency, outperforming traditional FL classifiers while ensuring
differential privacy. Additionally, DP-CVAE produces higher-fidelity embeddings
than DP-CGAN while requiring $5{\times}$ fewer parameters.

</details>


### [52] [Multi-Agent Reinforcement Learning for Dynamic Pricing in Supply Chains: Benchmarking Strategic Agent Behaviours under Realistically Simulated Market Conditions](https://arxiv.org/abs/2507.02698)
*Thomas Hazenberg,Yao Ma,Seyed Sahand Mohammadi Ziabari,Marijn van Rijswijk*

Main category: cs.LG

TL;DR: 研究探讨了多智能体强化学习（MARL）如何改进供应链中的动态定价策略，对比了三种MARL算法与静态规则方法的性能。


<details>
  <summary>Details</summary>
Motivation: 传统ERP系统的静态定价方法忽略了市场参与者间的战略互动，而现有强化学习研究多为单智能体，无法模拟真实供应链的相互依赖性。

Method: 在基于真实电商数据和LightGBM需求预测模型的模拟环境中，评估了MADDPG、MADQN和QMIX三种MARL算法与静态规则基线的表现。

Result: 静态规则方法公平性高（Jain指数0.9896）且价格稳定（波动0.024），但缺乏竞争性；MADQN定价最激进（公平性0.5844），MADDPG平衡了竞争与公平（公平性0.8819）。

Conclusion: MARL能捕捉静态规则无法模拟的战略行为，为动态定价的未来发展提供参考。

Abstract: This study investigates how Multi-Agent Reinforcement Learning (MARL) can
improve dynamic pricing strategies in supply chains, particularly in contexts
where traditional ERP systems rely on static, rule-based approaches that
overlook strategic interactions among market actors. While recent research has
applied reinforcement learning to pricing, most implementations remain
single-agent and fail to model the interdependent nature of real-world supply
chains. This study addresses that gap by evaluating the performance of three
MARL algorithms: MADDPG, MADQN, and QMIX against static rule-based baselines,
within a simulated environment informed by real e-commerce transaction data and
a LightGBM demand prediction model. Results show that rule-based agents achieve
near-perfect fairness (Jain's Index: 0.9896) and the highest price stability
(volatility: 0.024), but they fully lack competitive dynamics. Among MARL
agents, MADQN exhibits the most aggressive pricing behaviour, with the highest
volatility and the lowest fairness (0.5844). MADDPG provides a more balanced
approach, supporting market competition (share volatility: 9.5 pp) while
maintaining relatively high fairness (0.8819) and stable pricing. These
findings suggest that MARL introduces emergent strategic behaviour not captured
by static pricing rules and may inform future developments in dynamic pricing.

</details>


### [53] [Fluid Democracy in Federated Data Aggregation](https://arxiv.org/abs/2507.02710)
*Aditya Vema Reddy Kesari,Krishna Reddy Kesari*

Main category: cs.LG

TL;DR: 论文提出了一种基于共识的协议（FedVRD），用于在联邦学习中筛选最有用的客户端权重，以减少数据传输成本，并动态限制对抗性影响。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习机制要求所有客户端传输权重，无论其是否有用，导致数据传输成本浪费。

Method: 探索现有流体民主协议在联邦学习中的应用，提出新协议viscous-retained democracy，并开发算法FedVRD以动态限制对抗性影响。

Result: 新协议在相同假设下优于传统1p1v（FedAvg），同时避免影响力积累；FedVRD能有效减少对抗性影响。

Conclusion: FedVRD协议在减少数据传输成本和对抗性影响方面表现出色，为联邦学习提供了更高效的解决方案。

Abstract: Federated learning (FL) mechanisms typically require each client to transfer
their weights to a central server, irrespective of how useful they are. In
order to avoid wasteful data transfer costs from clients to the central server,
we propose the use of consensus based protocols to identify a subset of clients
with most useful model weights at each data transfer step. First, we explore
the application of existing fluid democracy protocols to FL from a performance
standpoint, comparing them with traditional one-person-one-vote (also known as
1p1v or FedAvg). We propose a new fluid democracy protocol named
viscous-retained democracy that always does better than 1p1v under the same
assumptions as existing fluid democracy protocols while also not allowing for
influence accumulation. Secondly, we identify weaknesses of fluid democracy
protocols from an adversarial lens in terms of their dependence on topology
and/ or number of adversaries required to negatively impact the global model
weights. To this effect, we propose an algorithm (FedVRD) that dynamically
limits the effect of adversaries while minimizing cost by leveraging the
delegation topology.

</details>


### [54] [A Forget-and-Grow Strategy for Deep Reinforcement Learning Scaling in Continuous Control](https://arxiv.org/abs/2507.02712)
*Zilin Kang,Chenyuan Hu,Yu Luo,Zhecheng Yuan,Ruijie Zheng,Huazhe Xu*

Main category: cs.LG

TL;DR: 论文提出FoG算法，通过遗忘早期经验和动态扩展网络容量，提升深度强化学习的样本效率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有深度强化学习方法存在早期经验过拟合问题（primacy bias），而人类因婴儿遗忘症不易受此影响。论文受此启发，提出新算法。

Method: FoG算法包含两个机制：1) 经验回放衰减（ER Decay），逐步减少早期经验的影响；2) 网络扩展，动态增加参数以提升学习能力。

Result: 在40多个任务的四个连续控制基准测试中，FoG表现优于现有SoTA算法（如BRO、SimBa、TD-MPC2）。

Conclusion: FoG通过模仿人类的遗忘与成长机制，显著提升了深度强化学习的性能。

Abstract: Deep reinforcement learning for continuous control has recently achieved
impressive progress. However, existing methods often suffer from primacy bias,
a tendency to overfit early experiences stored in the replay buffer, which
limits an RL agent's sample efficiency and generalizability. In contrast,
humans are less susceptible to such bias, partly due to infantile amnesia,
where the formation of new neurons disrupts early memory traces, leading to the
forgetting of initial experiences. Inspired by this dual processes of
forgetting and growing in neuroscience, in this paper, we propose Forget and
Grow (FoG), a new deep RL algorithm with two mechanisms introduced. First,
Experience Replay Decay (ER Decay) "forgetting early experience", which
balances memory by gradually reducing the influence of early experiences.
Second, Network Expansion, "growing neural capacity", which enhances agents'
capability to exploit the patterns of existing data by dynamically adding new
parameters during training. Empirical results on four major continuous control
benchmarks with more than 40 tasks demonstrate the superior performance of FoG
against SoTA existing deep RL algorithms, including BRO, SimBa, and TD-MPC2.

</details>


### [55] [A Comprehensive Machine Learning Framework for Micromobility Demand Prediction](https://arxiv.org/abs/2507.02715)
*Omri Porat,Michael Fire,Eran Ben-Elia*

Main category: cs.LG

TL;DR: 该研究提出了一种结合空间、时间和网络依赖性的框架，用于改进无桩电动滑板车的需求预测，显著提高了准确性。


<details>
  <summary>Details</summary>
Motivation: 无桩电动滑板车作为环保灵活的交通工具，需求预测对其管理和规划至关重要，但现有研究未能综合考虑多维度因素。

Method: 提出了一种集成空间、时间和网络依赖性的框架，用于预测需求。

Result: 框架将需求预测准确性提高了27%至49%，优于基线模型。

Conclusion: 该框架为数据驱动的微移动管理提供了支持，有助于优化车队分布和可持续城市规划。

Abstract: Dockless e-scooters, a key micromobility service, have emerged as
eco-friendly and flexible urban transport alternatives. These services improve
first and last-mile connectivity, reduce congestion and emissions, and
complement public transport for short-distance travel. However, effective
management of these services depends on accurate demand prediction, which is
crucial for optimal fleet distribution and infrastructure planning. While
previous studies have focused on analyzing spatial or temporal factors in
isolation, this study introduces a framework that integrates spatial, temporal,
and network dependencies for improved micromobility demand forecasting. This
integration enhances accuracy while providing deeper insights into urban
micromobility usage patterns. Our framework improves demand prediction accuracy
by 27 to 49% over baseline models, demonstrating its effectiveness in capturing
micromobility demand patterns. These findings support data-driven micromobility
management, enabling optimized fleet distribution, cost reduction, and
sustainable urban planning.

</details>


### [56] [Hierarchical Multi-Label Contrastive Learning for Protein-Protein Interaction Prediction Across Organisms](https://arxiv.org/abs/2507.02724)
*Shiyi Liu,Buwen Liang,Yuetong Fang,Zixuan Jiang,Renjing Xu*

Main category: cs.LG

TL;DR: HIPPO是一个基于层次对比学习的蛋白质-蛋白质相互作用预测框架，通过多层次的生物表征匹配和对比损失函数，实现了跨物种的高性能预测和零样本迁移能力。


<details>
  <summary>Details</summary>
Motivation: 解决跨物种蛋白质-蛋白质相互作用预测中数据异构性和稀疏性的问题，同时提升模型在低数据场景下的鲁棒性。

Method: 提出层次对比学习框架HIPPO，结合蛋白质序列和层次属性，通过多层次的生物表征匹配和对比损失函数优化嵌入空间。

Result: 在基准数据集上达到最优性能，表现出低数据场景的鲁棒性和零样本迁移能力。

Conclusion: HIPPO为稀疏或多物种不平衡数据下的相互作用预测提供了统一框架，并揭示了层次特征融合对捕获保守相互作用决定因素的重要性。

Abstract: Recent advances in AI for science have highlighted the power of contrastive
learning in bridging heterogeneous biological data modalities. Building on this
paradigm, we propose HIPPO (HIerarchical Protein-Protein interaction prediction
across Organisms), a hierarchical contrastive framework for protein-protein
interaction(PPI) prediction, where protein sequences and their hierarchical
attributes are aligned through multi-tiered biological representation matching.
The proposed approach incorporates hierarchical contrastive loss functions that
emulate the structured relationship among functional classes of proteins. The
framework adaptively incorporates domain and family knowledge through a
data-driven penalty mechanism, enforcing consistency between the learned
embedding space and the intrinsic hierarchy of protein functions. Experiments
on benchmark datasets demonstrate that HIPPO achieves state-of-the-art
performance, outperforming existing methods and showing robustness in low-data
regimes. Notably, the model demonstrates strong zero-shot transferability to
other species without retraining, enabling reliable PPI prediction and
functional inference even in less characterized or rare organisms where
experimental data are limited. Further analysis reveals that hierarchical
feature fusion is critical for capturing conserved interaction determinants,
such as binding motifs and functional annotations. This work advances
cross-species PPI prediction and provides a unified framework for interaction
prediction in scenarios with sparse or imbalanced multi-species data.

</details>


### [57] [Classification by Separating Hypersurfaces: An Entropic Approach](https://arxiv.org/abs/2507.02732)
*Argimiro Arratia,Mahmoud El Daou,Henryk Gzyl*

Main category: cs.LG

TL;DR: 提出一种基于熵最小化的新型分类方法，通过在高维超立方体中搜索参数向量，支持复杂决策边界。


<details>
  <summary>Details</summary>
Motivation: 解决传统线性或二次优化方法（如支持向量机和梯度下降）在处理复杂分类任务时的局限性。

Method: 在高维超立方体中搜索参数向量，并最小化熵函数，扩展到多项式曲面以实现复杂决策边界。

Result: 数值实验表明该方法在多种分类任务（包括线性和非线性可分性）中高效且灵活。

Conclusion: 该方法为分类问题提供了稳健的替代方案，适用于复杂决策边界。

Abstract: We consider the following classification problem: Given a population of
individuals characterized by a set of attributes represented as a vector in
${\mathbb R}^N$, the goal is to find a hyperplane in ${\mathbb R}^N$ that
separates two sets of points corresponding to two distinct classes. This
problem, with a history dating back to the perceptron model, remains central to
machine learning. In this paper we propose a novel approach by searching for a
vector of parameters in a bounded $N$-dimensional hypercube centered at the
origin and a positive vector in ${\mathbb R}^M$, obtained through the
minimization of an entropy-based function defined over the space of unknown
variables. The method extends to polynomial surfaces, allowing the separation
of data points by more complex decision boundaries. This provides a robust
alternative to traditional linear or quadratic optimization techniques, such as
support vector machines and gradient descent. Numerical experiments demonstrate
the efficiency and versatility of the method in handling diverse classification
tasks, including linear and non-linear separability.

</details>


### [58] [Fast and Simplex: 2-Simplicial Attention in Triton](https://arxiv.org/abs/2507.02754)
*Aurko Roy,Timothy Chou,Sai Surya Duvvuri,Sijia Chen,Jiecao Yu,Xiaodong Wang,Manzil Zaheer,Rohan Anil*

Main category: cs.LG

TL;DR: 论文探讨了2-单纯形Transformer在提高token效率方面的优势，相比标准Transformer在数学、编码、推理和逻辑任务中表现更优。


<details>
  <summary>Details</summary>
Motivation: 现代大语言模型依赖大规模数据集，计算资源不再是唯一瓶颈，因此需要更高效的架构来优化token使用。

Method: 提出2-单纯形Transformer，通过三线性函数扩展标准点积注意力，并实现高效的Triton内核。

Result: 在固定token预算下，2-单纯形Transformer在多项任务中优于标准Transformer，并改变了知识和推理任务的缩放定律指数。

Conclusion: 2-单纯形Transformer显著提升了token效率，为计算资源有限的任务提供了更优的解决方案。

Abstract: Recent work has shown that training loss scales as a power law with both
model size and the number of tokens, and that achieving compute-optimal models
requires scaling model size and token count together. However, these scaling
laws assume an infinite supply of data and apply primarily in compute-bound
settings. As modern large language models increasingly rely on massive
internet-scale datasets, the assumption that they are compute-bound is becoming
less valid. This shift highlights the need for architectures that prioritize
token efficiency.
  In this work, we investigate the use of the 2-simplicial Transformer, an
architecture that generalizes standard dot-product attention to trilinear
functions through an efficient Triton kernel implementation. We demonstrate
that the 2-simplicial Transformer achieves better token efficiency than
standard Transformers: for a fixed token budget, similarly sized models
outperform their dot-product counterparts on tasks involving mathematics,
coding, reasoning, and logic. We quantify these gains by demonstrating that
$2$-simplicial attention changes the exponent in the scaling laws for knowledge
and reasoning tasks compared to dot product attention.

</details>


### [59] [Contextual Online Pricing with (Biased) Offline Data](https://arxiv.org/abs/2507.02762)
*Yixuan Zhang,Ruihao Zhu,Qiaomin Xie*

Main category: cs.LG

TL;DR: 研究了带有偏置离线数据的上下文在线定价问题，提出了基于OFU策略的最优遗憾界限，并扩展到一般价格弹性情况。


<details>
  <summary>Details</summary>
Motivation: 解决在存在偏置离线数据的情况下，如何优化在线定价策略的统计复杂性和遗憾界限。

Method: 使用Optimism-in-the-Face-of-Uncertainty (OFU)策略，结合离线数据的偏置界限、规模和分散性，推导出最优遗憾界限。

Result: 在标量价格弹性情况下，得到了实例依赖的遗憾界限；在一般价格弹性情况下，得到了最坏情况下的最优遗憾界限。

Conclusion: 首次为带有偏置离线数据的上下文定价问题提供了紧致的遗憾保证，并推广到随机线性bandit问题。

Abstract: We study contextual online pricing with biased offline data. For the scalar
price elasticity case, we identify the instance-dependent quantity $\delta^2$
that measures how far the offline data lies from the (unknown) online optimum.
We show that the time length $T$, bias bound $V$, size $N$ and dispersion
$\lambda_{\min}(\hat{\Sigma})$ of the offline data, and $\delta^2$ jointly
determine the statistical complexity. An Optimism-in-the-Face-of-Uncertainty
(OFU) policy achieves a minimax-optimal, instance-dependent regret bound
$\tilde{\mathcal{O}}\big(d\sqrt{T} \wedge (V^2T +
\frac{dT}{\lambda_{\min}(\hat{\Sigma}) + (N \wedge T) \delta^2})\big)$. For
general price elasticity, we establish a worst-case, minimax-optimal rate
$\tilde{\mathcal{O}}\big(d\sqrt{T} \wedge (V^2T + \frac{dT
}{\lambda_{\min}(\hat{\Sigma})})\big)$ and provide a generalized OFU algorithm
that attains it. When the bias bound $V$ is unknown, we design a robust variant
that always guarantees sub-linear regret and strictly improves on purely online
methods whenever the exact bias is small. These results deliver the first tight
regret guarantees for contextual pricing in the presence of biased offline
data. Our techniques also transfer verbatim to stochastic linear bandits with
biased offline data, yielding analogous bounds.

</details>


### [60] [Understanding and Improving Length Generalization in Recurrent Models](https://arxiv.org/abs/2507.02782)
*Ricardo Buitrago Ruiz,Albert Gu*

Main category: cs.LG

TL;DR: 该论文探讨了循环模型在长度泛化上的失败原因，并提出简单训练干预措施以提升其处理长序列的能力。


<details>
  <summary>Details</summary>
Motivation: 循环模型理论上能处理任意长序列，但实际性能在超出训练上下文长度时会显著下降，即无法长度泛化。

Method: 通过实证和理论分析支持'未探索状态假设'，并提出训练干预措施（如高斯噪声初始化或状态共享）以增加状态覆盖。

Result: 仅需少量额外训练步骤（约0.1%的预训练预算），模型即可处理远超训练上下文长度的序列（如从2k到128k）。

Conclusion: 这些干预措施简单高效，能显著提升循环模型的长度泛化能力。

Abstract: Recently, recurrent models such as state space models and linear attention
have become popular due to their linear complexity in the sequence length.
Thanks to their recurrent nature, in principle they can process arbitrarily
long sequences, but their performance sometimes drops considerably beyond their
training context lengths-i.e. they fail to length generalize. In this work, we
provide comprehensive empirical and theoretical analysis to support the
unexplored states hypothesis, which posits that models fail to length
generalize when during training they are only exposed to a limited subset of
the distribution of all attainable states (i.e. states that would be attained
if the recurrence was applied to long sequences). Furthermore, we investigate
simple training interventions that aim to increase the coverage of the states
that the model is trained on, e.g. by initializing the state with Gaussian
noise or with the final state of a different input sequence. With only 500
post-training steps ($\sim 0.1\%$ of the pre-training budget), these
interventions enable length generalization for sequences that are orders of
magnitude longer than the training context (e.g. $2k\longrightarrow 128k$) and
show improved performance in long context tasks, thus presenting a simple and
efficient way to enable robust length generalization in general recurrent
models.

</details>


### [61] [In-Training Multicalibrated Survival Analysis for Healthcare via Constrained Optimization](https://arxiv.org/abs/2507.02807)
*Thiti Suttaket,Stanley Kok*

Main category: cs.LG

TL;DR: GRADUATE模型通过多校准技术确保所有亚群体的预测概率准确，解决了现有生存分析模型仅在群体层面校准的问题。


<details>
  <summary>Details</summary>
Motivation: 现有生存分析模型仅在校准群体层面表现良好，可能导致对少数亚群体的预测不准确，从而影响临床决策。

Method: GRADUATE将多校准问题建模为约束优化问题，同时在训练中优化校准和区分度。

Result: 理论证明和实证分析表明，GRADUATE在真实临床数据集上优于现有基线模型。

Conclusion: GRADUATE通过多校准技术显著提升了生存分析模型的预测准确性，尤其在少数亚群体中表现突出。

Abstract: Survival analysis is an important problem in healthcare because it models the
relationship between an individual's covariates and the onset time of an event
of interest (e.g., death). It is important for survival models to be
well-calibrated (i.e., for their predicted probabilities to be close to
ground-truth probabilities) because badly calibrated systems can result in
erroneous clinical decisions. Existing survival models are typically calibrated
at the population level only, and thus run the risk of being poorly calibrated
for one or more minority subpopulations. We propose a model called GRADUATE
that achieves multicalibration by ensuring that all subpopulations are
well-calibrated too. GRADUATE frames multicalibration as a constrained
optimization problem, and optimizes both calibration and discrimination
in-training to achieve a good balance between them. We mathematically prove
that the optimization method used yields a solution that is both near-optimal
and feasible with high probability. Empirical comparisons against
state-of-the-art baselines on real-world clinical datasets demonstrate
GRADUATE's efficacy. In a detailed analysis, we elucidate the shortcomings of
the baselines vis-a-vis GRADUATE's strengths.

</details>


### [62] [Replicable Distribution Testing](https://arxiv.org/abs/2507.02814)
*Ilias Diakonikolas,Jingyi Gao,Daniel Kane,Sihan Liu,Christopher Ye*

Main category: cs.LG

TL;DR: 本文系统研究了算法可复制性框架下的分布测试问题，提出了新的可复制算法用于测试离散分布的接近性和独立性，并开发了新的下界证明方法。


<details>
  <summary>Details</summary>
Motivation: 研究在算法可复制性框架下分布测试的样本复杂度，填补现有研究的空白。

Method: 开发新的可复制算法测试分布接近性和独立性，并提出新的下界证明方法。

Result: 建立了接近最优的样本复杂度下界，解决了先前工作中的开放性问题。

Conclusion: 本文为可复制分布测试提供了理论基础和实用算法，具有广泛的应用潜力。

Abstract: We initiate a systematic investigation of distribution testing in the
framework of algorithmic replicability. Specifically, given independent samples
from a collection of probability distributions, the goal is to characterize the
sample complexity of replicably testing natural properties of the underlying
distributions. On the algorithmic front, we develop new replicable algorithms
for testing closeness and independence of discrete distributions. On the lower
bound front, we develop a new methodology for proving sample complexity lower
bounds for replicable testing that may be of broader interest. As an
application of our technique, we establish near-optimal sample complexity lower
bounds for replicable uniformity testing -- answering an open question from
prior work -- and closeness testing.

</details>


### [63] [LLM-Driven Treatment Effect Estimation Under Inference Time Text Confounding](https://arxiv.org/abs/2507.02843)
*Yuchen Ma,Dennis Frauen,Jonas Schweisthal,Stefan Feuerriegel*

Main category: cs.LG

TL;DR: 论文提出了一种解决训练与推理数据差异导致的治疗效应估计偏差的框架，结合大语言模型和双重稳健学习器。


<details>
  <summary>Details</summary>
Motivation: 临床实践中，治疗效应估计在训练和推理时数据形式不一致（结构化数据 vs. 文本描述），可能导致偏差。

Method: 提出新框架，利用大语言模型和双重稳健学习器，显式处理推理时文本混杂问题。

Result: 实验证明框架在实际应用中的有效性。

Conclusion: 该框架能有效减少因推理时文本混杂导致的治疗效应估计偏差。

Abstract: Estimating treatment effects is crucial for personalized decision-making in
medicine, but this task faces unique challenges in clinical practice. At
training time, models for estimating treatment effects are typically trained on
well-structured medical datasets that contain detailed patient information.
However, at inference time, predictions are often made using textual
descriptions (e.g., descriptions with self-reported symptoms), which are
incomplete representations of the original patient information. In this work,
we make three contributions. (1) We show that the discrepancy between the data
available during training time and inference time can lead to biased estimates
of treatment effects. We formalize this issue as an inference time text
confounding problem, where confounders are fully observed during training time
but only partially available through text at inference time. (2) To address
this problem, we propose a novel framework for estimating treatment effects
that explicitly accounts for inference time text confounding. Our framework
leverages large language models together with a custom doubly robust learner to
mitigate biases caused by the inference time text confounding. (3) Through a
series of experiments, we demonstrate the effectiveness of our framework in
real-world applications.

</details>


### [64] [MvHo-IB: Multi-View Higher-Order Information Bottleneck for Brain Disorder Diagnosis](https://arxiv.org/abs/2507.02847)
*Kunyu Zhang,Qiang Li,Shujian Yu*

Main category: cs.LG

TL;DR: MvHo-IB是一个多视图学习框架，通过整合高阶互动（HOIs）和成对互动，提升fMRI数据的诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以有效提取和利用fMRI数据中的高阶互动（HOIs），限制了诊断准确性。

Method: 结合信息论中的O-information和矩阵熵估计器量化HOIs，使用Brain3DCNN编码器，并采用多视图学习信息瓶颈目标优化表示学习。

Result: 在三个基准fMRI数据集上，MvHo-IB表现优于现有方法，包括最新的超图技术。

Conclusion: MvHo-IB通过创新方法有效提取和利用HOIs，显著提升了诊断性能。

Abstract: Recent evidence suggests that modeling higher-order interactions (HOIs) in
functional magnetic resonance imaging (fMRI) data can enhance the diagnostic
accuracy of machine learning systems. However, effectively extracting and
utilizing HOIs remains a significant challenge. In this work, we propose
MvHo-IB, a novel multi-view learning framework that integrates both pairwise
interactions and HOIs for diagnostic decision-making, while automatically
compressing task-irrelevant redundant information. MvHo-IB introduces several
key innovations: (1) a principled method that combines O-information from
information theory with a matrix-based Renyi alpha-order entropy estimator to
quantify and extract HOIs, (2) a purpose-built Brain3DCNN encoder to
effectively utilize these interactions, and (3) a new multi-view learning
information bottleneck objective to enhance representation learning.
Experiments on three benchmark fMRI datasets demonstrate that MvHo-IB achieves
state-of-the-art performance, significantly outperforming previous methods,
including recent hypergraph-based techniques. The implementation of MvHo-IB is
available at https://github.com/zky04/MvHo-IB.

</details>


### [65] [Learnable-Differentiable Finite Volume Solver for Accelerated Simulation of Flows](https://arxiv.org/abs/2507.01975)
*Mengtao Yan,Qi Wang,Haining Wang,Ruizhi Chengze,Yi Zhang,Hongsheng Liu,Zidong Wang,Fan Yu,Qi Qi,Hao Sun*

Main category: cs.LG

TL;DR: 提出了一种名为LDSolver的可学习微分有限体积求解器，用于在粗网格上高效准确地模拟流体流动，解决了传统方法和机器学习方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统数值求解器计算成本高，机器学习方法存在可解释性、泛化性和数据依赖性问题，因此需要一种更高效的解决方案。

Method: LDSolver包含两部分：可微分有限体积求解器和可学习模块，用于在粗网格上提供等效近似和误差校正。

Result: 实验表明，LDSolver在多种流动系统中表现优异，计算效率高且泛化能力强。

Conclusion: LDSolver在粗网格上实现了高效准确的流体模拟，优于现有方法。

Abstract: Simulation of fluid flows is crucial for modeling physical phenomena like
meteorology, aerodynamics, and biomedicine. Classical numerical solvers often
require fine spatiotemporal grids to satisfy stability, consistency, and
convergence conditions, leading to substantial computational costs. Although
machine learning has demonstrated better efficiency, they typically suffer from
issues of interpretability, generalizability, and data dependency. Hence, we
propose a learnable and differentiable finite volume solver, called LDSolver,
designed for efficient and accurate simulation of fluid flows on spatiotemporal
coarse grids. LDSolver comprises two key components: (1) a differentiable
finite volume solver, and (2) an learnable module providing equivalent
approximation for fluxes (derivatives and interpolations), and temporal error
correction on coarse grids. Even with limited training data (e.g., only a few
trajectories), our model could accelerate the simulation while maintaining a
high accuracy with superior generalizability. Experiments on different flow
systems (e.g., Burgers, decaying, forced and shear flows) show that LDSolver
achieves state-of-the-art performance, surpassing baseline models with notable
margins.

</details>


### [66] [DKGCM: A Spatio-Temporal Prediction Model for Traffic Flow by Fusing Spatial Node Clustering Method and Fourier Bidirectional Mamba Mechanism](https://arxiv.org/abs/2507.01982)
*Siqing Long,Xiangzhi Huang,Jiemin Xie,Ming Cai*

Main category: cs.LG

TL;DR: 提出了一种名为DKGCM的新型图卷积网络结构，通过动态时间规整和K均值聚类改进空间依赖性捕捉，并结合FFT和双向Mamba框架优化时间依赖性预测，实验表明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 交通需求预测的准确性对资源分配至关重要，但复杂的时空关系限制了现有模型的性能，因此需要更有效的预测方法。

Method: 提出DK-GCN方法，结合DTW和K-means聚类分组交通节点；在时间尺度上，集成FFT和双向Mamba框架；采用GRPO强化学习策略优化训练。

Result: 在三个公开数据集上表现优于多种先进方法。

Conclusion: DKGCM模型能有效提升交通需求预测的准确性，为资源分配提供更可靠支持。

Abstract: Accurate traffic demand forecasting enables transportation management
departments to allocate resources more effectively, thereby improving their
utilization efficiency. However, complex spatiotemporal relationships in
traffic systems continue to limit the performance of demand forecasting models.
To improve the accuracy of spatiotemporal traffic demand prediction, we propose
a new graph convolutional network structure called DKGCM. Specifically, we
first consider the spatial flow distribution of different traffic nodes and
propose a novel temporal similarity-based clustering graph convolution method,
DK-GCN. This method utilizes Dynamic Time Warping (DTW) and K-means clustering
to group traffic nodes and more effectively capture spatial dependencies. On
the temporal scale, we integrate the Fast Fourier Transform (FFT) within the
bidirectional Mamba deep learning framework to capture temporal dependencies in
traffic demand. To further optimize model training, we incorporate the GRPO
reinforcement learning strategy to enhance the loss function feedback
mechanism. Extensive experiments demonstrate that our model outperforms several
advanced methods and achieves strong results on three public datasets.

</details>


### [67] [Multimodal Misinformation Detection Using Early Fusion of Linguistic, Visual, and Social Features](https://arxiv.org/abs/2507.01984)
*Gautam Kishore Shahi*

Main category: cs.LG

TL;DR: 研究探讨了多模态特征（文本、图像和社交特征）在虚假信息检测中的有效性，发现结合无监督和监督机器学习模型能显著提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 社交媒体在选举和危机期间充斥着虚假信息，现有研究多集中于单模态（文本或图像），而多模态特征组合的研究较少。

Method: 采用早期融合方法，结合文本、图像和社交特征，分析1,529条推文，并通过数据增强提取视觉和社交特征。

Result: 多模态模型比单模态模型性能提升15%，比双模态模型提升5%，并分析了虚假信息的传播模式。

Conclusion: 多模态特征组合能有效提升虚假信息检测性能，同时揭示了虚假信息的传播特征。

Abstract: Amid a tidal wave of misinformation flooding social media during elections
and crises, extensive research has been conducted on misinformation detection,
primarily focusing on text-based or image-based approaches. However, only a few
studies have explored multimodal feature combinations, such as integrating text
and images for building a classification model to detect misinformation. This
study investigates the effectiveness of different multimodal feature
combinations, incorporating text, images, and social features using an early
fusion approach for the classification model. This study analyzed 1,529 tweets
containing both text and images during the COVID-19 pandemic and election
periods collected from Twitter (now X). A data enrichment process was applied
to extract additional social features, as well as visual features, through
techniques such as object detection and optical character recognition (OCR).
The results show that combining unsupervised and supervised machine learning
models improves classification performance by 15% compared to unimodal models
and by 5% compared to bimodal models. Additionally, the study analyzes the
propagation patterns of misinformation based on the characteristics of
misinformation tweets and the users who disseminate them.

</details>


### [68] [Positive region preserved random sampling: an efficient feature selection method for massive data](https://arxiv.org/abs/2507.01998)
*Hexiang Bai,Deyu Li,Jiye Liang,Yanhui Zhai*

Main category: cs.LG

TL;DR: 提出了一种基于采样技术和粗糙集理论的新方法，用于大规模数据的特征选择，通过衡量特征集的判别能力并构建正区域保留样本，高效找到近似约简。


<details>
  <summary>Details</summary>
Motivation: 智能机器在处理海量数据时计算资源有限，需要高效的特征选择方法以提高成功率。

Method: 利用可区分对象对的比例衡量特征集的判别能力，构建正区域保留样本，提出新的特征选择方法。

Result: 在11个数据集上验证，方法能在短时间内找到近似约简，判别能力高于预估下限；在4个大规模数据集上也表现良好。

Conclusion: 该方法能在个人计算机上高效找到高判别能力的特征子集，适用于大规模数据。

Abstract: Selecting relevant features is an important and necessary step for
intelligent machines to maximize their chances of success. However, intelligent
machines generally have no enough computing resources when faced with huge
volume of data. This paper develops a new method based on sampling techniques
and rough set theory to address the challenge of feature selection for massive
data. To this end, this paper proposes using the ratio of discernible object
pairs to all object pairs that should be distinguished to measure the
discriminatory ability of a feature set. Based on this measure, a new feature
selection method is proposed. This method constructs positive region preserved
samples from massive data to find a feature subset with high discriminatory
ability. Compared with other methods, the proposed method has two advantages.
First, it is able to select a feature subset that can preserve the
discriminatory ability of all the features of the target massive data set
within an acceptable time on a personal computer. Second, the lower boundary of
the probability of the object pairs that can be discerned using the feature
subset selected in all object pairs that should be distinguished can be
estimated before finding reducts. Furthermore, 11 data sets of different sizes
were used to validate the proposed method. The results show that approximate
reducts can be found in a very short period of time, and the discriminatory
ability of the final reduct is larger than the estimated lower boundary.
Experiments on four large-scale data sets also showed that an approximate
reduct with high discriminatory ability can be obtained in reasonable time on a
personal computer.

</details>


### [69] [Continuous Wavelet Transform and Siamese Network-Based Anomaly Detection in Multi-variate Semiconductor Process Time Series](https://arxiv.org/abs/2507.01999)
*Bappaditya Dey,Daniel Sorensen,Minjin Hwang,Sandip Halder*

Main category: cs.LG

TL;DR: 提出了一种基于机器学习的多变量时间序列异常检测方法，通过连续小波变换将数据转换为图像，并利用预训练的VGG-16架构和孪生网络进行高效分类。


<details>
  <summary>Details</summary>
Motivation: 半导体制造过程中数据复杂且异常罕见，传统方法难以应对高维度、噪声和类别不平衡等问题。

Method: 1. 使用连续小波变换将时间序列数据转换为图像；2. 微调预训练的VGG-16架构；3. 构建孪生网络比较图像对。

Result: 在真实半导体制造数据集上表现出高精度，适用于离线和半监督场景。

Conclusion: 该方法为复杂制造环境中的异常检测提供了灵活且高效的解决方案。

Abstract: Semiconductor manufacturing is an extremely complex process, characterized by
thousands of interdependent parameters collected across diverse tools and
process steps. Multi-variate time-series (MTS) analysis has emerged as a
critical methodology for enabling real-time monitoring, fault detection, and
predictive maintenance in such environments. However, anomaly prediction in
semiconductor fabrication presents several critical challenges, including high
data dimensionality, severe class imbalance due to the rarity of true faults,
noisy and missing measurements, and non-stationary behavior of production
systems. Furthermore, the complex interdependencies between variables and the
delayed emergence of faults across downstream stages complicate both anomaly
detection and root-cause-analysis. This paper presents a novel and generic
approach for anomaly detection in MTS data using machine learning. The proposed
methodology consists of three main steps: a) converting MTS data into
image-based representations using the Continuous Wavelet Transform, b)
developing a multi-class image classifier by fine-tuning a pretrained VGG-16
architecture on custom CWT image datasets, and c) constructing a Siamese
network composed of two identical sub-networks, each utilizing the fine-tuned
VGG-16 as a backbone. The network takes pairs of CWT images as input -one
serving as a reference or anchor (representing a known-good signal), and the
other as a query (representing an unknown signal). The model then compares the
embeddings of both inputs to determine whether they belong to the same class at
a given time step. Our approach demonstrates high accuracy in identifying
anomalies on a real FAB process time-series dataset, offering a promising
solution for offline anomaly detection in process and tool trace data.
Moreover, the approach is flexible and can be applied in both supervised and
semi-supervised settings.

</details>


### [70] [Temporal Chain of Thought: Long-Video Understanding by Thinking in Frames](https://arxiv.org/abs/2507.02001)
*Anurag Arnab,Ahmet Iscen,Mathilde Caron,Alireza Fathi,Cordelia Schmid*

Main category: cs.LG

TL;DR: 提出了一种名为Temporal Chain of Thought的推理策略，通过迭代选择视频中最相关的帧来提升长视频问答的准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管现有的视觉语言模型（VLMs）能够处理约1000帧的输入，但在长视频理解中仍难以有效利用上下文，并容易受到无关干扰的影响。

Method: 使用VLM自身迭代识别并提取视频中最相关的帧，然后用于回答问题。

Result: 在4个不同的视频问答数据集上取得了最先进的结果，特别是在超过1小时的长视频上表现优异，优于标准推理方法。

Conclusion: 通过推理时计算选择最相关的上下文，能够显著提升长视频问答的准确性。

Abstract: Despite recent advances in Vision-Language Models (VLMs), long-video
understanding remains a challenging problem. Although state-of-the-art
long-context VLMs can process around 1000 input frames, they still struggle to
effectively leverage this sequence length, and succumb to irrelevant
distractors within the context window. We present Temporal Chain of Thought, an
inference strategy for video question-answering that curates the model's input
context. We use the VLM itself to iteratively identify and extract the most
relevant frames from the video, which are then used for answering. We
demonstrate how leveraging more computation at inference-time to select the
most relevant context leads to improvements in accuracy, in agreement with
recent work on inference-time scaling of LLMs. Moreover, we achieve
state-of-the-art results on 4 diverse video question-answering datasets,
showing consistent improvements with 3 different VLMs. In particular, our
method shines on longer videos which would not otherwise fit within the model's
context window: On longer videos of more than 1 hour on LVBench, our approach
using a context window of 32K outperforms the same VLM using standard inference
with a 700K context window by 2.8 points.

</details>


### [71] [AIRES: Accelerating Out-of-Core GCNs via Algorithm-System Co-Design](https://arxiv.org/abs/2507.02006)
*Shakya Jayakody,Youpeng Zhao,Jun Wang*

Main category: cs.LG

TL;DR: AIRES是一种算法-系统协同设计解决方案，用于加速GCN中的外核SpGEMM计算，通过块级数据对齐和三阶段动态调度，显著降低延迟。


<details>
  <summary>Details</summary>
Motivation: 现有系统在处理大规模图数据时存在高I/O延迟和GPU利用率低的问题，主要源于稀疏格式数据对齐和内存分配的性能瓶颈。

Method: AIRES提出块级稀疏格式数据对齐和分块算法，并采用三阶段动态调度和双路数据传输策略，结合GPU内存、GDS和主机内存。

Result: AIRES在真实图处理基准测试中，延迟降低高达1.8倍，显著优于现有方法。

Conclusion: AIRES通过算法和系统协同优化，有效解决了外核SpGEMM的性能瓶颈，为GCN的高效计算提供了新思路。

Abstract: Graph convolutional networks (GCNs) are fundamental in various scientific
applications, ranging from biomedical protein-protein interactions (PPI) to
large-scale recommendation systems. An essential component for modeling graph
structures in GCNs is sparse general matrix-matrix multiplication (SpGEMM). As
the size of graph data continues to scale up, SpGEMMs are often conducted in an
out-of-core fashion due to limited GPU memory space in resource-constrained
systems. Albeit recent efforts that aim to alleviate the memory constraints of
out-of-core SpGEMM through either GPU feature caching, hybrid CPU-GPU memory
layout, or performing the computation in sparse format, current systems suffer
from both high I/O latency and GPU under-utilization issues.
  In this paper, we first identify the problems of existing systems, where
sparse format data alignment and memory allocation are the main performance
bottlenecks, and propose AIRES, a novel algorithm-system co-design solution to
accelerate out-of-core SpGEMM computation for GCNs. Specifically, from the
algorithm angle, AIRES proposes to alleviate the data alignment issues on the
block level for matrices in sparse formats and develops a tiling algorithm to
facilitate row block-wise alignment. On the system level, AIRES employs a
three-phase dynamic scheduling that features a dual-way data transfer strategy
utilizing a tiered memory system: integrating GPU memory, GPU Direct Storage
(GDS), and host memory to reduce I/O latency and improve throughput.
Evaluations show that AIRES significantly outperforms the state-of-the-art
methods, achieving up to 1.8x lower latency in real-world graph processing
benchmarks.

</details>


### [72] [GeoAda: Efficiently Finetune Geometric Diffusion Models with Equivariant Adapters](https://arxiv.org/abs/2507.02085)
*Wanjia Zhao,Jiaqi Han,Siyi Gu,Mingjian Jiang,James Zou,Stefano Ermon*

Main category: cs.LG

TL;DR: GeoAda是一种SE(3)-等变适配器框架，用于高效微调几何扩散模型，保持几何一致性并避免过拟合和灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有几何扩散模型在下游任务中灵活微调的效率不足，需要一种参数高效且不修改原架构的方法。

Method: GeoAda通过结构化适配器设计，包括控制信号编码、可训练层复制和等变卷积投影，实现高效微调。

Result: GeoAda在多种几何控制任务中表现优异，保持原始任务准确性，优于其他基线方法。

Conclusion: GeoAda为几何扩散模型提供了一种高效、灵活的微调方案，具有广泛适用性。

Abstract: Geometric diffusion models have shown remarkable success in molecular
dynamics and structure generation. However, efficiently fine-tuning them for
downstream tasks with varying geometric controls remains underexplored. In this
work, we propose an SE(3)-equivariant adapter framework ( GeoAda) that enables
flexible and parameter-efficient fine-tuning for controlled generative tasks
without modifying the original model architecture. GeoAda introduces a
structured adapter design: control signals are first encoded through coupling
operators, then processed by a trainable copy of selected pretrained model
layers, and finally projected back via decoupling operators followed by an
equivariant zero-initialized convolution. By fine-tuning only these lightweight
adapter modules, GeoAda preserves the model's geometric consistency while
mitigating overfitting and catastrophic forgetting. We theoretically prove that
the proposed adapters maintain SE(3)-equivariance, ensuring that the geometric
inductive biases of the pretrained diffusion model remain intact during
adaptation. We demonstrate the wide applicability of GeoAda across diverse
geometric control types, including frame control, global control, subgraph
control, and a broad range of application domains such as particle dynamics,
molecular dynamics, human motion prediction, and molecule generation. Empirical
results show that GeoAda achieves state-of-the-art fine-tuning performance
while preserving original task accuracy, whereas other baselines experience
significant performance degradation due to overfitting and catastrophic
forgetting.

</details>


### [73] [Evaluating the Promise and Pitfalls of LLMs in Hiring Decisions](https://arxiv.org/abs/2507.02087)
*Eitan Anzenberg,Arunava Samajpati,Sivasankaran Chandrasekar,Varun Kacholia*

Main category: cs.LG

TL;DR: 研究表明，专用招聘模型Match Score在准确性和公平性上优于通用大语言模型（LLMs），强调高风险领域需定制化模型和偏见审计。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs在招聘中的潜在偏见问题，并验证专用模型是否能同时提升准确性和公平性。

Method: 对比多种LLMs与专用模型Match Score，评估其预测准确性和公平性指标。

Result: Match Score在ROC AUC（0.85 vs 0.77）和公平性（种族影响比0.957 vs 0.809）上显著优于LLMs。

Conclusion: 专用模型能兼顾准确性与公平性，强调高风险领域需避免直接使用通用LLMs。

Abstract: The use of large language models (LLMs) in hiring promises to streamline
candidate screening, but it also raises serious concerns regarding accuracy and
algorithmic bias where sufficient safeguards are not in place. In this work, we
benchmark several state-of-the-art foundational LLMs - including models from
OpenAI, Anthropic, Google, Meta, and Deepseek, and compare them with our
proprietary domain-specific hiring model (Match Score) for job candidate
matching. We evaluate each model's predictive accuracy (ROC AUC,
Precision-Recall AUC, F1-score) and fairness (impact ratio of cut-off analysis
across declared gender, race, and intersectional subgroups). Our experiments on
a dataset of roughly 10,000 real-world recent candidate-job pairs show that
Match Score outperforms the general-purpose LLMs on accuracy (ROC AUC 0.85 vs
0.77) and achieves significantly more equitable outcomes across demographic
groups. Notably, Match Score attains a minimum race-wise impact ratio of 0.957
(near-parity), versus 0.809 or lower for the best LLMs, (0.906 vs 0.773 for the
intersectionals, respectively). We discuss why pretraining biases may cause
LLMs with insufficient safeguards to propagate societal biases in hiring
scenarios, whereas a bespoke supervised model can more effectively mitigate
these biases. Our findings highlight the importance of domain-specific modeling
and bias auditing when deploying AI in high-stakes domains such as hiring, and
caution against relying on off-the-shelf LLMs for such tasks without extensive
fairness safeguards. Furthermore, we show with empirical evidence that there
shouldn't be a dichotomy between choosing accuracy and fairness in hiring: a
well-designed algorithm can achieve both accuracy in hiring and fairness in
outcomes.

</details>


### [74] [Sample Complexity Bounds for Linear Constrained MDPs with a Generative Model](https://arxiv.org/abs/2507.02089)
*Xingtu Liu,Lin F. Yang,Sharan Vaswani*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We consider infinite-horizon $\gamma$-discounted (linear) constrained Markov
decision processes (CMDPs) where the objective is to find a policy that
maximizes the expected cumulative reward subject to expected cumulative
constraints. Given access to a generative model, we propose to solve CMDPs with
a primal-dual framework that can leverage any black-box unconstrained MDP
solver. For linear CMDPs with feature dimension $d$, we instantiate the
framework by using mirror descent value iteration
(\texttt{MDVI})~\citep{kitamura2023regularization} an example MDP solver. We
provide sample complexity bounds for the resulting CMDP algorithm in two cases:
(i) relaxed feasibility, where small constraint violations are allowed, and
(ii) strict feasibility, where the output policy is required to exactly satisfy
the constraint. For (i), we prove that the algorithm can return an
$\epsilon$-optimal policy with high probability by using
$\tilde{O}\left(\frac{d^2}{(1-\gamma)^4\epsilon^2}\right)$ samples. We note
that these results exhibit a near-optimal dependence on both $d$ and
$\epsilon$. For (ii), we show that the algorithm requires
$\tilde{O}\left(\frac{d^2}{(1-\gamma)^6\epsilon^2\zeta^2}\right)$ samples,
where $\zeta$ is the problem-dependent Slater constant that characterizes the
size of the feasible region. Finally, we instantiate our framework for tabular
CMDPs and show that it can be used to recover near-optimal sample complexities
in this setting.

</details>


### [75] [Energy-Based Transformers are Scalable Learners and Thinkers](https://arxiv.org/abs/2507.02092)
*Alexi Gladstone,Ganesh Nanduru,Md Mofijul Islam,Peixuan Han,Hyeonjeong Ha,Aman Chadha,Yilun Du,Heng Ji,Jundong Li,Tariq Iqbal*

Main category: cs.LG

TL;DR: 论文提出了一种新型的Energy-Based Transformers（EBTs），通过无监督学习实现类似人类System 2 Thinking的推理能力，并在多模态任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有推理方法存在模态或问题特异性，且需要额外监督训练。本文旨在探索是否可以通过无监督学习实现通用的System 2 Thinking能力。

Method: 训练EBTs模型，通过能量最小化优化输入与候选预测的兼容性，实现推理。

Result: EBTs在训练和推理中均优于Transformer++和Diffusion Transformers，泛化能力更强。

Conclusion: EBTs是一种有潜力的新范式，可同时提升模型的学习和推理能力。

Abstract: Inference-time computation techniques, analogous to human System 2 Thinking,
have recently become popular for improving model performances. However, most
existing approaches suffer from several limitations: they are modality-specific
(e.g., working only in text), problem-specific (e.g., verifiable domains like
math and coding), or require additional supervision/training on top of
unsupervised pretraining (e.g., verifiers or verifiable rewards). In this
paper, we ask the question "Is it possible to generalize these System 2
Thinking approaches, and develop models that learn to think solely from
unsupervised learning?" Interestingly, we find the answer is yes, by learning
to explicitly verify the compatibility between inputs and
candidate-predictions, and then re-framing prediction problems as optimization
with respect to this verifier. Specifically, we train Energy-Based Transformers
(EBTs) -- a new class of Energy-Based Models (EBMs) -- to assign an energy
value to every input and candidate-prediction pair, enabling predictions
through gradient descent-based energy minimization until convergence. Across
both discrete (text) and continuous (visual) modalities, we find EBTs scale
faster than the dominant Transformer++ approach during training, achieving an
up to 35% higher scaling rate with respect to data, batch size, parameters,
FLOPs, and depth. During inference, EBTs improve performance with System 2
Thinking by 29% more than the Transformer++ on language tasks, and EBTs
outperform Diffusion Transformers on image denoising while using fewer forward
passes. Further, we find that EBTs achieve better results than existing models
on most downstream tasks given the same or worse pretraining performance,
suggesting that EBTs generalize better than existing approaches. Consequently,
EBTs are a promising new paradigm for scaling both the learning and thinking
capabilities of models.

</details>


### [76] [Parametric Neural Amp Modeling with Active Learning](https://arxiv.org/abs/2507.02109)
*Florian Grötschla,Luca A. Lanzendörfer,Longxiang Jiao,Roger Wattenhofer*

Main category: cs.LG

TL;DR: PANAMA是一个基于WaveNet架构的主动学习框架，用于训练端到端参数化吉他放大器模型，通过梯度优化算法选择最优数据点，减少所需样本量。


<details>
  <summary>Details</summary>
Motivation: 传统吉他放大器建模需要大量数据点，PANAMA旨在通过主动学习策略减少所需数据量，提高效率。

Method: 采用WaveNet-like架构，结合梯度优化算法主动选择最优数据点（如放大器旋钮设置）。

Result: 在样本量受限的情况下，该方法有效减少了所需数据点数量。

Conclusion: PANAMA为吉他放大器建模提供了一种高效的数据采样方法，适用于样本受限的场景。

Abstract: We introduce PANAMA, an active learning framework for the training of
end-to-end parametric guitar amp models using a WaveNet-like architecture. With
\model, one can create a virtual amp by recording samples that are determined
by an active learning strategy to use a minimum amount of datapoints (i.e., amp
knob settings). We show that gradient-based optimization algorithms can be used
to determine the optimal datapoints to sample, and that the approach helps
under a constrained number of samples.

</details>


### [77] [Scaling Collapse Reveals Universal Dynamics in Compute-Optimally Trained Neural Networks](https://arxiv.org/abs/2507.02119)
*Shikai Qiu,Lechao Xiao,Andrew Gordon Wilson,Jeffrey Pennington,Atish Agarwala*

Main category: cs.LG

TL;DR: 研究发现，在模型规模和训练时间同步增长时，神经网络的训练动态表现出一种精确的普适性，即不同规模模型的损失曲线可以归一化为一条通用曲线。


<details>
  <summary>Details</summary>
Motivation: 探索神经网络训练动态的普适规律，尤其是在模型规模和训练时间同步增长时的行为。

Method: 通过归一化训练计算和损失，分析不同规模模型的损失曲线，并结合学习率衰减和SGD噪声动态模型进行解释。

Result: 发现损失曲线可以归一化为一条通用曲线（称为“超级坍缩”），并验证了其在多种学习率调度、数据集和架构中的普适性。

Conclusion: 超级坍缩现象为神经网络训练的优化提供了精确的指标，并揭示了其与典型神经缩放定律的幂律结构之间的联系。

Abstract: What scaling limits govern neural network training dynamics when model size
and training time grow in tandem? We show that despite the complex interactions
between architecture, training algorithms, and data, compute-optimally trained
models exhibit a remarkably precise universality. Specifically, loss curves
from models of varying sizes collapse onto a single universal curve when
training compute and loss are normalized to unity at the end of training. With
learning rate decay, the collapse becomes so tight that differences in the
normalized curves across models fall below the noise floor of individual loss
curves across random seeds, a phenomenon we term supercollapse. We observe
supercollapse across learning rate schedules, datasets, and architectures,
including transformers trained on next-token prediction, and find it breaks
down when hyperparameters are scaled suboptimally, providing a precise and
practical indicator of good scaling. We explain these phenomena by connecting
collapse to the power-law structure in typical neural scaling laws, and
analyzing a simple yet surprisingly effective model of SGD noise dynamics that
accurately predicts loss curves across various learning rate schedules and
quantitatively explains the origin of supercollapse.

</details>


### [78] [CROP: Circuit Retrieval and Optimization with Parameter Guidance using LLMs](https://arxiv.org/abs/2507.02128)
*Jingyu Pan,Isaac Jacobson,Zheng Zhao,Tung-Chieh Chen,Guanglei Zhou,Chen-Chia Chang,Vineet Rashingkar,Yiran Chen*

Main category: cs.LG

TL;DR: CROP是一个基于大型语言模型（LLM）的自动VLSI设计流程调优框架，通过向量化RTL代码、相似电路匹配和检索增强生成技术，显著提升了设计效率和结果质量。


<details>
  <summary>Details</summary>
Motivation: 解决EDA工具参数空间庞大、手动调优效率低且依赖专家经验的问题。

Method: 1. 将RTL代码转换为密集向量表示；2. 基于嵌入的检索系统匹配相似电路；3. 使用RAG增强的LLM引导参数搜索。

Result: 实验显示CROP在工业设计中以更少迭代次数实现更优结果，功耗降低9.9%。

Conclusion: CROP为VLSI设计自动化提供了高效且可扩展的解决方案。

Abstract: Modern very large-scale integration (VLSI) design requires the implementation
of integrated circuits using electronic design automation (EDA) tools. Due to
the complexity of EDA algorithms, the vast parameter space poses a huge
challenge to chip design optimization, as the combination of even moderate
numbers of parameters creates an enormous solution space to explore. Manual
parameter selection remains industrial practice despite being excessively
laborious and limited by expert experience. To address this issue, we present
CROP, the first large language model (LLM)-powered automatic VLSI design flow
tuning framework. Our approach includes: (1) a scalable methodology for
transforming RTL source code into dense vector representations, (2) an
embedding-based retrieval system for matching designs with semantically similar
circuits, and (3) a retrieval-augmented generation (RAG)-enhanced LLM-guided
parameter search system that constrains the search process with prior knowledge
from similar designs. Experiment results demonstrate CROP's ability to achieve
superior quality-of-results (QoR) with fewer iterations than existing
approaches on industrial designs, including a 9.9% reduction in power
consumption.

</details>


### [79] [Generative Latent Diffusion for Efficient Spatiotemporal Data Reduction](https://arxiv.org/abs/2507.02129)
*Xiao Li,Liangji Zhu,Anand Rangarajan,Sanjay Ranka*

Main category: cs.LG

TL;DR: 提出了一种高效的潜在扩散框架，结合变分自编码器和条件扩散模型，显著提升了数据压缩的性能和可控性。


<details>
  <summary>Details</summary>
Motivation: 生成模型在条件设置下表现优异，但可控性和重建精度限制了其在数据压缩中的实际应用。

Method: 通过变分自编码器和条件扩散模型结合，仅压缩少量关键帧到潜在空间，并利用生成插值重建其余帧。

Result: 实验表明，该方法压缩比优于SZ3等规则方法10倍，比学习类方法性能提升63%。

Conclusion: 该方法在显著降低存储成本的同时，实现了高精度的时空重建。

Abstract: Generative models have demonstrated strong performance in conditional
settings and can be viewed as a form of data compression, where the condition
serves as a compact representation. However, their limited controllability and
reconstruction accuracy restrict their practical application to data
compression. In this work, we propose an efficient latent diffusion framework
that bridges this gap by combining a variational autoencoder with a conditional
diffusion model. Our method compresses only a small number of keyframes into
latent space and uses them as conditioning inputs to reconstruct the remaining
frames via generative interpolation, eliminating the need to store latent
representations for every frame. This approach enables accurate spatiotemporal
reconstruction while significantly reducing storage costs. Experimental results
across multiple datasets show that our method achieves up to 10 times higher
compression ratios than rule-based state-of-the-art compressors such as SZ3,
and up to 63 percent better performance than leading learning-based methods
under the same reconstruction error.

</details>


### [80] [Non-exchangeable Conformal Prediction for Temporal Graph Neural Networks](https://arxiv.org/abs/2507.02151)
*Tuo Wang,Jian Kang,Yujun Yan,Adithya Kulkarni,Dawei Zhou*

Main category: cs.LG

TL;DR: NCPNET是一个针对时序图的端到端共形预测框架，解决了现有方法在动态图数据中的局限性，通过扩散非共形分数和效率优化算法显著提升了预测效率和覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现有共形预测方法主要针对静态图，忽略了现实图中时序依赖性的影响，导致覆盖率假设失效。

Method: 提出NCPNET框架，包括扩散非共形分数和效率优化算法，以处理时序图中的拓扑和时间不确定性。

Result: 在多个真实时序图数据集上验证，NCPNET显著减少预测集大小（如WIKI数据集减少31%），并确保覆盖率。

Conclusion: NCPNET为时序图提供了一种高效的共形预测方法，显著提升了动态图数据中的不确定量化能力。

Abstract: Conformal prediction for graph neural networks (GNNs) offers a promising
framework for quantifying uncertainty, enhancing GNN reliability in high-stakes
applications. However, existing methods predominantly focus on static graphs,
neglecting the evolving nature of real-world graphs. Temporal dependencies in
graph structure, node attributes, and ground truth labels violate the
fundamental exchangeability assumption of standard conformal prediction
methods, limiting their applicability. To address these challenges, in this
paper, we introduce NCPNET, a novel end-to-end conformal prediction framework
tailored for temporal graphs. Our approach extends conformal prediction to
dynamic settings, mitigating statistical coverage violations induced by
temporal dependencies. To achieve this, we propose a diffusion-based
non-conformity score that captures both topological and temporal uncertainties
within evolving networks. Additionally, we develop an efficiency-aware
optimization algorithm that improves the conformal prediction process,
enhancing computational efficiency and reducing coverage violations. Extensive
experiments on diverse real-world temporal graphs, including WIKI, REDDIT,
DBLP, and IBM Anti-Money Laundering dataset, demonstrate NCPNET's capability to
ensure guaranteed coverage in temporal graphs, achieving up to a 31% reduction
in prediction set size on the WIKI dataset, significantly improving efficiency
compared to state-of-the-art methods. Our data and code are available at
https://github.com/ODYSSEYWT/NCPNET.

</details>


### [81] [Statistical Inference for Responsiveness Verification](https://arxiv.org/abs/2507.02169)
*Seung Hyun Cheon,Meredith Stewart,Bogdan Kulynych,Tsui-Wei Weng,Berk Ustun*

Main category: cs.LG

TL;DR: 论文提出了一种验证机器学习模型预测响应性的方法，通过干预特征分析敏感性，支持黑盒模型评估，应用于实际场景如再犯预测和内容审核。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在贷款、招聘等场景中因未考虑个体输入变化而导致安全失败，需验证预测对干预的响应性。

Method: 通过约束干预和下游效应分布，提出黑盒估计方法，生成可达点均匀样本以评估响应性。

Result: 开发算法支持预测的响应性估计，应用于再犯预测、器官移植优先级和内容审核等实际任务。

Conclusion: 该方法能提升机器学习模型在实际应用中的安全性，支持故障概率估计和验证任务。

Abstract: Many safety failures in machine learning arise when models are used to assign
predictions to people (often in settings like lending, hiring, or content
moderation) without accounting for how individuals can change their inputs. In
this work, we introduce a formal validation procedure for the responsiveness of
predictions with respect to interventions on their features. Our procedure
frames responsiveness as a type of sensitivity analysis in which practitioners
control a set of changes by specifying constraints over interventions and
distributions over downstream effects. We describe how to estimate
responsiveness for the predictions of any model and any dataset using only
black-box access, and how to use these estimates to support tasks such as
falsification and failure probability estimation. We develop algorithms that
construct these estimates by generating a uniform sample of reachable points,
and demonstrate how they can promote safety in real-world applications such as
recidivism prediction, organ transplant prioritization, and content moderation.

</details>


### [82] [Metric Design != Metric Behavior: Improving Metric Selection for the Unbiased Evaluation of Dimensionality Reduction](https://arxiv.org/abs/2507.02225)
*Jiyeon Bae,Hyeon Jeon,Jinwook Seo*

Main category: cs.LG

TL;DR: 提出一种新工作流程，通过基于经验相关性聚类评估指标，减少维度降维（DR）投影评估中的偏差。


<details>
  <summary>Details</summary>
Motivation: 现有评估指标可能因高度相关性导致评估偏差，影响维度降维技术的可靠性。

Method: 通过计算指标间的成对相关性，聚类以减少重叠，并从每个簇中选择代表性指标。

Result: 实验表明，该方法提高了DR评估的稳定性，减少了评估偏差。

Conclusion: 提出的工作流程有效减少了评估偏差，提升了维度降维投影的可靠性。

Abstract: Evaluating the accuracy of dimensionality reduction (DR) projections in
preserving the structure of high-dimensional data is crucial for reliable
visual analytics. Diverse evaluation metrics targeting different structural
characteristics have thus been developed. However, evaluations of DR
projections can become biased if highly correlated metrics--those measuring
similar structural characteristics--are inadvertently selected, favoring DR
techniques that emphasize those characteristics. To address this issue, we
propose a novel workflow that reduces bias in the selection of evaluation
metrics by clustering metrics based on their empirical correlations rather than
on their intended design characteristics alone. Our workflow works by computing
metric similarity using pairwise correlations, clustering metrics to minimize
overlap, and selecting a representative metric from each cluster. Quantitative
experiments demonstrate that our approach improves the stability of DR
evaluation, which indicates that our workflow contributes to mitigating
evaluation bias.

</details>


### [83] [PhysicsCorrect: A Training-Free Approach for Stable Neural PDE Simulations](https://arxiv.org/abs/2507.02227)
*Xinquan Huang,Paris Perdikaris*

Main category: cs.LG

TL;DR: PhysicsCorrect是一个无需训练的校正框架，通过基于PDE残差的线性化逆问题解决神经网络在长期预测中的误差累积问题，显著提升物理模拟的准确性。


<details>
  <summary>Details</summary>
Motivation: 神经网络作为PDE求解的代理模型存在长期预测误差累积的问题，导致与物理有效解完全偏离。

Method: 提出PhysicsCorrect框架，利用离线预计算Jacobian及其伪逆的高效缓存策略，显著降低计算开销。

Result: 在Navier-Stokes流体动力学、波动方程和Kuramoto-Sivashinsky混沌方程中，预测误差降低100倍，推理时间增加不到5%。

Conclusion: PhysicsCorrect将不稳定的神经代理转化为可靠的模拟工具，平衡了深度学习的高效计算和实际科学应用对物理保真度的需求。

Abstract: Neural networks have emerged as powerful surrogates for solving partial
differential equations (PDEs), offering significant computational speedups over
traditional methods. However, these models suffer from a critical limitation:
error accumulation during long-term rollouts, where small inaccuracies compound
exponentially, eventually causing complete divergence from physically valid
solutions. We present PhysicsCorrect, a training-free correction framework that
enforces PDE consistency at each prediction step by formulating correction as a
linearized inverse problem based on PDE residuals. Our key innovation is an
efficient caching strategy that precomputes the Jacobian and its pseudoinverse
during an offline warm-up phase, reducing computational overhead by two orders
of magnitude compared to standard correction approaches. Across three
representative PDE systems -- Navier-Stokes fluid dynamics, wave equations, and
the chaotic Kuramoto-Sivashinsky equation -- PhysicsCorrect reduces prediction
errors by up to 100x while adding negligible inference time (under 5\%). The
framework integrates seamlessly with diverse architectures including Fourier
Neural Operators, UNets, and Vision Transformers, effectively transforming
unstable neural surrogates into reliable simulation tools that bridge the gap
between deep learning's computational efficiency and the physical fidelity
demanded by practical scientific applications.

</details>


### [84] [VERBA: Verbalizing Model Differences Using Large Language Models](https://arxiv.org/abs/2507.02241)
*Shravan Doda,Shashidhar Reddy Javaji,Zining Zhu*

Main category: cs.LG

TL;DR: 论文提出了一种名为VERBA的方法，利用大语言模型（LLM）生成模型间的差异描述，解决了模型选择中大量手动比较的问题。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习领域存在大量性能相似但行为不同的模型，手动比较模型对用户和开发者来说成本过高。

Method: VERBA通过采样两个模型的行为，利用LLM生成差异描述，并通过模拟评估描述的信息量。

Result: VERBA在决策树模型上能准确描述差异（80%准确率），加入结构信息后提升至90%。

Conclusion: VERBA为提升机器学习模型的透明度和可比性提供了新思路。

Abstract: In the current machine learning landscape, we face a "model lake" phenomenon:
Given a task, there is a proliferation of trained models with similar
performances despite different behavior. For model users attempting to navigate
and select from the models, documentation comparing model pairs is helpful.
However, for every $N$ models there could be $O(N^2)$ pairwise comparisons, a
number prohibitive for the model developers to manually perform pairwise
comparisons and prepare documentations. To facilitate fine-grained pairwise
comparisons among models, we introduced $\textbf{VERBA}$. Our approach
leverages a large language model (LLM) to generate verbalizations of model
differences by sampling from the two models. We established a protocol that
evaluates the informativeness of the verbalizations via simulation. We also
assembled a suite with a diverse set of commonly used machine learning models
as a benchmark. For a pair of decision tree models with up to 5% performance
difference but 20-25% behavioral differences, $\textbf{VERBA}$ effectively
verbalizes their variations with up to 80% overall accuracy. When we included
the models' structural information, the verbalization's accuracy further
improved to 90%. $\textbf{VERBA}$ opens up new research avenues for improving
the transparency and comparability of machine learning models in a post-hoc
manner.

</details>


### [85] [Order Acquisition Under Competitive Pressure: A Rapidly Adaptive Reinforcement Learning Approach for Ride-Hailing Subsidy Strategies](https://arxiv.org/abs/2507.02244)
*Fangzhou Shi,Xiaopeng Ke,Xinye Xiong,Kexin Meng,Chang Men,Zhengdan Zhu*

Main category: cs.LG

TL;DR: 论文提出了一种基于强化学习的补贴策略框架FCA-RL，用于动态适应竞争对手的价格调整，优化网约车服务提供商的订单获取。


<details>
  <summary>Details</summary>
Motivation: 网约车聚合平台通过低价排名机制激励服务提供商采用优惠策略，但现有研究稀缺，动态适应市场波动和预算约束的优惠策略设计成为关键挑战。

Method: 提出FCA-RL框架，结合快速竞争适应（FCA）和强化拉格朗日调整（RLA），并开发专用模拟环境RideGym进行策略评估。

Result: 实验表明，FCA-RL在多样化市场条件下优于基线方法，有效优化了补贴策略。

Conclusion: FCA-RL为网约车服务提供商提供了一种动态适应竞争环境的高效补贴优化方案。

Abstract: The proliferation of ride-hailing aggregator platforms presents significant
growth opportunities for ride-service providers by increasing order volume and
gross merchandise value (GMV). On most ride-hailing aggregator platforms,
service providers that offer lower fares are ranked higher in listings and,
consequently, are more likely to be selected by passengers. This competitive
ranking mechanism creates a strong incentive for service providers to adopt
coupon strategies that lower prices to secure a greater number of orders, as
order volume directly influences their long-term viability and sustainability.
Thus, designing an effective coupon strategy that can dynamically adapt to
market fluctuations while optimizing order acquisition under budget constraints
is a critical research challenge. However, existing studies in this area remain
scarce.
  To bridge this gap, we propose FCA-RL, a novel reinforcement learning-based
subsidy strategy framework designed to rapidly adapt to competitors' pricing
adjustments. Our approach integrates two key techniques: Fast Competition
Adaptation (FCA), which enables swift responses to dynamic price changes, and
Reinforced Lagrangian Adjustment (RLA), which ensures adherence to budget
constraints while optimizing coupon decisions on new price landscape.
Furthermore, we introduce RideGym, the first dedicated simulation environment
tailored for ride-hailing aggregators, facilitating comprehensive evaluation
and benchmarking of different pricing strategies without compromising
real-world operational efficiency. Experimental results demonstrate that our
proposed method consistently outperforms baseline approaches across diverse
market conditions, highlighting its effectiveness in subsidy optimization for
ride-hailing service providers.

</details>


### [86] [Uncertainty-aware Reward Design Process](https://arxiv.org/abs/2507.02256)
*Yang Yang,Xiaolu Zhou,Bosong Ding,Miao Xin*

Main category: cs.LG

TL;DR: 论文提出了一种名为URDP的新框架，结合大语言模型和贝叶斯优化，以高效设计强化学习中的奖励函数。


<details>
  <summary>Details</summary>
Motivation: 传统奖励函数设计方法效率低且不一致，现有的大语言模型在数值优化上表现不佳，进化搜索范式则计算资源消耗大。

Method: URDP通过自一致性分析量化奖励函数的不确定性，引入不确定性感知贝叶斯优化（UABO），并采用双层优化架构。

Result: 在35个任务上的实验表明，URDP能生成更高质量的奖励函数，且设计效率显著提升。

Conclusion: URDP通过结合大语言模型的逻辑推理和贝叶斯优化的数值优化能力，解决了奖励函数设计的效率和质量问题。

Abstract: Designing effective reward functions is a cornerstone of reinforcement
learning (RL), yet it remains a challenging process due to the inefficiencies
and inconsistencies inherent in conventional reward engineering methodologies.
Recent advances have explored leveraging large language models (LLMs) to
automate reward function design. However, their suboptimal performance in
numerical optimization often yields unsatisfactory reward quality, while the
evolutionary search paradigm demonstrates inefficient utilization of simulation
resources, resulting in prohibitively lengthy design cycles with
disproportionate computational overhead. To address these challenges, we
propose the Uncertainty-aware Reward Design Process (URDP), a novel framework
that integrates large language models to streamline reward function design and
evaluation in RL environments. URDP quantifies candidate reward function
uncertainty based on self-consistency analysis, enabling simulation-free
identification of ineffective reward components while discovering novel reward
components. Furthermore, we introduce uncertainty-aware Bayesian optimization
(UABO), which incorporates uncertainty estimation to significantly enhance
hyperparameter configuration efficiency. Finally, we construct a bi-level
optimization architecture by decoupling the reward component optimization and
the hyperparameter tuning. URDP orchestrates synergistic collaboration between
the reward logic reasoning of the LLMs and the numerical optimization strengths
of the Bayesian Optimization. We conduct a comprehensive evaluation of URDP
across 35 diverse tasks spanning three benchmark environments. Our experimental
results demonstrate that URDP not only generates higher-quality reward
functions but also achieves significant improvements in the efficiency of
automated reward design compared to existing approaches.

</details>


### [87] [Knowledge Graph-Based Explainable and Generalized Zero-Shot Semantic Communications](https://arxiv.org/abs/2507.02291)
*Zhaoyu Zhang,Lingyi Wang,Wei Wu,Fuhui Zhou,Qihui Wu*

Main category: cs.LG

TL;DR: 论文提出了一种基于知识图谱的零样本语义通信网络（KGZS-SC），通过结构化语义信息提升泛化能力和推理能力，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动的语义通信缺乏可解释性和泛化能力，尤其在处理未见数据时表现不佳。

Method: 利用知识图谱语义知识库（KG-SKB）对齐语义特征，结合零样本学习（ZSL）实现直接分类，减少通信开销。

Result: 在APY数据集上的实验表明，KGZS-SC网络在未见类别分类任务中表现优异，且适应性强。

Conclusion: KGZS-SC网络通过知识图谱和零样本学习提升了语义通信的泛化能力和效率。

Abstract: Data-driven semantic communication is based on superficial statistical
patterns, thereby lacking interpretability and generalization, especially for
applications with the presence of unseen data. To address these challenges, we
propose a novel knowledge graph-enhanced zero-shot semantic communication
(KGZS-SC) network. Guided by the structured semantic information from a
knowledge graph-based semantic knowledge base (KG-SKB), our scheme provides
generalized semantic representations and enables reasoning for unseen cases.
Specifically, the KG-SKB aligns the semantic features in a shared category
semantics embedding space and enhances the generalization ability of the
transmitter through aligned semantic features, thus reducing communication
overhead by selectively transmitting compact visual semantics. At the receiver,
zero-shot learning (ZSL) is leveraged to enable direct classification for
unseen cases without the demand for retraining or additional computational
overhead, thereby enhancing the adaptability and efficiency of the
classification process in dynamic or resource-constrained environments. The
simulation results conducted on the APY datasets show that the proposed KGZS-SC
network exhibits robust generalization and significantly outperforms existing
SC frameworks in classifying unseen categories across a range of SNR levels.

</details>


### [88] [Holistic Continual Learning under Concept Drift with Adaptive Memory Realignment](https://arxiv.org/abs/2507.02310)
*Alif Ashrafee,Jedrzej Kozal,Michal Wozniak,Bartosz Krawczyk*

Main category: cs.LG

TL;DR: 论文提出了一种名为AMR的轻量级方法，用于解决持续学习中的概念漂移问题，通过选择性更新记忆缓冲区来减少标注和计算开销。


<details>
  <summary>Details</summary>
Motivation: 传统持续学习方法假设数据分布静态，忽视了现实数据流的动态性，导致无法适应概念漂移。

Method: 提出AMR方法，通过选择性移除过时样本并补充新样本，实现记忆缓冲区的动态对齐。

Result: AMR在多个数据集上表现优异，性能接近完全重新训练，但显著减少了资源消耗。

Conclusion: AMR是一种可扩展的解决方案，能够在非静态环境中平衡稳定性和可塑性。

Abstract: Traditional continual learning methods prioritize knowledge retention and
focus primarily on mitigating catastrophic forgetting, implicitly assuming that
the data distribution of previously learned tasks remains static. This
overlooks the dynamic nature of real-world data streams, where concept drift
permanently alters previously seen data and demands both stability and rapid
adaptation.
  We introduce a holistic framework for continual learning under concept drift
that simulates realistic scenarios by evolving task distributions. As a
baseline, we consider Full Relearning (FR), in which the model is retrained
from scratch on newly labeled samples from the drifted distribution. While
effective, this approach incurs substantial annotation and computational
overhead. To address these limitations, we propose Adaptive Memory Realignment
(AMR), a lightweight alternative that equips rehearsal-based learners with a
drift-aware adaptation mechanism. AMR selectively removes outdated samples of
drifted classes from the replay buffer and repopulates it with a small number
of up-to-date instances, effectively realigning memory with the new
distribution. This targeted resampling matches the performance of FR while
reducing the need for labeled data and computation by orders of magnitude.
  To enable reproducible evaluation, we introduce four concept-drift variants
of standard vision benchmarks: Fashion-MNIST-CD, CIFAR10-CD, CIFAR100-CD, and
Tiny-ImageNet-CD, where previously seen classes reappear with shifted
representations. Comprehensive experiments on these datasets using several
rehearsal-based baselines show that AMR consistently counters concept drift,
maintaining high accuracy with minimal overhead. These results position AMR as
a scalable solution that reconciles stability and plasticity in non-stationary
continual learning environments.

</details>


### [89] [Improving Constrained Generation in Language Models via Self-Distilled Twisted Sequential Monte Carlo](https://arxiv.org/abs/2507.02315)
*Sooyeon Kim,Giung Nam,Juho Lee*

Main category: cs.LG

TL;DR: 通过自蒸馏迭代优化基础模型，提升受限文本生成质量。


<details>
  <summary>Details</summary>
Motivation: 在受限文本生成中，目标分布集中于基础模型不太可能的输出，导致学习困难。

Method: 采用自蒸馏方法迭代优化基础模型，使其逐步与目标对齐。

Result: 显著提升了生成质量。

Conclusion: 自蒸馏是一种有效解决受限生成中学习挑战的方法。

Abstract: Recent work has framed constrained text generation with autoregressive
language models as a probabilistic inference problem. Among these, Zhao et al.
(2024) introduced a promising approach based on twisted Sequential Monte Carlo,
which incorporates learned twist functions and twist-induced proposals to guide
the generation process. However, in constrained generation settings where the
target distribution concentrates on outputs that are unlikely under the base
model, learning becomes challenging due to sparse and uninformative reward
signals. We show that iteratively refining the base model through
self-distillation alleviates this issue by making the model progressively more
aligned with the target, leading to substantial gains in generation quality.

</details>


### [90] [Transformer-based EEG Decoding: A Survey](https://arxiv.org/abs/2507.02320)
*Haodong Zhang,Hongqi Li*

Main category: cs.LG

TL;DR: 本文综述了Transformer模型在EEG解码中的最新应用，包括其基础架构、混合架构及定制化改进，并探讨了当前挑战与未来发展方向。


<details>
  <summary>Details</summary>
Motivation: EEG信号解码是脑机接口研究的核心，传统机器学习方法存在局限性，而深度学习方法（尤其是Transformer）因其自动学习特征的能力逐渐成为主流。本文旨在总结Transformer在EEG解码中的应用进展。

Method: 文章首先介绍Transformer的基础及其直接应用，然后详细分析其与其他深度学习技术（如卷积/循环神经网络、生成对抗网络等）的混合架构，并探讨定制化Transformer的改进。

Result: Transformer在EEG解码中表现出色，其混合架构和定制化改进进一步提升了性能。

Conclusion: 尽管Transformer在EEG解码中取得了显著进展，但仍面临挑战，未来需进一步优化模型架构和应用场景。

Abstract: Electroencephalography (EEG) is one of the most common signals used to
capture the electrical activity of the brain, and the decoding of EEG, to
acquire the user intents, has been at the forefront of brain-computer/machine
interfaces (BCIs/BMIs) research. Compared to traditional EEG analysis methods
with machine learning, the advent of deep learning approaches have gradually
revolutionized the field by providing an end-to-end long-cascaded architecture,
which can learn more discriminative features automatically. Among these,
Transformer is renowned for its strong handling capability of sequential data
by the attention mechanism, and the application of Transformers in various EEG
processing tasks is increasingly prevalent. This article delves into a relevant
survey, summarizing the latest application of Transformer models in EEG
decoding since it appeared. The evolution of the model architecture is followed
to sort and organize the related advances, in which we first elucidate the
fundamentals of the Transformer that benefits EEG decoding and its direct
application. Then, the common hybrid architectures by integrating basic
Transformer with other deep learning techniques
(convolutional/recurrent/graph/spiking neural netwo-rks, generative adversarial
networks, diffusion models, etc.) is overviewed in detail. The research
advances of applying the modified intrinsic structures of customized
Transformer have also been introduced. Finally, the current challenges and
future development prospects in this rapidly evolving field are discussed. This
paper aims to help readers gain a clear understanding of the current state of
Transformer applications in EEG decoding and to provide valuable insights for
future research endeavors.

</details>


### [91] [DeltaSHAP: Explaining Prediction Evolutions in Online Patient Monitoring with Shapley Values](https://arxiv.org/abs/2507.02342)
*Changhun Kim,Yechan Mun,Sangchul Hahn,Eunho Yang*

Main category: cs.LG

TL;DR: DeltaSHAP是一种新型XAI算法，专注于在线患者监测系统，通过解释连续预测变化、提供特征归因的大小和方向，并在实时环境中高效运行。


<details>
  <summary>Details</summary>
Motivation: 现有XAI方法无法满足临床时间序列解释的需求，DeltaSHAP旨在解决这一缺口，满足临床环境中对患者风险演变的实时解释需求。

Method: 通过调整Shapley值以适应时间序列设置，DeltaSHAP捕捉特征联合效应，并仅使用实际观察到的特征组合进行预测变化归因。

Result: 在MIMIC-III基准测试中，DeltaSHAP在解释质量上优于现有方法62%，计算效率提升33%。

Conclusion: DeltaSHAP为临床时间序列解释提供了高效、实用的解决方案，适用于实时患者监测。

Abstract: This study proposes DeltaSHAP, a novel explainable artificial intelligence
(XAI) algorithm specifically designed for online patient monitoring systems. In
clinical environments, discovering the causes driving patient risk evolution is
critical for timely intervention, yet existing XAI methods fail to address the
unique requirements of clinical time series explanation tasks. To this end,
DeltaSHAP addresses three key clinical needs: explaining the changes in the
consecutive predictions rather than isolated prediction scores, providing both
magnitude and direction of feature attributions, and delivering these insights
in real time. By adapting Shapley values to temporal settings, our approach
accurately captures feature coalition effects. It further attributes prediction
changes using only the actually observed feature combinations, making it
efficient and practical for time-sensitive clinical applications. We also
introduce new evaluation metrics to evaluate the faithfulness of the
attributions for online time series, and demonstrate through experiments on
online patient monitoring tasks that DeltaSHAP outperforms state-of-the-art XAI
methods in both explanation quality as 62% and computational efficiency as 33%
time reduction on the MIMIC-III decompensation benchmark. We release our code
at https://github.com/AITRICS/DeltaSHAP.

</details>


### [92] [Offline Reinforcement Learning with Penalized Action Noise Injection](https://arxiv.org/abs/2507.02356)
*JunHyeok Oh,Byung-Jun Lee*

Main category: cs.LG

TL;DR: 提出了一种名为PANI的离线强化学习方法，通过噪声注入动作覆盖整个动作空间，并惩罚噪声量，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习在环境交互成本高时具有实用性，但需提升泛化能力。扩散模型虽有效但计算成本高，PANI提供了一种更简单高效的替代方案。

Method: PANI方法通过噪声注入动作扩展动作空间，并根据噪声量进行惩罚，理论证明其解决了一个修改的MDP（噪声动作MDP）。

Result: PANI与多种离线强化学习算法兼容，实验表明其在多个基准测试中性能显著提升。

Conclusion: PANI是一种简单高效的离线强化学习方法，无需复杂扩散模型即可实现高性能。

Abstract: Offline reinforcement learning (RL) optimizes a policy using only a fixed
dataset, making it a practical approach in scenarios where interaction with the
environment is costly. Due to this limitation, generalization ability is key to
improving the performance of offline RL algorithms, as demonstrated by recent
successes of offline RL with diffusion models. However, it remains questionable
whether such diffusion models are necessary for highly performing offline RL
algorithms, given their significant computational requirements during
inference. In this paper, we propose Penalized Action Noise Injection (PANI), a
method that simply enhances offline learning by utilizing noise-injected
actions to cover the entire action space, while penalizing according to the
amount of noise injected. This approach is inspired by how diffusion models
have worked in offline RL algorithms. We provide a theoretical foundation for
this method, showing that offline RL algorithms with such noise-injected
actions solve a modified Markov Decision Process (MDP), which we call the noisy
action MDP. PANI is compatible with a wide range of existing off-policy and
offline RL algorithms, and despite its simplicity, it demonstrates significant
performance improvements across various benchmarks.

</details>


### [93] [Deep Reinforcement Learning-Based DRAM Equalizer Parameter Optimization Using Latent Representations](https://arxiv.org/abs/2507.02365)
*Muhammad Usama,Dong Eui Chang*

Main category: cs.LG

TL;DR: 本文提出了一种数据驱动框架，结合潜在信号表示和强化学习，优化高速DRAM系统中的信号完整性，显著提升了眼图性能。


<details>
  <summary>Details</summary>
Motivation: 高速DRAM系统中信号完整性优化通常计算量大或依赖模型，需要更高效、模型无关的方法。

Method: 使用潜在信号表示快速评估信号完整性，结合无模型强化学习（Advantage Actor-Critic）优化均衡器参数。

Result: 在行业标准DRAM波形上，眼图窗口面积显著提升（CTLE+DFE结构42.7%，DFE-only结构36.8%）。

Conclusion: 该方法在性能、计算效率和泛化能力上优于现有技术，适用于复杂均衡器架构。

Abstract: Equalizer parameter optimization for signal integrity in high-speed Dynamic
Random Access Memory systems is crucial but often computationally demanding or
model-reliant. This paper introduces a data-driven framework employing learned
latent signal representations for efficient signal integrity evaluation,
coupled with a model-free Advantage Actor-Critic reinforcement learning agent
for parameter optimization. The latent representation captures vital signal
integrity features, offering a fast alternative to direct eye diagram analysis
during optimization, while the reinforcement learning agent derives optimal
equalizer settings without explicit system models. Applied to industry-standard
Dynamic Random Access Memory waveforms, the method achieved significant
eye-opening window area improvements: 42.7\% for cascaded Continuous-Time
Linear Equalizer and Decision Feedback Equalizer structures, and 36.8\% for
Decision Feedback Equalizer-only configurations. These results demonstrate
superior performance, computational efficiency, and robust generalization
across diverse Dynamic Random Access Memory units compared to existing
techniques. Core contributions include an efficient latent signal integrity
metric for optimization, a robust model-free reinforcement learning strategy,
and validated superior performance for complex equalizer architectures.

</details>


### [94] [Improving Consistency in Vehicle Trajectory Prediction Through Preference Optimization](https://arxiv.org/abs/2507.02406)
*Caio Azevedo,Lina Achaji,Stefano Sabatini,Nicola Poerio,Grzegorz Bartyzel,Sascha Hornauer,Fabien Moutarde*

Main category: cs.LG

TL;DR: 论文提出了一种通过偏好优化微调轨迹预测模型的方法，以提高多智能体场景中的一致性，同时保持预测精度和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在复杂交互场景中难以捕捉智能体间的依赖关系，导致预测不一致。

Method: 利用偏好优化技术微调轨迹预测模型，输入自动计算的预测未来偏好排名。

Result: 在三个数据集上显著提高了场景一致性，同时预测精度损失极小，且不增加推理时的计算负担。

Conclusion: 偏好优化是提升多智能体轨迹预测一致性的有效方法。

Abstract: Trajectory prediction is an essential step in the pipeline of an autonomous
vehicle. Inaccurate or inconsistent predictions regarding the movement of
agents in its surroundings lead to poorly planned maneuvers and potentially
dangerous situations for the end-user. Current state-of-the-art
deep-learning-based trajectory prediction models can achieve excellent accuracy
on public datasets. However, when used in more complex, interactive scenarios,
they often fail to capture important interdependencies between agents, leading
to inconsistent predictions among agents in the traffic scene. Inspired by the
efficacy of incorporating human preference into large language models, this
work fine-tunes trajectory prediction models in multi-agent settings using
preference optimization. By taking as input automatically calculated preference
rankings among predicted futures in the fine-tuning process, our
experiments--using state-of-the-art models on three separate datasets--show
that we are able to significantly improve scene consistency while minimally
sacrificing trajectory prediction accuracy and without adding any excess
computational requirements at inference time.

</details>


### [95] [S2FGL: Spatial Spectral Federated Graph Learning](https://arxiv.org/abs/2507.02409)
*Zihan Tan,Suyuan Huang,Guancheng Wan,Wenke Huang,He Li,Mang Ye*

Main category: cs.LG

TL;DR: S2FGL框架结合空间和频谱策略，解决联邦图学习中的标签信号中断和频谱客户端漂移问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究仅从结构角度处理子图联邦学习，忽略了图信号在空间和频谱域的传播问题。

Method: 提出全局知识库缓解标签信号中断，频率对齐解决频谱客户端漂移，形成S2FGL框架。

Result: 多数据集实验证明S2FGL的优越性。

Conclusion: S2FGL有效提升联邦图学习的全局泛化能力。

Abstract: Federated Graph Learning (FGL) combines the privacy-preserving capabilities
of federated learning (FL) with the strong graph modeling capability of Graph
Neural Networks (GNNs). Current research addresses subgraph-FL only from the
structural perspective, neglecting the propagation of graph signals on spatial
and spectral domains of the structure. From a spatial perspective, subgraph-FL
introduces edge disconnections between clients, leading to disruptions in label
signals and a degradation in the class knowledge of the global GNN. From a
spectral perspective, spectral heterogeneity causes inconsistencies in signal
frequencies across subgraphs, which makes local GNNs overfit the local signal
propagation schemes. As a result, spectral client drifts occur, undermining
global generalizability. To tackle the challenges, we propose a global
knowledge repository to mitigate label signal disruption and a frequency
alignment to address spectral client drifts. The combination of spatial and
spectral strategies forms our framework S2FGL. Extensive experiments on
multiple datasets demonstrate the superiority of S2FGL. The code is available
at https://github.com/Wonder7racer/S2FGL.git.

</details>


### [96] [Variational Kolmogorov-Arnold Network](https://arxiv.org/abs/2507.02466)
*Francesco Alesiani,Henrik Christiansen,Federico Errica*

Main category: cs.LG

TL;DR: InfinityKAN通过变分推断优化问题自适应学习无限数量的基函数，解决了KANs中基函数数量选择的难题。


<details>
  <summary>Details</summary>
Motivation: KANs基于Kolmogorov-Arnold定理，但其作为MLP替代方案的应用受限于基函数数量的选择问题。

Method: 提出InfinityKAN，通过变分推断和反向传播自适应学习基函数数量。

Result: 扩展了KANs的适用性，将重要超参数纳入学习过程。

Conclusion: InfinityKAN为KANs的实际应用提供了更灵活的解决方案。

Abstract: Kolmogorov Arnold Networks (KANs) are an emerging architecture for building
machine learning models. KANs are based on the theoretical foundation of the
Kolmogorov-Arnold Theorem and its expansions, which provide an exact
representation of a multi-variate continuous bounded function as the
composition of a limited number of univariate continuous functions. While such
theoretical results are powerful, their use as a representation learning
alternative to a multi-layer perceptron (MLP) hinges on the ad-hoc choice of
the number of bases modeling each of the univariate functions. In this work, we
show how to address this problem by adaptively learning a potentially infinite
number of bases for each univariate function during training. We therefore
model the problem as a variational inference optimization problem. Our
proposal, called InfinityKAN, which uses backpropagation, extends the potential
applicability of KANs by treating an important hyperparameter as part of the
learning process.

</details>


### [97] [Online Conformal Prediction with Efficiency Guarantees](https://arxiv.org/abs/2507.02496)
*Vaidehi Srinivas*

Main category: cs.LG

TL;DR: 论文研究了在线框架下的共形预测问题，直接优化效率，比较了交换序列和任意序列下的表现差异。


<details>
  <summary>Details</summary>
Motivation: 探索在线环境中如何构建高效的置信区间，同时满足覆盖率和效率的要求。

Method: 提出了一种确定性算法，针对交换序列和任意序列分别优化覆盖率和区间长度。

Result: 在交换序列下，算法能接近最优覆盖率和长度；在任意序列下，存在效率与错误率之间的权衡。

Conclusion: 交换序列和任意序列下的表现存在显著差异，算法需针对不同场景优化。

Abstract: We study the problem of conformal prediction in a novel online framework that
directly optimizes efficiency. In our problem, we are given a target
miscoverage rate $\alpha > 0$, and a time horizon $T$. On each day $t \le T$ an
algorithm must output an interval $I_t \subseteq [0, 1]$, then a point $y_t \in
[0, 1]$ is revealed. The goal of the algorithm is to achieve coverage, that is,
$y_t \in I_t$ on (close to) a $(1 - \alpha)$-fraction of days, while
maintaining efficiency, that is, minimizing the average volume (length) of the
intervals played. This problem is an online analogue to the problem of
constructing efficient confidence intervals.
  We study this problem over arbitrary and exchangeable (random order) input
sequences. For exchangeable sequences, we show that it is possible to construct
intervals that achieve coverage $(1 - \alpha) - o(1)$, while having length
upper bounded by the best fixed interval that achieves coverage in hindsight.
For arbitrary sequences however, we show that any algorithm that achieves a
$\mu$-approximation in average length compared to the best fixed interval
achieving coverage in hindsight, must make a multiplicative factor more
mistakes than $\alpha T$, where the multiplicative factor depends on $\mu$ and
the aspect ratio of the problem. Our main algorithmic result is a matching
algorithm that can recover all Pareto-optimal settings of $\mu$ and number of
mistakes. Furthermore, our algorithm is deterministic and therefore robust to
an adaptive adversary.
  This gap between the exchangeable and arbitrary settings is in contrast to
the classical online learning problem. In fact, we show that no single
algorithm can simultaneously be Pareto-optimal for arbitrary sequences and
optimal for exchangeable sequences. On the algorithmic side, we give an
algorithm that achieves the near-optimal tradeoff between the two cases.

</details>


### [98] [Continual Gradient Low-Rank Projection Fine-Tuning for LLMs](https://arxiv.org/abs/2507.02503)
*Chenxu Wang,Yilin Lyu,Zicheng Sun,Liping Jing*

Main category: cs.LG

TL;DR: GORP（Gradient LOw Rank Projection）是一种新的训练策略，通过结合全参数和低秩参数，在统一低秩梯度子空间中联合更新，解决了LLMs持续微调中效率与表达能力之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 解决低秩适应（LoRA）在持续学习中因低秩特性和显式参数约束导致的任务学习和知识迁移能力受限的问题。

Method: 提出GORP，结合全参数和低秩参数，在统一低秩梯度子空间中联合更新，扩展优化空间同时保持效率。

Result: 在持续学习基准测试中，GORP表现出优于现有最先进方法的性能。

Conclusion: GORP通过创新的训练策略，有效平衡了效率与表达能力，同时减轻了灾难性遗忘问题。

Abstract: Continual fine-tuning of Large Language Models (LLMs) is hampered by the
trade-off between efficiency and expressiveness. Low-Rank Adaptation (LoRA)
offers efficiency but constrains the model's ability to learn new tasks and
transfer knowledge due to its low-rank nature and reliance on explicit
parameter constraints. We propose GORP (Gradient LOw Rank Projection) for
Continual Learning, a novel training strategy that overcomes these limitations
by synergistically combining full and low-rank parameters and jointly updating
within a unified low-rank gradient subspace. GORP expands the optimization
space while preserving efficiency and mitigating catastrophic forgetting.
Extensive experiments on continual learning benchmarks demonstrate GORP's
superior performance compared to existing state-of-the-art approaches. Code is
available at https://github.com/Wcxwcxw/GORP.

</details>


### [99] [OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device Speculative Decoding](https://arxiv.org/abs/2507.02659)
*Ramchalam Kinattinkara Ramakrishnan,Zhaocong Yuan,Shaojie Zhuo,Chen Feng,Yicheng Lin,Chenzheng Su,Xiaopeng Zhang*

Main category: cs.LG

TL;DR: OmniDraft是一个统一的框架，使单个草稿模型能够与任何目标模型配合使用，并通过在线学习和自适应技术提高解码速度。


<details>
  <summary>Details</summary>
Motivation: 解决在线部署中草稿模型与目标模型不兼容以及延迟改进的需求。

Method: 引入在线n-gram缓存和混合蒸馏微调，结合自适应草拟技术。

Result: OmniDraft使单个Llama-68M模型能与多种目标模型配合，提供1.5-2倍加速。

Conclusion: OmniDraft适用于设备端LLM应用，支持“一个草稿模型适配所有”的范式。

Abstract: Speculative decoding generally dictates having a small, efficient draft model
that is either pretrained or distilled offline to a particular target model
series, for instance, Llama or Qwen models. However, within online deployment
settings, there are two major challenges: 1) usage of a target model that is
incompatible with the draft model; 2) expectation of latency improvements over
usage and time. In this work, we propose OmniDraft, a unified framework that
enables a single draft model to operate with any target model and adapt
dynamically to user data. We introduce an online n-gram cache with hybrid
distillation fine-tuning to address the cross-vocabulary mismatch across draft
and target models; and further improve decoding speed by leveraging adaptive
drafting techniques. OmniDraft is particularly suitable for on-device LLM
applications where model cost, efficiency and user customization are the major
points of contention. This further highlights the need to tackle the above
challenges and motivates the \textit{``one drafter for all''} paradigm. We
showcase the proficiency of the OmniDraft framework by performing online
learning on math reasoning, coding and text generation tasks. Notably,
OmniDraft enables a single Llama-68M model to pair with various target models
including Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;
and additionally provides up to 1.5-2x speedup.

</details>


### [100] [TFOC-Net: A Short-time Fourier Transform-based Deep Learning Approach for Enhancing Cross-Subject Motor Imagery Classification](https://arxiv.org/abs/2507.02510)
*Ahmed G. Habashi,Ahmed M. Azab,Seif Eldawlatly,Gamal M. Aly*

Main category: cs.LG

TL;DR: 论文提出了一种优化预处理和深度学习方法，显著提升了跨被试运动想象分类性能，并在多个数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 跨被试运动想象分类因脑电图模式差异大而准确率低，阻碍了无需校准的脑机接口的实际应用。

Method: 采用短时傅里叶变换预处理、优化参数和平衡批训练策略，结合卷积神经网络进行分类。

Result: 在多个数据集上表现优异，准确率分别为67.60%（IV-1）、65.96%（IV-2A）和80.22%（IV-2B）。

Conclusion: 该方法为无需校准的运动想象分类设定了新基准，并贡献了开源数据集。

Abstract: Cross-subject motor imagery (CS-MI) classification in brain-computer
interfaces (BCIs) is a challenging task due to the significant variability in
Electroencephalography (EEG) patterns across different individuals. This
variability often results in lower classification accuracy compared to
subject-specific models, presenting a major barrier to developing
calibration-free BCIs suitable for real-world applications. In this paper, we
introduce a novel approach that significantly enhances cross-subject MI
classification performance through optimized preprocessing and deep learning
techniques. Our approach involves direct classification of Short-Time Fourier
Transform (STFT)-transformed EEG data, optimized STFT parameters, and a
balanced batching strategy during training of a Convolutional Neural Network
(CNN). This approach is uniquely validated across four different datasets,
including three widely-used benchmark datasets leading to substantial
improvements in cross-subject classification, achieving 67.60% on the BCI
Competition IV Dataset 1 (IV-1), 65.96% on Dataset 2A (IV-2A), and 80.22% on
Dataset 2B (IV-2B), outperforming state-of-the-art techniques. Additionally, we
systematically investigate the classification performance using MI windows
ranging from the full 4-second window to 1-second windows. These results
establish a new benchmark for generalizable, calibration-free MI classification
in addition to contributing a robust open-access dataset to advance research in
this domain.

</details>


### [101] [RetrySQL: text-to-SQL training with retry data for self-correcting query generation](https://arxiv.org/abs/2507.02529)
*Alicja Rączkowska,Riccardo Belluzzo,Piotr Zieliński,Joanna Baran,Paweł Olszewski*

Main category: cs.LG

TL;DR: RetrySQL是一种新的文本到SQL生成模型训练方法，通过生成和纠正推理步骤提高准确性。


<details>
  <summary>Details</summary>
Motivation: 现有文本到SQL任务的研究缺乏针对SQL的生成模型，且自校正生成策略的应用尚未探索。

Method: 通过准备参考SQL查询的推理步骤并故意引入错误，生成包含错误和纠正步骤的数据，用于持续预训练开源编码模型。

Result: RetrySQL在整体和挑战性执行准确性指标上提升了4个百分点，且需要全参数预训练而非LoRA微调。

Conclusion: RetrySQL证明自校正能力可学习，并提供了提高SQL生成准确性的新方法。

Abstract: The text-to-SQL task is an active challenge in Natural Language Processing.
Many existing solutions focus on using black-box language models extended with
specialized components within customized end-to-end text-to-SQL pipelines.
While these solutions use both closed-source proprietary language models and
coding-oriented open-source models, there is a lack of research regarding
SQL-specific generative models. At the same time, recent advancements in
self-correcting generation strategies show promise for improving the
capabilities of existing architectures. The application of these concepts to
the text-to-SQL task remains unexplored. In this paper, we introduce RetrySQL,
a new approach to training text-to-SQL generation models. We prepare reasoning
steps for reference SQL queries and then corrupt them to create retry data that
contains both incorrect and corrected steps, divided with a special token. We
continuously pre-train an open-source coding model with this data and
demonstrate that retry steps yield an improvement of up to 4 percentage points
in both overall and challenging execution accuracy metrics, compared to
pre-training without retry data. Additionally, we confirm that supervised
fine-tuning with LoRA is ineffective for learning from retry data and that
full-parameter pre-training is a necessary requirement for that task. We
showcase that the self-correcting behavior is learned by the model and the
increase in downstream accuracy metrics is a result of this additional skill.
Finally, we incorporate RetrySQL-trained models into the full text-to-SQL
pipeline and showcase that they are competitive in terms of execution accuracy
with proprietary models that contain orders of magnitude more parameters.
RetrySQL demonstrates that self-correction can be learned in the text-to-SQL
task and provides a novel way of improving generation accuracy for SQL-oriented
language models.

</details>


### [102] [ExPO: Unlocking Hard Reasoning with Self-Explanation-Guided Reinforcement Learning](https://arxiv.org/abs/2507.02834)
*Ruiyang Zhou,Shuozhe Li,Amy Zhang,Liu Leqi*

Main category: cs.LG

TL;DR: 论文提出了一种名为ExPO的新方法，通过生成符合当前策略的高质量样本来提升语言模型的推理能力，优于基于专家演示的方法。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习后训练方法依赖模型初始能力生成正样本，限制了其在早期训练和复杂推理任务中的表现。

Method: 提出Self-Explanation Policy Optimization (ExPO)，通过基于真实答案生成高质量样本，实现高效探索。

Result: ExPO在推理任务（如MATH level-5）中显著提升了学习效率和最终性能。

Conclusion: ExPO为语言模型在复杂推理任务中的训练提供了更有效的解决方案。

Abstract: Recent advances in large language models have been driven by reinforcement
learning (RL)-style post-training, which improves reasoning by optimizing model
outputs based on reward or preference signals. GRPO-style approaches implement
this by using self-generated samples labeled by an outcome-based verifier.
However, these methods depend heavily on the model's initial ability to produce
positive samples. They primarily refine what the model already knows
(distribution sharpening) rather than enabling the model to solve problems
where it initially fails. This limitation is especially problematic in
early-stage RL training and on challenging reasoning tasks, where positive
samples are unlikely to be generated. To unlock reasoning ability in such
settings, the model must explore new reasoning trajectories beyond its current
output distribution. Such exploration requires access to sufficiently good
positive samples to guide the learning. While expert demonstrations seem like a
natural solution, we find that they are often ineffective in RL post-training.
Instead, we identify two key properties of effective positive samples: they
should (1) be likely under the current policy, and (2) increase the model's
likelihood of predicting the correct answer. Based on these insights, we
propose $\textbf{Self-Explanation Policy Optimization (ExPO)}$-a simple and
modular framework that generates such samples by conditioning on the
ground-truth answer. ExPO enables efficient exploration and guides the model to
produce reasoning trajectories more aligned with its policy than expert-written
CoTs, while ensuring higher quality than its own (incorrect) samples.
Experiments show that ExPO improves both learning efficiency and final
performance on reasoning benchmarks, surpassing expert-demonstration-based
methods in challenging settings such as MATH level-5, where the model initially
struggles the most.

</details>


### [103] [Position: A Theory of Deep Learning Must Include Compositional Sparsity](https://arxiv.org/abs/2507.02550)
*David A. Danhofer,Davide D'Ascenzo,Rafael Dubach,Tomaso Poggio*

Main category: cs.LG

TL;DR: 深度神经网络（DNNs）通过利用目标函数的组合稀疏结构取得成功，这种结构使得复杂函数可由低维子函数组合而成。


<details>
  <summary>Details</summary>
Motivation: 探讨DNNs成功背后的基本原理，尤其是其如何利用组合稀疏性解决高维问题。

Method: 提出组合稀疏性是DNNs成功的关键，并分析其在可计算函数中的普遍性。

Result: 组合稀疏性存在于所有高效可计算函数中，为DNNs的理论基础提供了支持。

Conclusion: 深入理解组合稀疏性对完善深度学习理论及人工智能发展至关重要。

Abstract: Overparametrized Deep Neural Networks (DNNs) have demonstrated remarkable
success in a wide variety of domains too high-dimensional for classical shallow
networks subject to the curse of dimensionality. However, open questions about
fundamental principles, that govern the learning dynamics of DNNs, remain. In
this position paper we argue that it is the ability of DNNs to exploit the
compositionally sparse structure of the target function driving their success.
As such, DNNs can leverage the property that most practically relevant
functions can be composed from a small set of constituent functions, each of
which relies only on a low-dimensional subset of all inputs. We show that this
property is shared by all efficiently Turing-computable functions and is
therefore highly likely present in all current learning problems. While some
promising theoretical insights on questions concerned with approximation and
generalization exist in the setting of compositionally sparse functions,
several important questions on the learnability and optimization of DNNs
remain. Completing the picture of the role of compositional sparsity in deep
learning is essential to a comprehensive theory of artificial, and even
general, intelligence.

</details>


### [104] [Transformers Don't Need LayerNorm at Inference Time: Scaling LayerNorm Removal to GPT-2 XL and the Implications for Mechanistic Interpretability](https://arxiv.org/abs/2507.02559)
*Luca Baroni,Galvin Khara,Joachim Schaeffer,Marat Subkhankulov,Stefan Heimersheim*

Main category: cs.LG

TL;DR: 研究发现层归一化（LN）在GPT-2模型中可以被移除，且对验证损失影响很小，表明LN在语言建模中作用有限。移除LN后，模型的可解释性有所提升。


<details>
  <summary>Details</summary>
Motivation: 探究LN在推理阶段的作用及其对模型可解释性的影响。

Method: 移除GPT-2模型中的所有LN层，并通过微调恢复性能。

Result: 移除LN后验证损失仅小幅增加（如GPT-2 XL增加0.03交叉熵损失），且微调数据需求随模型参数增长呈次线性关系。

Conclusion: LN在语言建模中并非关键，移除LN可提升模型可解释性，为未来研究提供新方向。

Abstract: Layer-wise normalization (LN) is an essential component of virtually all
transformer-based large language models. While its effects on training
stability are well documented, its role at inference time is poorly understood.
Additionally, LN layers hinder mechanistic interpretability by introducing
additional nonlinearities and increasing the interconnectedness of individual
model components. Here, we show that all LN layers can be removed from every
GPT-2 model with only a small increase in validation loss (e.g. +0.03
cross-entropy loss for GPT-2 XL). Thus, LN cannot play a substantial role in
language modeling. We find that the amount of fine-tuning data needed for LN
removal grows sublinearly with model parameters, suggesting scaling to larger
models is feasible. We release a suite of LN-free GPT-2 models on Hugging Face.
Furthermore, we test interpretability techniques on LN-free models. Direct
logit attribution now gives the exact direct effect of individual components,
while the accuracy of attribution patching does not significantly improve. We
also confirm that GPT-2's "confidence neurons" are inactive in the LN-free
models. Our work clarifies the role of LN layers in language modeling, showing
that GPT-2-class models can function without LN layers. We hope that our
LN-free analogs of the GPT-2 family of models will enable more precise
interpretability research and improve our understanding of language models.

</details>


### [105] [Scalable Interconnect Learning in Boolean Networks](https://arxiv.org/abs/2507.02585)
*Fabian Kresse,Emily Yu,Christoph H. Lampert*

Main category: cs.LG

TL;DR: 论文提出了一种可扩展的DBNs方法，通过可训练微分互连和两阶段剪枝技术，提升了模型效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决DBNs在输入宽度增加时参数爆炸的问题，同时优化模型大小和性能。

Method: 引入可训练微分互连，并采用两阶段剪枝（基于SAT的逻辑等价剪枝和基于相似性的数据驱动剪枝）。

Result: DBNs可扩展到更宽的层，同时保持高准确性，剪枝技术显著减小模型大小。

Conclusion: 该方法在资源受限硬件上实现了高效推理，并提供了优越的压缩-准确性权衡。

Abstract: Learned Differentiable Boolean Logic Networks (DBNs) already deliver
efficient inference on resource-constrained hardware. We extend them with a
trainable, differentiable interconnect whose parameter count remains constant
as input width grows, allowing DBNs to scale to far wider layers than earlier
learnable-interconnect designs while preserving their advantageous accuracy. To
further reduce model size, we propose two complementary pruning stages: an
SAT-based logic equivalence pass that removes redundant gates without affecting
performance, and a similarity-based, data-driven pass that outperforms a
magnitude-style greedy baseline and offers a superior compression-accuracy
trade-off.

</details>


### [106] [Padé Approximant Neural Networks for Enhanced Electric Motor Fault Diagnosis Using Vibration and Acoustic Data](https://arxiv.org/abs/2507.02599)
*Sertac Kilickaya,Levent Eren*

Main category: cs.LG

TL;DR: 研究旨在通过Padé Approximant Neuron (PAON)模型提升感应电机故障诊断性能，PadéNets在振动和声学数据诊断中优于传统CNN和Self-ONNs。


<details>
  <summary>Details</summary>
Motivation: 传统感应电机故障诊断依赖加速度计和麦克风，但非线性神经元架构的深度学习模型有望提升诊断性能，研究探讨PadéNets是否优于CNN和Self-ONNs。

Method: 比较一维CNN、Self-ONNs和PadéNets在振动和声学数据上的诊断能力，PadéNets设计增强非线性并兼容无界激活函数。

Result: PadéNets诊断准确率显著高于基线模型，加速度计和声学传感器分别达到99.96%和98.33%。

Conclusion: PadéNets的非线性增强和无界激活函数兼容性显著提升了感应电机故障诊断性能。

Abstract: Purpose: The primary aim of this study is to enhance fault diagnosis in
induction machines by leveraging the Pad\'e Approximant Neuron (PAON) model.
While accelerometers and microphones are standard in motor condition
monitoring, deep learning models with nonlinear neuron architectures offer
promising improvements in diagnostic performance. This research addresses the
question: Can Pad\'e Approximant Neural Networks (Pad\'eNets) outperform
conventional Convolutional Neural Networks (CNNs) and Self-Organized
Operational Neural Networks (Self-ONNs) in diagnosing electrical and mechanical
faults using vibration and acoustic data?
  Methods: We evaluate and compare the diagnostic capabilities of three deep
learning architectures: one-dimensional CNNs, Self-ONNs, and Pad\'eNets. These
models are tested on the University of Ottawa's publicly available
constant-speed induction motor datasets, which include both vibration and
acoustic sensor data. The Pad\'eNet model is designed to introduce enhanced
nonlinearity and is compatible with unbounded activation functions such as
Leaky ReLU.
  Results and Conclusion: Pad\'eNets consistently outperformed the baseline
models, achieving diagnostic accuracies of 99.96%, 98.26%, 97.61%, and 98.33%
for accelerometers 1, 2, 3, and the acoustic sensor, respectively. The enhanced
nonlinearity of Pad\'eNets, together with their compatibility with unbounded
activation functions, significantly improves fault diagnosis performance in
induction motor condition monitoring.

</details>


### [107] [Lost in Latent Space: An Empirical Study of Latent Diffusion Models for Physics Emulation](https://arxiv.org/abs/2507.02608)
*François Rozet,Ruben Ohana,Michael McCabe,Gilles Louppe,François Lanusse,Shirley Ho*

Main category: cs.LG

TL;DR: 扩散模型在推理时的高计算成本阻碍了其作为快速物理模拟器的应用。本文研究了在潜在空间而非像素空间生成的方法是否适用于动力学系统模拟，并探讨了其代价。


<details>
  <summary>Details</summary>
Motivation: 解决扩散模型在推理时的高计算成本问题，探索其在动力学系统模拟中的潜在空间生成方法的有效性。

Method: 在潜在空间而非像素空间生成，研究不同压缩率下的模拟准确性，并与非生成式方法对比。

Result: 潜在空间模拟的准确性对高达1000倍的压缩率表现出惊人的鲁棒性，扩散模型比非生成式方法更准确且预测多样性更强。

Conclusion: 潜在空间模拟在动力学系统中具有可行性，扩散模型在此任务中表现优越，同时提供了关键的设计选择建议。

Abstract: The steep computational cost of diffusion models at inference hinders their
use as fast physics emulators. In the context of image and video generation,
this computational drawback has been addressed by generating in the latent
space of an autoencoder instead of the pixel space. In this work, we
investigate whether a similar strategy can be effectively applied to the
emulation of dynamical systems and at what cost. We find that the accuracy of
latent-space emulation is surprisingly robust to a wide range of compression
rates (up to 1000x). We also show that diffusion-based emulators are
consistently more accurate than non-generative counterparts and compensate for
uncertainty in their predictions with greater diversity. Finally, we cover
practical design choices, spanning from architectures to optimizers, that we
found critical to train latent-space emulators.

</details>


### [108] [L-VAE: Variational Auto-Encoder with Learnable Beta for Disentangled Representation](https://arxiv.org/abs/2507.02619)
*Hazal Mogultay Ozcan,Sinan Kalkan,Fatos T. Yarman-Vural*

Main category: cs.LG

TL;DR: L-VAE是一种新型模型，通过学习损失函数的超参数和表示解耦，改进了β-VAE的局限性，实现了重建和解耦的动态平衡。


<details>
  <summary>Details</summary>
Motivation: 解决β-VAE中超参数η需手动调整的问题，实现更灵活的重建和解耦权衡。

Method: L-VAE同时学习损失项权重和模型参数，并加入正则化项防止偏向。

Result: 在多个数据集上表现优于或接近现有方法，成功解耦面部属性。

Conclusion: L-VAE在重建和解耦之间找到了有效平衡，性能优越。

Abstract: In this paper, we propose a novel model called Learnable VAE (L-VAE), which
learns a disentangled representation together with the hyperparameters of the
cost function. L-VAE can be considered as an extension of \b{eta}-VAE, wherein
the hyperparameter, \b{eta}, is empirically adjusted. L-VAE mitigates the
limitations of \b{eta}-VAE by learning the relative weights of the terms in the
loss function to control the dynamic trade-off between disentanglement and
reconstruction losses. In the proposed model, the weight of the loss terms and
the parameters of the model architecture are learned concurrently. An
additional regularization term is added to the loss function to prevent bias
towards either reconstruction or disentanglement losses. Experimental analyses
show that the proposed L-VAE finds an effective balance between reconstruction
fidelity and disentangling the latent dimensions. Comparisons of the proposed
L-VAE against \b{eta}-VAE, VAE, ControlVAE, DynamicVAE, and {\sigma}-VAE on
datasets, such as dSprites, MPI3D-complex, Falcor3D, and Isaac3D reveals that
L-VAE consistently provides the best or the second best performances measured
by a set of disentanglement metrics. Moreover, qualitative experiments on
CelebA dataset, confirm the success of the L-VAE model for disentangling the
facial attributes.

</details>


### [109] [A Matrix Variational Auto-Encoder for Variant Effect Prediction in Pharmacogenes](https://arxiv.org/abs/2507.02624)
*Antoine Honoré,Borja Rodríguez Gálvez,Yoomi Park,Yitian Zhou,Volker M. Lauschke,Ming Xiao*

Main category: cs.LG

TL;DR: 提出了一种基于Transformer的矩阵变分自编码器（matVAE），用于评估蛋白质变体的功能影响，并在DMS数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖多序列比对（MSA），但MSA假设自然变体是适应的，这在某些药物基因中不成立。DMS数据集提供了更直接的适应性评分。

Method: 使用matVAE结合结构化先验，并在33个DMS数据集上评估性能，同时比较了基于MSA和DMS训练的模型。

Result: matVAE-MSA在零样本预测中优于DeepSequence，且参数更少。结合AlphaFold结构后性能进一步提升。

Conclusion: DMS数据集有望替代MSA，为变体效应预测提供新方向。

Abstract: Variant effect predictors (VEPs) aim to assess the functional impact of
protein variants, traditionally relying on multiple sequence alignments (MSAs).
This approach assumes that naturally occurring variants are fit, an assumption
challenged by pharmacogenomics, where some pharmacogenes experience low
evolutionary pressure. Deep mutational scanning (DMS) datasets provide an
alternative by offering quantitative fitness scores for variants. In this work,
we propose a transformer-based matrix variational auto-encoder (matVAE) with a
structured prior and evaluate its performance on 33 DMS datasets corresponding
to 26 drug target and ADME proteins from the ProteinGym benchmark. Our model
trained on MSAs (matVAE-MSA) outperforms the state-of-the-art DeepSequence
model in zero-shot prediction on DMS datasets, despite using an order of
magnitude fewer parameters and requiring less computation at inference time. We
also compare matVAE-MSA to matENC-DMS, a model of similar capacity trained on
DMS data, and find that the latter performs better on supervised prediction
tasks. Additionally, incorporating AlphaFold-generated structures into our
transformer model further improves performance, achieving results comparable to
DeepSequence trained on MSAs and finetuned on DMS. These findings highlight the
potential of DMS datasets to replace MSAs without significant loss in
predictive performance, motivating further development of DMS datasets and
exploration of their relationships to enhance variant effect prediction.

</details>


### [110] [Medical Data Pecking: A Context-Aware Approach for Automated Quality Evaluation of Structured Medical Data](https://arxiv.org/abs/2507.02628)
*Irena Girshovitz,Atai Ambus,Moni Shahar,Ran Gilad-Bachrach*

Main category: cs.LG

TL;DR: 论文提出了一种基于软件工程概念的医学数据质量评估方法（Medical Data Pecking），通过自动化测试工具（MDPT）识别数据质量问题，并在三个数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录（EHR）数据质量对流行病学研究和AI训练至关重要，但现有方法不足以系统性评估数据质量。

Method: 采用软件工程中的单元测试和覆盖率概念，开发了MDPT工具，包括自动化测试生成器和数据测试框架。

Result: 在三个数据集上生成了55-73个测试，识别出20-43个数据问题，验证了方法的有效性。

Conclusion: 该方法通过结合外部医学知识，提升了数据质量测试的上下文敏感性，为未来改进奠定了基础。

Abstract: Background: The use of Electronic Health Records (EHRs) for epidemiological
studies and artificial intelligence (AI) training is increasing rapidly. The
reliability of the results depends on the accuracy and completeness of EHR
data. However, EHR data often contain significant quality issues, including
misrepresentations of subpopulations, biases, and systematic errors, as they
are primarily collected for clinical and billing purposes. Existing quality
assessment methods remain insufficient, lacking systematic procedures to assess
data fitness for research.
  Methods: We present the Medical Data Pecking approach, which adapts unit
testing and coverage concepts from software engineering to identify data
quality concerns. We demonstrate our approach using the Medical Data Pecking
Tool (MDPT), which consists of two main components: (1) an automated test
generator that uses large language models and grounding techniques to create a
test suite from data and study descriptions, and (2) a data testing framework
that executes these tests, reporting potential errors and coverage.
  Results: We evaluated MDPT on three datasets: All of Us (AoU), MIMIC-III, and
SyntheticMass, generating 55-73 tests per cohort across four conditions. These
tests correctly identified 20-43 non-aligned or non-conforming data issues. We
present a detailed analysis of the LLM-generated test suites in terms of
reference grounding and value accuracy.
  Conclusion: Our approach incorporates external medical knowledge to enable
context-sensitive data quality testing as part of the data analysis workflow to
improve the validity of its outcomes. Our approach tackles these challenges
from a quality assurance perspective, laying the foundation for further
development such as additional data modalities and improved grounding methods.

</details>


### [111] [High-Order Deep Meta-Learning with Category-Theoretic Interpretation](https://arxiv.org/abs/2507.02634)
*David H. Mguni*

Main category: cs.LG

TL;DR: 提出了一种分层深度学习框架，用于递归高阶元学习，使神经网络能构建、解决和泛化任务层次结构。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习依赖人类生成数据的局限性，通过生成虚拟任务和软约束，提升泛化能力。

Method: 采用生成机制创建虚拟任务，通过探索任务难度迭代优化约束区域，并利用范畴论中的函子概念构建层次化学习结构。

Result: 框架能自主生成任务和解决方案，增强归纳偏置和泛化能力，支持跨任务知识迁移。

Conclusion: 该架构有望推动机器学习向通用人工智能发展，通过自主生成任务和解决方案提升泛化能力。

Abstract: We introduce a new hierarchical deep learning framework for recursive
higher-order meta-learning that enables neural networks (NNs) to construct,
solve, and generalise across hierarchies of tasks. Central to this approach is
a generative mechanism that creates \emph{virtual tasks} -- synthetic problem
instances designed to enable the meta-learner to learn \emph{soft constraints}
and unknown generalisable rules across related tasks. Crucially, this enables
the framework to generate its own informative, task-grounded datasets thereby
freeing machine learning (ML) training from the limitations of relying entirely
on human-generated data. By actively exploring the virtual point landscape and
seeking out tasks lower-level learners find difficult, the meta-learner
iteratively refines constraint regions. This enhances inductive biases,
regularises the adaptation process, and produces novel, unanticipated tasks and
constraints required for generalisation. Each meta-level of the hierarchy
corresponds to a progressively abstracted generalisation of problems solved at
lower levels, enabling a structured and interpretable learning progression. By
interpreting meta-learners as category-theoretic \emph{functors} that generate
and condition a hierarchy of subordinate learners, we establish a compositional
structure that supports abstraction and knowledge transfer across progressively
generalised tasks. The category-theoretic perspective unifies existing
meta-learning models and reveals how learning processes can be transformed and
compared through functorial relationships, while offering practical design
principles for structuring meta-learning. We speculate this architecture may
underpin the next generation of NNs capable of autonomously generating novel,
instructive tasks and their solutions, thereby advancing ML towards general
artificial intelligence.

</details>


### [112] [On Efficient Bayesian Exploration in Model-Based Reinforcement Learning](https://arxiv.org/abs/2507.02639)
*Alberto Caron,Chris Hicks,Vasilios Mavroudis*

Main category: cs.LG

TL;DR: 该论文提出了一种基于信息论的内在动机方法，通过针对认知不确定性设计探索奖励，解决了强化学习中数据高效探索的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习中数据高效探索的问题，特别是针对认知不确定性而非环境固有噪声的探索奖励。

Method: 提出了一种基于信息论的探索奖励方法，并通过稀疏变分高斯过程、深度核和深度集成模型实现可操作性。进一步提出了PTS-BE框架，结合模型规划与信息论奖励。

Result: PTS-BE在稀疏奖励和纯探索任务中显著优于其他基线方法。

Conclusion: 该方法为信息增益探索提供了理论保证，并通过PTS-BE框架实现了样本高效深度探索。

Abstract: In this work, we address the challenge of data-efficient exploration in
reinforcement learning by examining existing principled, information-theoretic
approaches to intrinsic motivation. Specifically, we focus on a class of
exploration bonuses that targets epistemic uncertainty rather than the
aleatoric noise inherent in the environment. We prove that these bonuses
naturally signal epistemic information gains and converge to zero once the
agent becomes sufficiently certain about the environment's dynamics and
rewards, thereby aligning exploration with genuine knowledge gaps. Our analysis
provides formal guarantees for IG-based approaches, which previously lacked
theoretical grounding. To enable practical use, we also discuss tractable
approximations via sparse variational Gaussian Processes, Deep Kernels and Deep
Ensemble models. We then outline a general framework - Predictive Trajectory
Sampling with Bayesian Exploration (PTS-BE) - which integrates model-based
planning with information-theoretic bonuses to achieve sample-efficient deep
exploration. We empirically demonstrate that PTS-BE substantially outperforms
other baselines across a variety of environments characterized by sparse
rewards and/or purely exploratory tasks.

</details>


### [113] [Fair Deepfake Detectors Can Generalize](https://arxiv.org/abs/2507.02645)
*Harry Cheng,Ming-Hui Liu,Yangyang Guo,Tianyi Wang,Liqiang Nie,Mohan Kankanhalli*

Main category: cs.LG

TL;DR: 论文提出DAID框架，通过因果分析解决深度伪造检测中公平性与泛化性的冲突，并在实验中验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 深度伪造检测模型面临泛化性和公平性的冲突，现有方法难以兼顾二者。本文首次揭示并定义了公平性与泛化性之间的因果关系。

Method: 提出DAID框架，包括人口统计属性无关的数据重平衡和特征聚合方法，通过控制混杂变量优化公平性与泛化性。

Result: 在三个跨域基准测试中，DAID在公平性和泛化性上均优于现有方法。

Conclusion: DAID通过因果干预成功解决了公平性与泛化性的冲突，为深度伪造检测提供了新思路。

Abstract: Deepfake detection models face two critical challenges: generalization to
unseen manipulations and demographic fairness among population groups. However,
existing approaches often demonstrate that these two objectives are inherently
conflicting, revealing a trade-off between them. In this paper, we, for the
first time, uncover and formally define a causal relationship between fairness
and generalization. Building on the back-door adjustment, we show that
controlling for confounders (data distribution and model capacity) enables
improved generalization via fairness interventions. Motivated by this insight,
we propose Demographic Attribute-insensitive Intervention Detection (DAID), a
plug-and-play framework composed of: i) Demographic-aware data rebalancing,
which employs inverse-propensity weighting and subgroup-wise feature
normalization to neutralize distributional biases; and ii) Demographic-agnostic
feature aggregation, which uses a novel alignment loss to suppress
sensitive-attribute signals. Across three cross-domain benchmarks, DAID
consistently achieves superior performance in both fairness and generalization
compared to several state-of-the-art detectors, validating both its theoretical
foundation and practical effectiveness.

</details>


### [114] [Guided Generation for Developable Antibodies](https://arxiv.org/abs/2507.02670)
*Siqi Zhao,Joshua Moller,Porfi Quintero-Cadena,Lood van Niekerk*

Main category: cs.LG

TL;DR: 论文提出了一种基于离散扩散模型的抗体序列优化框架，结合SVDD模块提升开发性评分。


<details>
  <summary>Details</summary>
Motivation: 优化抗体序列以实现高开发性（包括可制造性、稳定性和安全性）是临床有效性的关键。

Method: 使用自然抗体序列和临床抗体数据训练离散扩散模型，并引入SVDD模块引导生成。

Result: 模型能重现自然抗体特征，SVDD显著提升开发性评分预测。

Conclusion: 该框架结合高通量实验，可实现结合与生物物理标准的抗体设计。

Abstract: Therapeutic antibodies require not only high-affinity target engagement, but
also favorable manufacturability, stability, and safety profiles for clinical
effectiveness. These properties are collectively called `developability'. To
enable a computational framework for optimizing antibody sequences for
favorable developability, we introduce a guided discrete diffusion model
trained on natural paired heavy- and light-chain sequences from the Observed
Antibody Space (OAS) and quantitative developability measurements for 246
clinical-stage antibodies. To steer generation toward biophysically viable
candidates, we integrate a Soft Value-based Decoding in Diffusion (SVDD) Module
that biases sampling without compromising naturalness. In unconstrained
sampling, our model reproduces global features of both the natural repertoire
and approved therapeutics, and under SVDD guidance we achieve significant
enrichment in predicted developability scores over unguided baselines. When
combined with high-throughput developability assays, this framework enables an
iterative, ML-driven pipeline for designing antibodies that satisfy binding and
biophysical criteria in tandem.

</details>


### [115] [Embedding-Based Federated Data Sharing via Differentially Private Conditional VAEs](https://arxiv.org/abs/2507.02671)
*Francesco Di Salvo,Hanh Huyen My Nguyen,Christian Ledig*

Main category: cs.LG

TL;DR: 提出了一种基于差分隐私生成模型的数据共享方法，通过联合训练DP-CVAE提升隐私保护和效率，优于传统联邦学习。


<details>
  <summary>Details</summary>
Motivation: 深度学习在医学影像中的应用受限于数据稀缺和隐私法规，联邦学习存在通信成本高和任务单一的问题。

Method: 采用差分隐私条件变分自编码器（DP-CVAE）建模全局隐私感知数据分布，支持多样化下游任务。

Result: 方法在隐私、可扩展性和效率上表现优异，生成比DP-CGAN更高保真度的嵌入且参数更少。

Conclusion: DP-CVAE为医学影像中的隐私保护和高效数据共享提供了可行方案。

Abstract: Deep Learning (DL) has revolutionized medical imaging, yet its adoption is
constrained by data scarcity and privacy regulations, limiting access to
diverse datasets. Federated Learning (FL) enables decentralized training but
suffers from high communication costs and is often restricted to a single
downstream task, reducing flexibility. We propose a data-sharing method via
Differentially Private (DP) generative models. By adopting foundation models,
we extract compact, informative embeddings, reducing redundancy and lowering
computational overhead. Clients collaboratively train a Differentially Private
Conditional Variational Autoencoder (DP-CVAE) to model a global, privacy-aware
data distribution, supporting diverse downstream tasks. Our approach, validated
across multiple feature extractors, enhances privacy, scalability, and
efficiency, outperforming traditional FL classifiers while ensuring
differential privacy. Additionally, DP-CVAE produces higher-fidelity embeddings
than DP-CGAN while requiring $5{\times}$ fewer parameters.

</details>


### [116] [Multi-Agent Reinforcement Learning for Dynamic Pricing in Supply Chains: Benchmarking Strategic Agent Behaviours under Realistically Simulated Market Conditions](https://arxiv.org/abs/2507.02698)
*Thomas Hazenberg,Yao Ma,Seyed Sahand Mohammadi Ziabari,Marijn van Rijswijk*

Main category: cs.LG

TL;DR: 研究探讨了多智能体强化学习（MARL）如何改进供应链中的动态定价策略，对比了三种MARL算法与静态规则基线的表现。


<details>
  <summary>Details</summary>
Motivation: 传统ERP系统依赖静态规则定价，忽略了市场参与者间的战略互动，而现有强化学习研究多为单智能体，无法模拟真实供应链的相互依赖性。

Method: 在基于真实电商交易数据和LightGBM需求预测模型的模拟环境中，评估了MADDPG、MADQN和QMIX三种MARL算法的性能。

Result: 规则基智能体公平性高（Jain指数0.9896）且价格稳定（波动0.024），但缺乏竞争动态；MADQN定价最激进（公平性0.5844），MADDPG平衡了市场竞争和公平性（公平性0.8819）。

Conclusion: MARL能捕捉静态规则无法模拟的战略行为，为动态定价的未来发展提供了新思路。

Abstract: This study investigates how Multi-Agent Reinforcement Learning (MARL) can
improve dynamic pricing strategies in supply chains, particularly in contexts
where traditional ERP systems rely on static, rule-based approaches that
overlook strategic interactions among market actors. While recent research has
applied reinforcement learning to pricing, most implementations remain
single-agent and fail to model the interdependent nature of real-world supply
chains. This study addresses that gap by evaluating the performance of three
MARL algorithms: MADDPG, MADQN, and QMIX against static rule-based baselines,
within a simulated environment informed by real e-commerce transaction data and
a LightGBM demand prediction model. Results show that rule-based agents achieve
near-perfect fairness (Jain's Index: 0.9896) and the highest price stability
(volatility: 0.024), but they fully lack competitive dynamics. Among MARL
agents, MADQN exhibits the most aggressive pricing behaviour, with the highest
volatility and the lowest fairness (0.5844). MADDPG provides a more balanced
approach, supporting market competition (share volatility: 9.5 pp) while
maintaining relatively high fairness (0.8819) and stable pricing. These
findings suggest that MARL introduces emergent strategic behaviour not captured
by static pricing rules and may inform future developments in dynamic pricing.

</details>


### [117] [Fluid Democracy in Federated Data Aggregation](https://arxiv.org/abs/2507.02710)
*Aditya Vema Reddy Kesari,Krishna Reddy Kesari*

Main category: cs.LG

TL;DR: 提出了一种基于共识的联邦学习协议（FedVRD），通过动态限制对手影响并优化数据传输成本。


<details>
  <summary>Details</summary>
Motivation: 减少联邦学习中不必要的数据传输成本，同时防止对手对全局模型的负面影响。

Method: 提出新的流体民主协议（viscous-retained democracy）和算法FedVRD，动态限制对手影响。

Result: FedVRD在相同假设下优于传统1p1v协议，且能有效减少对手影响。

Conclusion: FedVRD是一种高效且安全的联邦学习协议，适用于实际应用。

Abstract: Federated learning (FL) mechanisms typically require each client to transfer
their weights to a central server, irrespective of how useful they are. In
order to avoid wasteful data transfer costs from clients to the central server,
we propose the use of consensus based protocols to identify a subset of clients
with most useful model weights at each data transfer step. First, we explore
the application of existing fluid democracy protocols to FL from a performance
standpoint, comparing them with traditional one-person-one-vote (also known as
1p1v or FedAvg). We propose a new fluid democracy protocol named
viscous-retained democracy that always does better than 1p1v under the same
assumptions as existing fluid democracy protocols while also not allowing for
influence accumulation. Secondly, we identify weaknesses of fluid democracy
protocols from an adversarial lens in terms of their dependence on topology
and/ or number of adversaries required to negatively impact the global model
weights. To this effect, we propose an algorithm (FedVRD) that dynamically
limits the effect of adversaries while minimizing cost by leveraging the
delegation topology.

</details>


### [118] [A Forget-and-Grow Strategy for Deep Reinforcement Learning Scaling in Continuous Control](https://arxiv.org/abs/2507.02712)
*Zilin Kang,Chenyuan Hu,Yu Luo,Zhecheng Yuan,Ruijie Zheng,Huazhe Xu*

Main category: cs.LG

TL;DR: FoG算法通过模拟人类遗忘与成长机制，提出ER Decay和Network Expansion，显著提升深度强化学习的样本效率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有深度强化学习方法存在primacy bias问题，影响样本效率和泛化能力，而人类通过遗忘和成长机制避免此问题。

Method: FoG算法包含两个机制：ER Decay（逐步减少早期经验影响）和Network Expansion（动态增加网络参数）。

Result: 在40多个任务上的实验表明，FoG优于现有SoTA算法（如BRO、SimBa、TD-MPC2）。

Conclusion: FoG通过模拟人类遗忘与成长机制，有效解决了primacy bias问题，提升了深度强化学习的性能。

Abstract: Deep reinforcement learning for continuous control has recently achieved
impressive progress. However, existing methods often suffer from primacy bias,
a tendency to overfit early experiences stored in the replay buffer, which
limits an RL agent's sample efficiency and generalizability. In contrast,
humans are less susceptible to such bias, partly due to infantile amnesia,
where the formation of new neurons disrupts early memory traces, leading to the
forgetting of initial experiences. Inspired by this dual processes of
forgetting and growing in neuroscience, in this paper, we propose Forget and
Grow (FoG), a new deep RL algorithm with two mechanisms introduced. First,
Experience Replay Decay (ER Decay) "forgetting early experience", which
balances memory by gradually reducing the influence of early experiences.
Second, Network Expansion, "growing neural capacity", which enhances agents'
capability to exploit the patterns of existing data by dynamically adding new
parameters during training. Empirical results on four major continuous control
benchmarks with more than 40 tasks demonstrate the superior performance of FoG
against SoTA existing deep RL algorithms, including BRO, SimBa, and TD-MPC2.

</details>


### [119] [A Comprehensive Machine Learning Framework for Micromobility Demand Prediction](https://arxiv.org/abs/2507.02715)
*Omri Porat,Michael Fire,Eran Ben-Elia*

Main category: cs.LG

TL;DR: 该论文提出了一种整合空间、时间和网络依赖性的框架，以改进无桩电动滑板车的需求预测，准确率提高了27%至49%。


<details>
  <summary>Details</summary>
Motivation: 无桩电动滑板车作为一种环保灵活的交通工具，需求预测对其管理和规划至关重要。现有研究多孤立分析空间或时间因素，缺乏综合视角。

Method: 提出了一种整合空间、时间和网络依赖性的框架，用于改进需求预测。

Result: 框架的需求预测准确率比基线模型提高了27%至49%。

Conclusion: 该框架为数据驱动的微移动管理提供了支持，有助于优化车队分布、降低成本并促进可持续城市规划。

Abstract: Dockless e-scooters, a key micromobility service, have emerged as
eco-friendly and flexible urban transport alternatives. These services improve
first and last-mile connectivity, reduce congestion and emissions, and
complement public transport for short-distance travel. However, effective
management of these services depends on accurate demand prediction, which is
crucial for optimal fleet distribution and infrastructure planning. While
previous studies have focused on analyzing spatial or temporal factors in
isolation, this study introduces a framework that integrates spatial, temporal,
and network dependencies for improved micromobility demand forecasting. This
integration enhances accuracy while providing deeper insights into urban
micromobility usage patterns. Our framework improves demand prediction accuracy
by 27 to 49% over baseline models, demonstrating its effectiveness in capturing
micromobility demand patterns. These findings support data-driven micromobility
management, enabling optimized fleet distribution, cost reduction, and
sustainable urban planning.

</details>


### [120] [Hierarchical Multi-Label Contrastive Learning for Protein-Protein Interaction Prediction Across Organisms](https://arxiv.org/abs/2507.02724)
*Shiyi Liu,Buwen Liang,Yuetong Fang,Zixuan Jiang,Renjing Xu*

Main category: cs.LG

TL;DR: HIPPO是一个基于层次对比学习的蛋白质-蛋白质相互作用预测框架，通过多层次的生物表示匹配和对比损失函数，实现了跨物种的高性能预测，尤其在数据稀疏情况下表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决跨物种蛋白质-蛋白质相互作用预测中数据异质性和稀疏性的挑战，同时提升模型的零样本迁移能力。

Method: 提出层次对比学习框架HIPPO，结合蛋白质序列和层次属性，通过多层次的生物表示匹配和数据驱动的惩罚机制优化嵌入空间。

Result: 在基准数据集上表现优异，优于现有方法，并在低数据量和零样本迁移任务中展现出鲁棒性。

Conclusion: HIPPO为跨物种蛋白质相互作用预测提供了统一框架，特别适用于数据稀疏或分布不均的场景。

Abstract: Recent advances in AI for science have highlighted the power of contrastive
learning in bridging heterogeneous biological data modalities. Building on this
paradigm, we propose HIPPO (HIerarchical Protein-Protein interaction prediction
across Organisms), a hierarchical contrastive framework for protein-protein
interaction(PPI) prediction, where protein sequences and their hierarchical
attributes are aligned through multi-tiered biological representation matching.
The proposed approach incorporates hierarchical contrastive loss functions that
emulate the structured relationship among functional classes of proteins. The
framework adaptively incorporates domain and family knowledge through a
data-driven penalty mechanism, enforcing consistency between the learned
embedding space and the intrinsic hierarchy of protein functions. Experiments
on benchmark datasets demonstrate that HIPPO achieves state-of-the-art
performance, outperforming existing methods and showing robustness in low-data
regimes. Notably, the model demonstrates strong zero-shot transferability to
other species without retraining, enabling reliable PPI prediction and
functional inference even in less characterized or rare organisms where
experimental data are limited. Further analysis reveals that hierarchical
feature fusion is critical for capturing conserved interaction determinants,
such as binding motifs and functional annotations. This work advances
cross-species PPI prediction and provides a unified framework for interaction
prediction in scenarios with sparse or imbalanced multi-species data.

</details>


### [121] [Classification by Separating Hypersurfaces: An Entropic Approach](https://arxiv.org/abs/2507.02732)
*Argimiro Arratia,Mahmoud El Daou,Henryk Gzyl*

Main category: cs.LG

TL;DR: 提出了一种基于熵最小化的新型分类方法，适用于线性和非线性可分数据。


<details>
  <summary>Details</summary>
Motivation: 解决传统分类方法（如支持向量机和梯度下降）在处理复杂决策边界时的局限性。

Method: 在N维超立方体中搜索参数向量，并通过最小化熵函数优化，扩展到多项式曲面。

Result: 数值实验表明该方法高效且适用于多种分类任务。

Conclusion: 该方法为分类问题提供了鲁棒的替代方案，适用于复杂决策边界。

Abstract: We consider the following classification problem: Given a population of
individuals characterized by a set of attributes represented as a vector in
${\mathbb R}^N$, the goal is to find a hyperplane in ${\mathbb R}^N$ that
separates two sets of points corresponding to two distinct classes. This
problem, with a history dating back to the perceptron model, remains central to
machine learning. In this paper we propose a novel approach by searching for a
vector of parameters in a bounded $N$-dimensional hypercube centered at the
origin and a positive vector in ${\mathbb R}^M$, obtained through the
minimization of an entropy-based function defined over the space of unknown
variables. The method extends to polynomial surfaces, allowing the separation
of data points by more complex decision boundaries. This provides a robust
alternative to traditional linear or quadratic optimization techniques, such as
support vector machines and gradient descent. Numerical experiments demonstrate
the efficiency and versatility of the method in handling diverse classification
tasks, including linear and non-linear separability.

</details>


### [122] [Fast and Simplex: 2-Simplicial Attention in Triton](https://arxiv.org/abs/2507.02754)
*Aurko Roy,Timothy Chou,Sai Surya Duvvuri,Sijia Chen,Jiecao Yu,Xiaodong Wang,Manzil Zaheer,Rohan Anil*

Main category: cs.LG

TL;DR: 论文探讨了2-simplicial Transformer在提高token效率方面的优势，相比标准Transformer在数学、编码、推理和逻辑任务中表现更优。


<details>
  <summary>Details</summary>
Motivation: 现代大语言模型依赖海量数据，但传统扩展定律假设数据无限且计算受限，这一假设逐渐失效，因此需要更高效的架构。

Method: 提出2-simplicial Transformer，通过三线性函数和高效Triton内核实现，替代标准点积注意力机制。

Result: 在固定token预算下，2-simplicial Transformer在多项任务中优于标准Transformer，并改变了知识和推理任务的扩展定律指数。

Conclusion: 2-simplicial Transformer显著提升了token效率，为未来高效模型设计提供了新方向。

Abstract: Recent work has shown that training loss scales as a power law with both
model size and the number of tokens, and that achieving compute-optimal models
requires scaling model size and token count together. However, these scaling
laws assume an infinite supply of data and apply primarily in compute-bound
settings. As modern large language models increasingly rely on massive
internet-scale datasets, the assumption that they are compute-bound is becoming
less valid. This shift highlights the need for architectures that prioritize
token efficiency.
  In this work, we investigate the use of the 2-simplicial Transformer, an
architecture that generalizes standard dot-product attention to trilinear
functions through an efficient Triton kernel implementation. We demonstrate
that the 2-simplicial Transformer achieves better token efficiency than
standard Transformers: for a fixed token budget, similarly sized models
outperform their dot-product counterparts on tasks involving mathematics,
coding, reasoning, and logic. We quantify these gains by demonstrating that
$2$-simplicial attention changes the exponent in the scaling laws for knowledge
and reasoning tasks compared to dot product attention.

</details>


### [123] [Contextual Online Pricing with (Biased) Offline Data](https://arxiv.org/abs/2507.02762)
*Yixuan Zhang,Ruihao Zhu,Qiaomin Xie*

Main category: cs.LG

TL;DR: 研究了带有偏置离线数据的上下文在线定价问题，提出了基于OFU策略的优化方法，并分析了统计复杂性和遗憾界。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用带有偏置的离线数据优化在线定价策略，以降低遗憾并提高性能。

Method: 采用Optimism-in-the-Face-of-Uncertainty (OFU)策略，结合离线数据的偏置界限、尺寸和分散度，设计了两种算法：一种针对已知偏置界限的情况，另一种针对未知偏置界限的鲁棒变体。

Result: 在标量价格弹性情况下，得到了实例依赖的遗憾界；在一般价格弹性情况下，得到了最坏情况下的最优遗憾界。

Conclusion: 首次在带有偏置离线数据的上下文定价中实现了紧密的遗憾保证，且方法可推广到带有偏置离线数据的随机线性bandits问题。

Abstract: We study contextual online pricing with biased offline data. For the scalar
price elasticity case, we identify the instance-dependent quantity $\delta^2$
that measures how far the offline data lies from the (unknown) online optimum.
We show that the time length $T$, bias bound $V$, size $N$ and dispersion
$\lambda_{\min}(\hat{\Sigma})$ of the offline data, and $\delta^2$ jointly
determine the statistical complexity. An Optimism-in-the-Face-of-Uncertainty
(OFU) policy achieves a minimax-optimal, instance-dependent regret bound
$\tilde{\mathcal{O}}\big(d\sqrt{T} \wedge (V^2T +
\frac{dT}{\lambda_{\min}(\hat{\Sigma}) + (N \wedge T) \delta^2})\big)$. For
general price elasticity, we establish a worst-case, minimax-optimal rate
$\tilde{\mathcal{O}}\big(d\sqrt{T} \wedge (V^2T + \frac{dT
}{\lambda_{\min}(\hat{\Sigma})})\big)$ and provide a generalized OFU algorithm
that attains it. When the bias bound $V$ is unknown, we design a robust variant
that always guarantees sub-linear regret and strictly improves on purely online
methods whenever the exact bias is small. These results deliver the first tight
regret guarantees for contextual pricing in the presence of biased offline
data. Our techniques also transfer verbatim to stochastic linear bandits with
biased offline data, yielding analogous bounds.

</details>


### [124] [Understanding and Improving Length Generalization in Recurrent Models](https://arxiv.org/abs/2507.02782)
*Ricardo Buitrago Ruiz,Albert Gu*

Main category: cs.LG

TL;DR: 该论文探讨了循环模型在序列长度泛化上的失败原因，并提出简单训练干预措施以提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 循环模型（如状态空间模型和线性注意力）理论上能处理任意长序列，但实际性能在超出训练上下文长度时会显著下降。本文旨在分析这一现象并提出解决方案。

Method: 通过实证和理论分析支持'未探索状态假设'，并提出训练干预措施（如高斯噪声初始化或使用其他序列的最终状态）以增加状态覆盖。

Result: 仅用500次后训练步骤（约0.1%的预训练预算），干预措施使模型能泛化到远超训练长度的序列（如2k→128k），并在长上下文任务中表现更优。

Conclusion: 简单的训练干预能有效提升循环模型的长度泛化能力，为实际应用提供高效解决方案。

Abstract: Recently, recurrent models such as state space models and linear attention
have become popular due to their linear complexity in the sequence length.
Thanks to their recurrent nature, in principle they can process arbitrarily
long sequences, but their performance sometimes drops considerably beyond their
training context lengths-i.e. they fail to length generalize. In this work, we
provide comprehensive empirical and theoretical analysis to support the
unexplored states hypothesis, which posits that models fail to length
generalize when during training they are only exposed to a limited subset of
the distribution of all attainable states (i.e. states that would be attained
if the recurrence was applied to long sequences). Furthermore, we investigate
simple training interventions that aim to increase the coverage of the states
that the model is trained on, e.g. by initializing the state with Gaussian
noise or with the final state of a different input sequence. With only 500
post-training steps ($\sim 0.1\%$ of the pre-training budget), these
interventions enable length generalization for sequences that are orders of
magnitude longer than the training context (e.g. $2k\longrightarrow 128k$) and
show improved performance in long context tasks, thus presenting a simple and
efficient way to enable robust length generalization in general recurrent
models.

</details>


### [125] [In-Training Multicalibrated Survival Analysis for Healthcare via Constrained Optimization](https://arxiv.org/abs/2507.02807)
*Thiti Suttaket,Stanley Kok*

Main category: cs.LG

TL;DR: GRADUATE模型通过多校准优化解决生存分析中的子群体校准问题，平衡校准与区分度，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有生存模型仅在全群体层面校准，可能导致少数子群体校准不佳，影响临床决策准确性。

Method: GRADUATE将多校准问题转化为约束优化，同时优化校准与区分度，数学证明其解接近最优且可行。

Result: 在真实临床数据上，GRADUATE优于现有基准模型，分析揭示了其优势与基准的不足。

Conclusion: GRADUATE通过多校准优化提升了生存模型的子群体校准能力，具有实际应用价值。

Abstract: Survival analysis is an important problem in healthcare because it models the
relationship between an individual's covariates and the onset time of an event
of interest (e.g., death). It is important for survival models to be
well-calibrated (i.e., for their predicted probabilities to be close to
ground-truth probabilities) because badly calibrated systems can result in
erroneous clinical decisions. Existing survival models are typically calibrated
at the population level only, and thus run the risk of being poorly calibrated
for one or more minority subpopulations. We propose a model called GRADUATE
that achieves multicalibration by ensuring that all subpopulations are
well-calibrated too. GRADUATE frames multicalibration as a constrained
optimization problem, and optimizes both calibration and discrimination
in-training to achieve a good balance between them. We mathematically prove
that the optimization method used yields a solution that is both near-optimal
and feasible with high probability. Empirical comparisons against
state-of-the-art baselines on real-world clinical datasets demonstrate
GRADUATE's efficacy. In a detailed analysis, we elucidate the shortcomings of
the baselines vis-a-vis GRADUATE's strengths.

</details>


### [126] [Replicable Distribution Testing](https://arxiv.org/abs/2507.02814)
*Ilias Diakonikolas,Jingyi Gao,Daniel Kane,Sihan Liu,Christopher Ye*

Main category: cs.LG

TL;DR: 论文系统研究了算法可复制性框架下的分布测试问题，提出了新的可复制算法，并建立了样本复杂度下界。


<details>
  <summary>Details</summary>
Motivation: 研究在算法可复制性框架下，如何高效测试概率分布的自然属性。

Method: 开发了新的可复制算法用于测试离散分布的接近性和独立性，并提出了证明样本复杂度下界的新方法。

Result: 建立了接近最优的样本复杂度下界，解决了先前工作中的开放性问题。

Conclusion: 论文为可复制分布测试提供了理论和算法支持，具有广泛的应用潜力。

Abstract: We initiate a systematic investigation of distribution testing in the
framework of algorithmic replicability. Specifically, given independent samples
from a collection of probability distributions, the goal is to characterize the
sample complexity of replicably testing natural properties of the underlying
distributions. On the algorithmic front, we develop new replicable algorithms
for testing closeness and independence of discrete distributions. On the lower
bound front, we develop a new methodology for proving sample complexity lower
bounds for replicable testing that may be of broader interest. As an
application of our technique, we establish near-optimal sample complexity lower
bounds for replicable uniformity testing -- answering an open question from
prior work -- and closeness testing.

</details>


### [127] [LLM-Driven Treatment Effect Estimation Under Inference Time Text Confounding](https://arxiv.org/abs/2507.02843)
*Yuchen Ma,Dennis Frauen,Jonas Schweisthal,Stefan Feuerriegel*

Main category: cs.LG

TL;DR: 论文提出了一种解决训练与推断数据不一致导致的治疗效应估计偏差的方法，利用大型语言模型和双重稳健学习框架。


<details>
  <summary>Details</summary>
Motivation: 临床实践中，训练数据与推断数据的不一致可能导致治疗效应估计偏差，影响个性化医疗决策。

Method: 提出了一种结合大型语言模型和双重稳健学习的新框架，以解决推断时的文本混杂问题。

Result: 实验证明该框架在实际应用中能有效减少偏差。

Conclusion: 该框架为临床实践中的治疗效应估计提供了更可靠的方法。

Abstract: Estimating treatment effects is crucial for personalized decision-making in
medicine, but this task faces unique challenges in clinical practice. At
training time, models for estimating treatment effects are typically trained on
well-structured medical datasets that contain detailed patient information.
However, at inference time, predictions are often made using textual
descriptions (e.g., descriptions with self-reported symptoms), which are
incomplete representations of the original patient information. In this work,
we make three contributions. (1) We show that the discrepancy between the data
available during training time and inference time can lead to biased estimates
of treatment effects. We formalize this issue as an inference time text
confounding problem, where confounders are fully observed during training time
but only partially available through text at inference time. (2) To address
this problem, we propose a novel framework for estimating treatment effects
that explicitly accounts for inference time text confounding. Our framework
leverages large language models together with a custom doubly robust learner to
mitigate biases caused by the inference time text confounding. (3) Through a
series of experiments, we demonstrate the effectiveness of our framework in
real-world applications.

</details>


### [128] [MvHo-IB: Multi-View Higher-Order Information Bottleneck for Brain Disorder Diagnosis](https://arxiv.org/abs/2507.02847)
*Kunyu Zhang,Qiang Li,Shujian Yu*

Main category: cs.LG

TL;DR: MvHo-IB是一个多视图学习框架，通过整合高阶相互作用（HOIs）和成对相互作用，提升fMRI数据的诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以有效提取和利用HOIs，限制了诊断准确性。

Method: 结合信息论的O-information和Renyi熵估计器量化HOIs，设计Brain3DCNN编码器，并引入多视图信息瓶颈目标。

Result: 在三个基准fMRI数据集上表现优于现有方法，包括超图技术。

Conclusion: MvHo-IB通过创新方法显著提升了诊断性能，代码已开源。

Abstract: Recent evidence suggests that modeling higher-order interactions (HOIs) in
functional magnetic resonance imaging (fMRI) data can enhance the diagnostic
accuracy of machine learning systems. However, effectively extracting and
utilizing HOIs remains a significant challenge. In this work, we propose
MvHo-IB, a novel multi-view learning framework that integrates both pairwise
interactions and HOIs for diagnostic decision-making, while automatically
compressing task-irrelevant redundant information. MvHo-IB introduces several
key innovations: (1) a principled method that combines O-information from
information theory with a matrix-based Renyi alpha-order entropy estimator to
quantify and extract HOIs, (2) a purpose-built Brain3DCNN encoder to
effectively utilize these interactions, and (3) a new multi-view learning
information bottleneck objective to enhance representation learning.
Experiments on three benchmark fMRI datasets demonstrate that MvHo-IB achieves
state-of-the-art performance, significantly outperforming previous methods,
including recent hypergraph-based techniques. The implementation of MvHo-IB is
available at https://github.com/zky04/MvHo-IB.

</details>
